{"id":"2k6wXud2N91xpf4PwnSfo4j5WSuTNX5k","title":"Tech News - Last 2 days","displayTitle":"Tech News - Last 2 days","url":"","feedLink":"","is_query":true,"items":[{"title":"'Y2K Seems Like a Joke Now, But in 1999 People Were Freaking Out'","url":"https://it.slashdot.org/story/24/12/29/195226/y2k-seems-like-a-joke-now-but-in-1999-people-were-freaking-out?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735502220,"author":"EditorDavid","unread":true,"desc":"","content":"NPR remembers when the world \"prepared for the impending global meltdown\" that might've been, on December 31, 1999 &mdash; and the possible bug known as Y2K:\n\nThe Clinton administration said that preparing the U.S. for Y2K was probably \"the single largest technology management challenge in history.\" The bug threatened a cascade of potential disruptions &mdash; blackouts, medical equipment failures, banks shutting down, travel screeching to a halt &mdash; if the systems and software that helped keep society functioning no longer knew what year it was... Computer specialist and grassroots organizer Paloma O'Riley compared the scale and urgency of Y2K prep to telling somebody to change out a rivet on the Golden Gate Bridge. Changing out just one rivet is simple, but \"if you suddenly tell this person he now has to change out all the rivets on the bridge and he has only 24 hours to do it in &mdash; that's a problem,\" O'Riley told reporter Jason Beaubien in 1998.... \n\nThe date switchover rattled a swath of vital tech, including Wall Street trading systems, power plants and tools used in air traffic control. The Federal Aviation Administration put its systems through stress tests and mock scenarios as 2000 drew closer. \"Twenty-three million lines of code in the air traffic control system did seem a little more daunting, I will say, than I had probably anticipated,\" FAA Administrator Jane Garvey told NPR in 1998. Ultimately there were no systemwide aviation breakdowns, but airlines were put on a Y2K alert.... \n\nSome financial analysts remained skeptical Y2K would come and go with minimal disruption. But by November 1999 the Federal Reserve said it was confident the U.S. economy would weather the big switch. \"Federal banking agencies have been visited and inspected. Every bank in the United States, which includes probably 9,000 to 10,000 institutions, over 99% received a satisfactory rating,\" Fed Board Governor Edward Kelley said at the time. \n\nThe article also remembers a California programmer who bought a mobile home, a propane generator, and a year's supply of dehydrated food. (They were also considering buying a handgun &mdash; and converting his bank savings into gold, silver, and cash.) And \"Dozens of communities across the U.S. formed Y2K preparedness groups to stave off unnecessary panic...\" \n\nBut the article concludes that \"the aggressive planning and recalibration paid off. Humanity passed into the year 2000 without pandemonium...\" \n\nAnd \"People like Jack Pentes of Charlotte, N.C., were left to figure out what to do with their emergency stockpiles.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status='Y2K+Seems+Like+a+Joke+Now%2C+But+in+1999+People+Were+Freaking+Out'%3A+https%3A%2F%2Fit.slashdot.org%2Fstory%2F24%2F12%2F29%2F195226%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fit.slashdot.org%2Fstory%2F24%2F12%2F29%2F195226%2Fy2k-seems-like-a-joke-now-but-in-1999-people-were-freaking-out%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://it.slashdot.org/story/24/12/29/195226/y2k-seems-like-a-joke-now-but-in-1999-people-were-freaking-out?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564159&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"'Did Anything Good Happen in 2024? Actually, Yes!'","url":"https://science.slashdot.org/story/24/12/29/030248/did-anything-good-happen-in-2024-actually-yes?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735498620,"author":"EditorDavid","unread":true,"desc":"","content":"The Washington Post shares some good news from 2024:\n\nResearchers were able to detect a significant dip in atmospheric levels of hydrochlorofluorocarbons &mdash; harmful gases that deplete the ozone layer &mdash; for the first time, almost 30 years after countries first agreed to phase out the chemicals. \n\nA new satellite launched in March to track and publicly reveal the biggest methane polluters in the oil and gas industry &mdash; an important step in tackling the greenhouse gas that accounts for almost a third of global warming. The NASA/Carbon Mapper satellite, which measures CO2 and methane emissions, also launched, providing detailed images from individual oil and gas facilities across the world. \n\nBack on Earth, the world's largest plant for pulling carbon dioxide out of the atmosphere opened in Iceland. Norway became the first country to have more electric than gas-powered vehicles, while one Japanese island began using a new generation of batteries to help stockpile massive amounts of clean electricity. \n\nThere were also small but important victories for animal conservation. The Iberian lynx, a European wildcat once on the brink of extinction, is no longer classed as an \"endangered\" species &mdash; in what experts have hailed as the \"greatest recovery of a cat species ever achieved through conservation....\" \n\nDespite a large number of powerful tornadoes to hit the United States in early 2024, the death tolls were fortunately not as high as meteorologists feared, in part due to improved forecasting technology. \n\nThe article also notes America's Food and Drug Administration approved a new therapy which uses a patients' own cells to attack skin cancer for adults for whom surgery isn't an option. \"Experts said the decision could open the door to similar treatments for far more common cancers.\" \n\nAnd one more inspiring story from 2024: 105-year-old Virginia Hislop, of Yakima, Washington received her master's degree from Stanford University...<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status='Did+Anything+Good+Happen+in+2024%3F++Actually%2C+Yes!'%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F030248%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F030248%2Fdid-anything-good-happen-in-2024-actually-yes%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/29/030248/did-anything-good-happen-in-2024-actually-yes?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563775&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Benchmarking The AMD INVLPGB Linux Kernel Patches For Better Performance","url":"https://www.phoronix.com/review/amd-invlpgb-linux","date":1735493720,"author":"Michael Larabel","unread":true,"desc":"","content":"Last weekend a Meta engineer posted Linux kernel patches to make use of the AMD INVLPGB instruction for broadcast TLB invalidation. The Linux kernel can in turn invalidate TLB entries on remote CPUs without needing to send IPIs and without having to wait for remote CPUs to handle those interrupts. Synthetic benchmarks shown in that patch series were very promising and thus I carried out some benchmarking over the holidays of this AMD INVLPGB support for the Linux kernel.","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Electric Air Taxis are Taking Flight. Can They Succeed as a Business?","url":"https://tech.slashdot.org/story/24/12/28/2346253/electric-air-taxis-are-taking-flight-can-they-succeed-as-a-business?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735490040,"author":"EditorDavid","unread":true,"desc":"","content":"An anonymous reader shared this report from the Washington Post:\n \nArcher is aiming to launch its first commercially operated [and electrically-powered] flights with a pilot and passengers within a year in Abu Dhabi. A competitor, Joby Aviation, says it is aiming to launch passenger service in Dubai as soon as late 2025. Advancements in batteries and other technologies required for the futuristic tilt-rotor craft are moving so fast that they could soon move beyond the novelty stage and into broader commercial use in a matter of years. Both companies are laying plans to operate at the 2028 Olympics in Los Angeles... \n\nScaling the industry from a novelty ride for the wealthy to a broadly available commuter option will take billions more in start-up money, executives said, including building out a network of takeoff and landing areas (called vertiports) and charging stations. Some high-profile ventures have already faltered. A plan for air taxis to transport spectators around the Paris Olympics fizzled... Still, investors, including big names like Stellantis and Toyota, have poured money into Silicon Valley companies like Archer and Joby. Boeing and Airbus are developing their own versions. All are betting that quieter, greener and battery-powered aircraft can revolutionize the way people travel. Major U.S. airlines including American, Delta, Southwest and United also are building relationships and planting seeds for deals with air taxi companies.\n \n\nTwo interesting quotes from the article:\n\n\"It feels like the modern-day American Dream, where you can invent a technology and actually bring it to market even [if it's] as crazy as what some people call flying cars.\" &mdash; Adam Goldstein, CEO of Archer Aviation.\n\n\n\"They have created these amazing new aircraft that really 10 or 15 years ago would've been unimaginable. I think there's something innately attractive about being able to leapfrog all of your terrestrial obstacles. Who hasn't wished that if you live in the suburbs that, you know, something could drop into your cul-de-sac and 15 minutes later you're at the office.\" \n&mdash; Roger Connor, curator of the vertical flight collection at the Smithsonian's National Air and Space Museum.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Electric+Air+Taxis+are+Taking+Flight.+Can+They+Succeed+as+a+Business%3F%3A+https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F28%2F2346253%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F28%2F2346253%2Felectric-air-taxis-are-taking-flight-can-they-succeed-as-a-business%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://tech.slashdot.org/story/24/12/28/2346253/electric-air-taxis-are-taking-flight-can-they-succeed-as-a-business?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563677&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Evaluating vLLM With Basic Sampling","url":"https://hackernoon.com/evaluating-vllm-with-basic-sampling?source=rss","date":1735488913,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/UZfnXsE8xpuPm7xJGzSd\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Joq6DOP9CkD0S4F8MX1r\">2 Background and 2.1 Transformer-Based Large Language Models</a></p>\n<p><a href=\"http://hackernoon.com/preview/25g0m9nrxol991ZY9qci\">2.2 LLM Service & Autoregressive Generation</a></p>\n<p><a href=\"http://hackernoon.com/preview/IQ9Fd8hlh5MpHVMFA27O\">2.3 Batching Techniques for LLMs</a></p>\n<p><a href=\"https://hackernoon.com/preview/ytSMq2pxVtKIRC7iS3kK\">3 Memory Challenges in LLM Serving</a></p>\n<p><a href=\"http://hackernoon.com/preview/6BZmg60VAii9DNx0L1qc\">3.1 Memory Management in Existing Systems</a></p>\n<p><a href=\"http://hackernoon.com/preview/ZzGN1QRkg16N6zN7hCJi\">4 Method and 4.1 PagedAttention</a></p>\n<p><a href=\"http://hackernoon.com/preview/4Da8juHWnURn6g4urrl4\">4.2 KV Cache Manager</a></p>\n<p><a href=\"http://hackernoon.com/preview/GieTLdxmIGxSHDideqV5\">4.3 Decoding with PagedAttention and vLLM</a></p>\n<p><a href=\"http://hackernoon.com/preview/7g3isP8BzgyEsTasNUr2\">4.4 Application to Other Decoding Scenarios</a></p>\n<p><a href=\"http://hackernoon.com/preview/TEjiBAga2TDQ1ZZp5Vbg\">4.5 Scheduling and Preemption</a></p>\n<p><a href=\"http://hackernoon.com/preview/OjYLeVPi5jM2NSWKvipy\">4.6 Distributed Execution</a></p>\n<p><a href=\"http://hackernoon.com/preview/yqADDmfsCggYCcSdA6Nj\">5 Implementation</a></p>\n<p><a href=\"http://hackernoon.com/preview/S6sF02nTOzlXKZE1iQcP\">6 Evaluation and 6.1 Experimental Setup</a></p>\n<p><a href=\"http://hackernoon.com/preview/K3TeA7MNOi0g142ICu5W\">6.2 Basic Sampling</a></p>\n<p><a href=\"http://hackernoon.com/preview/cJxKQHExtKQrburnndpo\">6.3 Parallel Sampling and Beam Search</a></p>\n<p><a href=\"http://hackernoon.com/preview/JEOm7WvIfRfxEjepPqXW\">6.4 Shared prefix</a></p>\n<p><a href=\"http://hackernoon.com/preview/DWk5D3KHLfKhdQC02ayZ\">6.5 Chatbot</a></p>\n<p><a href=\"http://hackernoon.com/preview/l7VvLlkEDSBJ5GzMlwJf\">7 Ablation Studies</a></p>\n<p><a href=\"http://hackernoon.com/preview/PJb0S41IDQAbwYp0e6RZ\">8 Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/5d2sL9hRMUBNmVCZWMBC\">9 Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/HL77hmYOoM9MPB5fKgmq\">10 Conclusion, Acknowledgement and References</a></p>\n<h2 id=\"62basicsampling\">6.2 Basic Sampling</h2>\n<p>We evaluate the performance of vLLM with basic sampling (one sample per request) on three models and two datasets. The first row of Fig. 12 shows the results on the ShareGPT dataset. The curves illustrate that as the request rate increases, the latency initially increases at a gradual pace but then suddenly explodes. This can be attributed to the fact that when the request rate surpasses the capacity of the serving system, the queue length continues to grow infinitely and so does the latency of the requests.</p>\n<p>\\\nOn the ShareGPT dataset, vLLM can sustain 1.7×–2.7× higher request rates compared to Orca (Oracle) and 2.7×–8× compared to Orca (Max), while maintaining similar latencies. This is because vLLM’s PagedAttention can efficiently manage the memory usage and thus enable batching more requests than Orca. For example, as shown in Fig. 13a, for OPT-13B vLLM processes 2.2× more requests at the same time than Orca (Oracle) and 4.3× more requests than Orca (Max). Compared to FasterTransformer, vLLM can sustain up to 22× higher request rates, as FasterTransformer does not utilize a fine-grained scheduling mechanism and inefficiently manages the memory like Orca (Max).</p>\n<p>\\\nThe second row of Fig. 12 and Fig. 13b shows the results on the Alpaca dataset, which follows a similar trend to the ShareGPT dataset. One exception is Fig. 12 (f), where vLLM’s advantage over Orca (Oracle) and Orca (Pow2) is less pronounced. This is because the model and server configuration for OPT-175B (Table 1) allows for large GPU memory space available to store KV cache, while the Alpaca dataset has short sequences. In this setup, Orca (Oracle) and Orca (Pow2) can also batch a large number of requests despite the inefficiencies in their memory management. As a result, the performance of the systems becomes compute-bound rather than memory-bound.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-ql830zk.png\" alt=\"Figure 15. Average amount of memory saving from sharing KV blocks, when serving OPT-13B for the Alpaca trace.\" /></p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2309.06180\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Woosuk Kwon, UC Berkeley with Equal contribution;</p>\n<p>(2) Zhuohan Li, UC Berkeley with Equal contribution;</p>\n<p>(3) Siyuan Zhuang, UC Berkeley;</p>\n<p>(4) Ying Sheng, UC Berkeley and Stanford University;</p>\n<p>(5) Lianmin Zheng, UC Berkeley;</p>\n<p>(6) Cody Hao Yu, Independent Researcher;</p>\n<p>(7) Cody Hao Yu, Independent Researcher;</p>\n<p>(8) Joseph E. Gonzalez, UC Berkeley;</p>\n<p>(9) Hao Zhang, UC San Diego;</p>\n<p>(10) Ion Stoica, UC Berkeley.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The HackerNoon Newsletter: Heres Why High Achievers Feel Like Failures (12/29/2024)","url":"https://hackernoon.com/12-29-2024-newsletter?source=rss","date":1735488338,"author":"Noonification","unread":true,"desc":"","content":"\n              \n        <p><strong>How are you, hacker?</strong></p>\n        <br />\n        <p>🪐 What’s happening in tech today, December 29, 2024?</p>\n        <br />\n        <p>\n          The\n          <a href=\"https://hackernoon.com/noonification\" target=\"_blank\" rel=\"noopener\"> HackerNoon Newsletter</a>\n          brings the HackerNoon \n          <a href=\"https://hackernoon.com\" target=\"_blank\" rel=\"noopener\">homepage</a>\n          straight to your inbox.\n          <a href=\"https://hackernoon.com/on-this-day\" target=\"_blank\" rel=\"noopener\">On this day,</a>\n          \n            <strong>The HMS Warrior is Launched</strong> in 1860,  <strong>The Apex of the Japanese Asset Price Bubble Occured</strong> in 1989,  <strong> The UK settled its Anglo-American, post WWII loan debt</strong> in 2006, \n          \n          and  we present you with these top quality stories. \n          \n            From \n        <a href=\"https://hackernoon.com/future-proof-your-marketing-with-this-guide-on-writing-for-ai-search-engines\" class=\"eventTitle\"><strong>Future-proof Your Marketing With This Guide on Writing for AI Search Engines</strong></a>\n       to \n        <a href=\"https://hackernoon.com/data-availability-or-how-rollups-learned-to-stop-worrying-and-love-ethereum\" class=\"eventTitle\"><strong>Data Availability Or: How Rollups Learned To Stop Worrying And Love Ethereum</strong></a>,\n       let’s dive right in.\n          \n        </p>\n      \n              \n          <h2><a href=\"https://hackernoon.com/rootstockcollective-in-depth-empowering-bitcoin-builders\">RootstockCollective In-Depth: Empowering Bitcoin Builders</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/InxBRjRIs6M1kdhuWcyNHiiUrxm1-j5034qi.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/rootstock_io\">@rootstock_io</a> [ 7 Min read ] Empowering Bitcoin builders with RootstockCollective DAO: Rewarding innovation, stakers, and developers in the Bitcoin sidechain ecosystem. <a href=\"https://hackernoon.com/rootstockcollective-in-depth-empowering-bitcoin-builders\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/join-lumoz-zkverifier-node-mining-and-share-25-billion-moz-rewards\">Join Lumoz zkVerifier Node Mining and Share 2.5 Billion MOZ Rewards</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/Wls6TtjOLGMbl8aKwQlIbcyfjQF2-cx03ymq.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/lumoz\">@lumoz</a> [ 4 Min read ] Lumoz Node Network  MOZ staking are live! Join the zkVerifier network, stake MOZ, or run nodes to share 25% of $90M in rewards. Act now! <a href=\"https://hackernoon.com/join-lumoz-zkverifier-node-mining-and-share-25-billion-moz-rewards\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/future-proof-your-marketing-with-this-guide-on-writing-for-ai-search-engines\">Future-proof Your Marketing With This Guide on Writing for AI Search Engines</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/R40xrKHcy9QXU6NDkd58YY2mQOz1-e313j4y.webp\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/darragh\">@darragh</a> [ 4 Min read ] Learn how to write for AI  search engines with actionable tips, examples,  FAQs. Future-proof your content for ChatGPT, Gemini,  Google SEO success! <a href=\"https://hackernoon.com/future-proof-your-marketing-with-this-guide-on-writing-for-ai-search-engines\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/the-sneaky-way-web-browsers-are-identifying-you-even-when-you-turn-off-cookies\">The Sneaky Way Web Browsers Are Identifying You (Even When You Turn Off Cookies)</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/lvbwxpoO4WXH8MNqfBFKapMQkAi2-1d0370t.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/rampageproxies\">@rampageproxies</a> [ 12 Min read ] A guide on browser fingerprinting, how it identifies us, fingerprint testing, and what techniques we can use to ensure our browsing is kept anonymous. <a href=\"https://hackernoon.com/the-sneaky-way-web-browsers-are-identifying-you-even-when-you-turn-off-cookies\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/harnessing-shared-security-for-secure-cross-chain-interoperability\">Harnessing Shared Security For Secure Cross-Chain Interoperability</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/WSQJfCSXOxWphTNQ7sneVvhdWGu1-l903680.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/2077research\">@2077research</a> [ 47 Min read ] A deep dive on shared security and the role of shared security infrastructure in building robust and secure cross-chain interoperability solutions for users. <a href=\"https://hackernoon.com/harnessing-shared-security-for-secure-cross-chain-interoperability\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/data-availability-or-how-rollups-learned-to-stop-worrying-and-love-ethereum\">Data Availability Or: How Rollups Learned To Stop Worrying And Love Ethereum</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/WSQJfCSXOxWphTNQ7sneVvhdWGu1-hd036k1.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/2077research\">@2077research</a> [ 27 Min read ] Data availability is a critical component of scaling Ethereum. Learn how and why Layer 2 rollups use Ethereum for data availability—and why this matters.  <a href=\"https://hackernoon.com/data-availability-or-how-rollups-learned-to-stop-worrying-and-love-ethereum\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/heres-why-high-achievers-feel-like-failures\">Heres Why High Achievers Feel Like Failures</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/standing-on-top-of-a-mountain-d0kxvq2p6k7msfxxmb3n6r8m.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/scottdclary\">@scottdclary</a> [ 11 Min read ] The invisible progress paradox is a trap in personal growth. <a href=\"https://hackernoon.com/heres-why-high-achievers-feel-like-failures\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/goodbye-passwords-hello-passkeys-the-future-of-authentication\">Goodbye Passwords, Hello Passkeys: The Future of Authentication </a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/security-symbol-on-a-laptop-screen-e9ol03dr3i89katj0uwpomzi.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/radioactive\">@radioactive</a> [ 6 Min read ] Discover how passkeys revolutionize online authentication.  <a href=\"https://hackernoon.com/goodbye-passwords-hello-passkeys-the-future-of-authentication\">Read More.</a></p>\n        \n              \n        <br />\n        <p>🧑‍💻 What happened in your world this week?</p>\n        <p>\n          It's been said that\n          <a href=\"https://hackernoon.com/developers-the-why-and-how-to-writing-technical-articles-54e824789ef6\">writing can help consolidate technical knowledge</a>,\n          <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\">establish credibility</a>,\n          <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\"> and contribute to emerging community standards</a>.\n          Feeling stuck? We got you covered ⬇️⬇️⬇️\n        </p>\n        <br />\n        <p>\n          <a href=\"https://app.hackernoon.com/mobile/lZx3fmlPdlPJpVBIdble\">ANSWER THESE GREATEST INTERVIEW QUESTIONS OF ALL TIME</a>\n        </p>\n        <br />\n        <p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>\n        <br />\n        <p><img src=\"https://cdn.hackernoon.com/images/the-hackernoon-newsletter-footer.png\" alt /></p>\n      \n            ","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Evaluating the Performance of vLLM: How Did It Do?","url":"https://hackernoon.com/evaluating-the-performance-of-vllm-how-did-it-do?source=rss","date":1735488014,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/UZfnXsE8xpuPm7xJGzSd\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Joq6DOP9CkD0S4F8MX1r\">2 Background and 2.1 Transformer-Based Large Language Models</a></p>\n<p><a href=\"http://hackernoon.com/preview/25g0m9nrxol991ZY9qci\">2.2 LLM Service & Autoregressive Generation</a></p>\n<p><a href=\"http://hackernoon.com/preview/IQ9Fd8hlh5MpHVMFA27O\">2.3 Batching Techniques for LLMs</a></p>\n<p><a href=\"https://hackernoon.com/preview/ytSMq2pxVtKIRC7iS3kK\">3 Memory Challenges in LLM Serving</a></p>\n<p><a href=\"http://hackernoon.com/preview/6BZmg60VAii9DNx0L1qc\">3.1 Memory Management in Existing Systems</a></p>\n<p><a href=\"http://hackernoon.com/preview/ZzGN1QRkg16N6zN7hCJi\">4 Method and 4.1 PagedAttention</a></p>\n<p><a href=\"http://hackernoon.com/preview/4Da8juHWnURn6g4urrl4\">4.2 KV Cache Manager</a></p>\n<p><a href=\"http://hackernoon.com/preview/GieTLdxmIGxSHDideqV5\">4.3 Decoding with PagedAttention and vLLM</a></p>\n<p><a href=\"http://hackernoon.com/preview/7g3isP8BzgyEsTasNUr2\">4.4 Application to Other Decoding Scenarios</a></p>\n<p><a href=\"http://hackernoon.com/preview/TEjiBAga2TDQ1ZZp5Vbg\">4.5 Scheduling and Preemption</a></p>\n<p><a href=\"http://hackernoon.com/preview/OjYLeVPi5jM2NSWKvipy\">4.6 Distributed Execution</a></p>\n<p><a href=\"http://hackernoon.com/preview/yqADDmfsCggYCcSdA6Nj\">5 Implementation</a></p>\n<p><a href=\"http://hackernoon.com/preview/S6sF02nTOzlXKZE1iQcP\">6 Evaluation and 6.1 Experimental Setup</a></p>\n<p><a href=\"http://hackernoon.com/preview/K3TeA7MNOi0g142ICu5W\">6.2 Basic Sampling</a></p>\n<p><a href=\"http://hackernoon.com/preview/cJxKQHExtKQrburnndpo\">6.3 Parallel Sampling and Beam Search</a></p>\n<p><a href=\"http://hackernoon.com/preview/JEOm7WvIfRfxEjepPqXW\">6.4 Shared prefix</a></p>\n<p><a href=\"http://hackernoon.com/preview/DWk5D3KHLfKhdQC02ayZ\">6.5 Chatbot</a></p>\n<p><a href=\"http://hackernoon.com/preview/l7VvLlkEDSBJ5GzMlwJf\">7 Ablation Studies</a></p>\n<p><a href=\"http://hackernoon.com/preview/PJb0S41IDQAbwYp0e6RZ\">8 Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/5d2sL9hRMUBNmVCZWMBC\">9 Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/HL77hmYOoM9MPB5fKgmq\">10 Conclusion, Acknowledgement and References</a></p>\n<h2 id=\"6evaluation\">6 Evaluation</h2>\n<p>In this section, we evaluate the performance of vLLM under a variety of workloads.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-9z830sn.png\" alt=\"Figure 12. Single sequence generation with OPT models on the ShareGPT and Alpaca dataset\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-hr930dj.png\" alt=\"Figure 13. Average number of batched requests when serving OPT-13B for the ShareGPT (2 reqs/s) and Alpaca (30 reqs/s) traces.\" /></p>\n<h3 id=\"61experimentalsetup\">6.1 Experimental Setup</h3>\n<p><strong>Model and server configurations.</strong> We use OPT [62] models with 13B, 66B, and 175B parameters and LLaMA [52] with 13B parameters for our evaluation. 13B and 66B are popular sizes for LLMs as shown in an LLM leaderboard [38], while 175B is the size of the famous GPT-3 [5] model. For all of our experiments, we use A2 instances with NVIDIA A100 GPUs on Google Cloud Platform. The detailed model sizes and server configurations are shown in Table 1.</p>\n<p>\\\n<strong>Workloads</strong>. We synthesize workloads based on ShareGPT [51] and Alpaca [50] datasets, which contain input and output texts of real LLM services. The ShareGPT dataset is a collection of user-shared conversations with ChatGPT [35]. The Alpaca dataset is an instruction dataset generated by GPT3.5 with self-instruct [57]. We tokenize the datasets and use their input and output lengths to synthesize client requests. As shown in Fig. 11, the ShareGPT dataset has 8.4× longer input prompts and 5.8× longer outputs on average than the Alpaca dataset, with higher variance. Since these datasets do not include timestamps, we generate request arrival times using Poisson distribution with different request rates.</p>\n<p>\\\n<strong>Baseline 1:</strong> FasterTransformer. FasterTransformer [31] is a distributed inference engine highly optimized for latency. As FasterTransformer does not have its own scheduler, we implement a custom scheduler with a dynamic batching mechanism similar to the existing serving systems such as Triton [30]. Specifically, we set a maximum batch size 𝐵 as large as possible for each experiment, according to the GPU memory capacity. The scheduler takes up to 𝐵 number of earliest arrived requests and sends the batch to FasterTransformer for processing.</p>\n<p>\\\n<strong>Baseline 2:</strong> Orca. Orca [60] is a state-of-the-art LLM serving system optimized for throughput. Since Orca is not publicly available for use, we implement our own version of Orca. We assume Orca uses the buddy allocation algorithm to determine the memory address to store KV cache. We implement three versions of Orca based on how much it over-reserves the space for request outputs:</p>\n<p>\\\n<strong>• Orca (Oracle).</strong> We assume the system has the knowledge of the lengths of the outputs that will be actually generated for the requests. This shows the upper-bound performance of Orca, which is infeasible to achieve in practice.</p>\n<p>\\\n<strong>• Orca (Pow2)</strong>. We assume the system over-reserves the space for outputs by at most 2×. For example, if the true output length is 25, it reserves 32 positions for outputs.</p>\n<p>\\\n<strong>• Orca (Max).</strong> We assume the system always reserves the space up to the maximum sequence length of the model, i.e., 2048 tokens.</p>\n<p>\\\n<strong>Key metrics.</strong> We focus on serving throughput. Specifically, using the workloads with different request rates, we measure normalized latency of the systems, the mean of every request’s end-to-end latency divided by its output length, as in Orca [60]. A high-throughput serving system should retain low normalized latency against high request rates. For most experiments, we evaluate the systems with 1-hour traces. As an exception, we use 15-minute traces for the OPT-175B model due to the cost limit.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-tya309r.png\" alt=\"Figure 14. Parallel generation and beam search with OPT-13B on the Alpaca dataset.\" /></p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2309.06180\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Woosuk Kwon, UC Berkeley with Equal contribution;</p>\n<p>(2) Zhuohan Li, UC Berkeley with Equal contribution;</p>\n<p>(3) Siyuan Zhuang, UC Berkeley;</p>\n<p>(4) Ying Sheng, UC Berkeley and Stanford University;</p>\n<p>(5) Lianmin Zheng, UC Berkeley;</p>\n<p>(6) Cody Hao Yu, Independent Researcher;</p>\n<p>(7) Cody Hao Yu, Independent Researcher;</p>\n<p>(8) Joseph E. Gonzalez, UC Berkeley;</p>\n<p>(9) Hao Zhang, UC San Diego;</p>\n<p>(10) Ion Stoica, UC Berkeley.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Could a Sponge Made from Squid Bones Help Remove Microplastics?","url":"https://science.slashdot.org/story/24/12/28/2124223/could-a-sponge-made-from-squid-bones-help-remove-microplastics?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735486440,"author":"EditorDavid","unread":true,"desc":"","content":"While microplastics seem to be everywhere, CNN reports that scientists in China \"have come up with a possible solution: a biodegradable sponge made of squid bones and cotton\" (which contain two organic compounds \"known for eliminating pollution from wastewater...\")\n\n\nThey then tested the sponge in four different water samples, taken from irrigation water, pond water, lake water and sea water, and found it removed up to 99.9% of microplastics, according to a study published last month in Science Advances... The sponge created by the Wuhan researchers was able to absorb microplastics both by physically intercepting them and through electromagnetic attraction, the study said. \n\n Previously studied methods for absorbing plastics tend to be expensive and difficult to make, limiting their scalability. Last year, researchers in Qingdao, China developed a synthetic sponge made of starch and gelatin designed to remove microplastics from water, though its efficacy varied depending on water conditions. The low cost and wide availability of both cotton and squid bones mean [the Chinese researchers' sponge] \"has great potential to be used in the extraction of microplastic from complex water bodies,\" according to the study. \n\n Shima Ziajahromi, a lecturer at Australia's Griffith University who studies microplastics, called the squid-cotton-sponge method \"promising\" and said it could be an effective way to \"clean up the high risk and vulnerable aquatic ecosystem.\" However, the study's authors did not address whether the sponge can remove microplastics that sink to the sediment, which is the majority of microplastics in our waters, said Ziajahromi, who was not involved in the study. Another \"critical issue\" is the proper disposal of the sponges, Ziajahromi said. \"Although the material is biodegradable, the microplastics it absorbs need to be disposed of properly,\" she said. \"Without careful management, this process risks transferring microplastics from one ecosystem to another.\" \n\nUltimately, Ziajahromi added, minimizing plastic pollution is in the first place should remain a \"top priority.\"\n<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Could+a+Sponge+Made+from+Squid+Bones+Help+Remove+Microplastics%3F%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F28%2F2124223%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F28%2F2124223%2Fcould-a-sponge-made-from-squid-bones-help-remove-microplastics%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/28/2124223/could-a-sponge-made-from-squid-bones-help-remove-microplastics?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563595&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Kdenlive Preparing For An Exciting 2025 With Background Removal Tool & More","url":"https://www.phoronix.com/news/Kdenlive-Background-Removal-25","date":1735485314,"author":"Michael Larabel","unread":true,"desc":"","content":"The KDE app Kdenlive that is a very popular and featureful open-source video editor is preparing for an exciting 2025...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"GPS Spoofing Attacks Are Dangerously Misleading Airliners","url":"https://spectrum.ieee.org/gps-spoofing-2670499105","date":1735484403,"author":"Margo Anderson","unread":true,"desc":"","content":"<p>Electronic warfare is taking a perilous turn into civilian airspace</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4MzgwMi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc0ODQyNTA2NX0.5iWnSeL_UGfAyzZ6VJZNQQrm6u3ONzKQipV7JL7Ylhg/image.png?width=600","enclosureMime":""},{"title":"Africa’s newest fintech unicorns are winning by keeping their feet on the ground","url":"https://techcrunch.com/2024/12/29/africas-fintech-unicorns-blend-digital-banking-and-physical-touchpoints/","date":1735484400,"author":"Tage Kene-Okafor","unread":true,"desc":"","content":"<p>Africa’s tech ecosystem just got a boost of attention, with South Africa’s TymeBank and Nigeria’s Moniepoint both raising funds in recent weeks at valuations of over $1 billion and joining the coveted unicorn pantheon. But those valuations don’t just reflect investor confidence. They signal the success they’ve had in taking disruptive fintech models originally developed [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Faster USB Performance For xHCI DbC Coming With Linux 6.14 Plus A 10 Year Old Bug Fixed","url":"https://www.phoronix.com/news/Faster-USB-xHCI-DbC-Linux-6.14","date":1735483524,"author":"Michael Larabel","unread":true,"desc":"","content":"Thanks to work from Intel engineers, the upcoming Linux 6.14 kernel cycle will feature faster USB xHCI DbC performance for debug performance and a few other missing xHCI bits being addressed. Plus there is a fix for a rare 10 year old USB bug report...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New Research Claims Employees Place More Emphasis on Work-related Automation Than Compensation","url":"https://hackernoon.com/new-research-claims-employees-place-more-emphasis-on-work-related-automation-than-compensation?source=rss","date":1735480819,"author":"Dmitry Matveev","unread":true,"desc":"","content":"<p>Money is not the only tool for employee' motivation. Supplementing financial incentives with something intangible helps make employees more loyal to the company and maintains their productivity over the long term. Management is currently performing a balancing act: They must deliver the right amount of mentorship, steer corporate culture and offer attractive compensation while introducing efficiency-boosting automation without alienating employees or losing profits.</p>\n<p>\\\nWe've all heard about how process automation changes businesses, increases productivity, and reduces costs. Now let's consider another aspect: How does automation affect the emotional state of employees and their job satisfaction? I’ve encountered this in big corporations, mid-sized companies and even startups.</p>\n<p>\\</p>\n<ul>\n<li><strong>Leverage technology for streamlined processes</strong></li>\n</ul>\n<p>Introducing automation to processes that have been done the same way for generations is where many businesses falter because change is often daunting. However, <strong><a href=\"https://www.consultancy-me.com/news/7676/four-tech-trends-influencing-employee-experience-in-2024\">research</a></strong> from the consulting platform Consultancy-me is eye-opening: Employee satisfaction directly depends on the level of automation of processes in the company.</p>\n<p>\\\nI have often met clients who ask for help in simplifying their internal processes, which has a bonus effect on the team. In one instance, we decided to clarify feedback not only from executives but employees all the way down the chain. It became an effective mode of interaction between employees, managers, and department heads. Many reported a significant boost in job satisfaction, citing a newfound sense of purpose and creativity in their roles, leading to higher morale and increased productivity. Overall, the reduction of repetitive tasks through automation not only enhances efficiency but also fosters a more fulfilling work environment.</p>\n<p>\\</p>\n<ul>\n<li><strong>Give your workplace culture a competitive edge</strong></li>\n</ul>\n<p>Companies are increasingly turning to automation as a means to enhance efficiency and productivity. For example, teams that have adopted automation tools often <strong><a href=\"https://systems.enpress-publisher.com/index.php/jipd/article/viewFile/6918/3799\">report</a></strong> fewer overtime hours and a more manageable workload, allowing for more personal time and reduced burnout rates. Enhanced flexibility in employees’ schedules is vital for maintaining mental and emotional health.&nbsp; Moreover, the rise of automation creates increased demand for advanced skills, opening up opportunities for professional growth. Many organizations now offer <strong><a href=\"https://www.weforum.org/publications/the-future-of-jobs-report-2023/digest/\">training programs</a></strong> tailored to equip employees with the necessary skills to navigate automated technologies, which not only enhances their job performance but also positions them for career advancement. This shift is vital for closing the skills gap and preparing the workforce for the evolving job market, ensuring that employees remain competitive in their fields.</p>\n<p>\\\nThe desire to automate processes within a company is almost always driven by the need to better allocate resources, particularly human resources. We frequently have clients who want to streamline invoice approvals or create a database that doesn't require manual management, which causes unnecessary stress as employees try to avoid mistakes. By automating these tasks, the CFO can focus on scaling the business while employees save time by not having to search for information across multiple sources. Transforming the workplace to prioritize creativity over endless paperwork consistently shows a productivity boost. This shift not only enhances efficiency but also fosters a culture of innovation that keeps teams engaged and motivated. Fostering сreativity, work-life balance, skill development and mental health can serve as core strategies for driving employee engagement and satisfaction.</p>\n<p>\\</p>\n<ul>\n<li><strong>Equip employees to embrace and adapt to change</strong></li>\n</ul>\n<p>It will be an HR trend in 2025 to shift from AI adoption to AI adaptation — companies will move beyond simply using AI for automation and efficiency in this phase. Balancing the impact of automation requires strong management and leadership to ensure a smooth transition for all employees. Transparent communication is crucial because it helps simplify the changes and alleviates fears about job displacement. Leaders should clearly outline the benefits of automation, not only for the organization but also for individual roles, emphasizing how it can enhance productivity and job satisfaction. Involving employees in the automation process is equally important. Strategies such as soliciting feedback, incorporating their insights into decision-making, and providing opportunities for upskilling can foster a sense of ownership and reduce resistance to change. Сreating an inclusive environment where employees feel valued and informed, management can effectively navigate the challenges of automation while maintaining a motivated and engaged team.</p>\n<p>\\</p>\n<ul>\n<li><strong>Experiment with side innovation projects</strong></li>\n</ul>\n<p>\\\nA good example is <strong><a href=\"https://www.inc.com/bill-murphy-jr/google-says-it-still-uses-20-percent-rule-you-should-totally-copy-it.html\">Google’s famous “20% time” rule</a>,</strong> where employees can dedicate a portion of their work hours to personal projects that align with company goals. Give employees time each week to work on projects that interest them and could benefit the organization and host a regular meeting where employees can share ideas and get feedback from peers, fostering a culture of innovation and idea-sharing.</p>\n<p>\\\nFinally, becoming creative thinkers instead of copy-pasting machines, employees bring more value to the company and become happier. The positive impact on work-life balance, coupled with increased opportunities for skill development underscores the transformative power of automation in the modern workplace. As organizations continue to adapt to technological advancements, employees who engage with these changes will find themselves better equipped for success and self-fulfillment in their careers. Embracing automation is not just about efficiency; it’s about creating a more sustainable and rewarding work environment for all.</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Let's Build an MLOps Pipeline With Databricks and Spark - Part 2","url":"https://hackernoon.com/lets-build-an-mlops-pipeline-with-databricks-and-spark-part-2?source=rss","date":1735480815,"author":"Mohsen Jadidi","unread":true,"desc":"","content":"<p>In the ==<a href=\"https://hackernoon.com/mlops-with-databricks-and-spark-part-1\">first part of this tutorial</a>== series, we took the first steps for building an end-to-end MLOps pipeline using Databricks and Spark, guided by Databricks' reference architecture. Here's a recap of the key steps we covered:</p>\n<p>\\</p>\n<ul>\n<li><p><strong>Setting Up the Unity Catalog for Medallion Architecture</strong>: We organized our data into bronze, silver, and gold layers within the Unity Catalog, establishing a structured and efficient data management system.</p></li>\n<li><p><strong>Ingesting Data into Unity Catalog</strong>: We demonstrated how to import raw data into the system, ensuring consistency and quality for subsequent processing stages.</p></li>\n<li><p><strong>Training the Model</strong>: Utilizing Databricks, we trained a machine learning model tailored to our dataset, following best practices for scalable and effective model development.</p></li>\n<li><p><strong>Hyperparameter Tuning with HyperOpt</strong>: To enhance model performance, we employed HyperOpt to automate the search for optimal hyperparameters, improving accuracy and efficiency.</p></li>\n<li><p><strong>Experiment Tracking with Databricks MLflow</strong>: We utilized MLflow to log and monitor our experiments, maintaining a comprehensive record of model versions, metrics, and parameters for easy comparison and reproducibility.</p>\n<p>\\</p></li>\n</ul>\n<p>With these foundational steps completed, your model is now primed for deployment. In this second part, we'll focus on integrating two critical components into our system:</p>\n<p>\\</p>\n<ol>\n<li><strong>Batch Inference</strong>: Implementing batch processing to generate predictions on large datasets, suitable for applications like bulk scoring and periodic reporting.</li>\n<li><strong>Online Inference (Model Serving)</strong>: Setting up real-time model serving to provide immediate predictions, essential for interactive applications and services.</li>\n<li><strong>Model Monitoring:</strong> to ensure your deployed models maintain optimal performance and reliability over time.</li>\n</ol>\n<p>\\\nLet's get into it!</p>\n<h2 id=\"modeldeployment\">Model Deployment</h2>\n<p>The departure point of the last blog was model evaluation. Now imagine we did the comparison and found that our model shows a higher performance compare to this production model. As we want (assume) to use the model in production, we want to take advantage of all the data that we have. The next step is to train and test the model using the full dataset. Then persist our model for later use by deploying it as our champion model. Since this is the final model that we want to use for inference, we use the Feature Engineering client to train the model. This way we are not only track the model lineage easier, but also offload the schema validation and feature transformation (if any) to the client.</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">with mlflow.start_run(run_name=\"ALS_best_model\") as run:\n        als = ALS()\n        # Now we set the parameters for the method\n        als.setMaxIter(MAX_ITER)\\\n        .setSeed(SEED)\\\n        .setRegParam(best_params[\"REG_PARAM\"])\\\n        .setUserCol(COL_USER)\\\n        .setItemCol(COL_ITEM)\\\n        .setRatingCol(COL_LABEL)\\\n        .setRank(best_params[\"RANK\"])\n\n        mlflow.log_param(\"MAX_ITER\", MAX_ITER)\n        mlflow.log_param(\"RANK\", best_params[\"RANK\"])\n        mlflow.log_param(\"REG_PARAM\", best_params[\"REG_PARAM\"])\n\n        # Create the model with these parameters.\n        model = als.fit(df_full_data)\n        #drop predictions where users and products from the test test and didn't make it into the training set. in this case, the prediction is NaN\n        model.setColdStartStrategy('drop') \n        predictions = model.transform(df_full_data)\n        signature = infer_signature(model_input = df_full_data, model_output = predictions.select(COL_LABEL))\n\n        #log the model\n        mlflow.spark.log_model(model, model_name,\n                               sample_input=df_full_data.limit(3),\n                               signature = signature,\n                               conda_env=mlflow.spark.get_default_conda_env(),\n                               registered_model_name=f\"{catalog_name}.{model_schema}.{model_name}\")\n\n        evaluator = RegressionEvaluator(predictionCol=COL_PRED, labelCol=COL_LABEL)\n        rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n        mlflow.log_metric('rmse', rmse)\n</code></pre>\n<p>\\\nwe can also use the ==<a href=\"https://docs.databricks.com/en/machine-learning/feature-store/train-models-with-feature-store.html\">Feature Store or Feature Engineering APIs</a>== to train and log the models</p>\n<pre><code class=\"python language-python\">model_info = fe.log_model(model=model, \n                          artifact_path = model_name,\n                          flavor=mlflow.spark,\n                          training_set=fe_full_data,\n                          conda_env=mlflow.spark.get_default_conda_env(),\n                          registered_model_name= f\"{catalog_name}.{model_schema}.{model_name}\"\n                        )\n</code></pre>\n<p>\\\nwhen we use the feature engineering API we can view the model’s lineage in Catalog Explorer</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-ay034p9.jpeg\" alt=\"data lineage in Dataticks Unity Catalog\" /></p>\n<p>\\\nNow lets update the model description and assign a Champion label to it.</p>\n<pre><code class=\"python language-python\">import time\nfrom mlflow.tracking.client import MlflowClient\nfrom mlflow.entities.model_registry.model_version_status import ModelVersionStatus\n\nclient = MlflowClient()\n\n#find the latest model version\nmodel_name_path = f\"{catalog_name}.{model_schema}.{model_name}\"\nmodel_version_infos = client.search_model_versions(f\"name ='{model_name_path}'\")\nnew_model_version = max([int(model_version_info.version) for model_version_info in model_version_infos])\n\n#add the model and model version descirption\nclient.update_registered_model(\n  name=model_name_path,\n  description=\"collaborative filtering using Spark mllib ALS. This model use rating table\"\n)\n\nclient.update_model_version(\n  name=model_name_path,\n  version=new_model_version,\n  description=\"this model is optimized Rank and REG_PARAM with Hyperopt and rmse as a loss function. trained on the full dataset\"\n)\n\n# assign alias\nclient.set_registered_model_alias(model_name_path, \"Champion\", new_model_version)\n</code></pre>\n<p>\\\nNow go ahead and check the schema that you registered the model. you should see all your updates as follows</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-rg1342y.jpeg\" alt=\"Model registry in Databricks Unity Catalog\" /></p>\n<p>:::tip\n<strong>Model stages</strong>: If you use workspace for model registry you should stages to manage your models. Using aliases won’t work. Check out ==<a href=\"https://docs.databricks.com/en/mlflow/workspace-model-registry-example.html\">here</a>==<a href=\"https://docs.databricks.com/en/mlflow/workspace-model-registry-example.html\"> </a>to see how it works</p>\n<p>:::</p>\n<h2 id=\"modelinference\">Model Inference</h2>\n<h3 id=\"batchscoring\">Batch Scoring</h3>\n<p>Now imagine we want to use our model in production for inference. In this step we load the champion model and use it to generate 20 movie recommendations for each users.</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">from mlflow.spark import load_model as spark_load_model\nfrom mlflow.tracking.client import MlflowClient\nfrom create_training_set import split_data\n\n#-- set UC as model registray \nmlflow.set_registry_uri(\"databricks-uc\")\n\n#-- initate mlflow client\nclient = MlflowClient()\n\n# -- read the config file\nwith open('config.json') as config_file:\n    config = json.load(config_file)\n\ncatalog_name = config[\"catalog_name\"]\ngold_layer = config[\"gold_layer_name\"]\nsilver_layer = config[\"silver_layer_name\"]\nuser_item_table_name = config[\"user_item_table_name\"]\nft_user_item_name = config[\"ft_user_item_name\"]\nmodel_name = config[\"model_name\"]\nmodel_schema = config[\"model_schema\"]\n\n#-- create the model uri\nmodel_path = f\"{catalog_name}.{model_schema}.{model_name}\"\n# --create the model_uri: there are two ways to do this\n# 1: using the alias (we use this*)\nmodel_version_uri = f\"models:/{model_uri}@champion\"\n\n# 2: using model version\n#champion_version = client.get_model_version_by_alias(model_uri, \"champion\")\n#model_version_uri = f\"models:/{model_uri}/{champion_version.version}\"\n\n# -- load the model pipline and exctract the model\nmodel_pipeline = spark_load_model(model_version_uri)\nmodel = model_pipeline.stages[0]\n\n# -- batch scoring using the the model\nfe_full_data, df_full_data, df_train, df_test = split_data()\ndf_batch_input = df_full_data.drop(\"rating\")\ndf_scores = model.transform(df_batch_input)\n\n\n# --- in case you used Feature Engineering to train and register model \n#from databricks.feature_engineering import FeatureEngineeringClient\n#fe = FeatureEngineeringClient()\n# fe.score_batch(model_uri=f\"{model_version_uri}\",df = df_batch_input)\n</code></pre>\n<p>\\\nand you can see we used the same training data for batch scoring. Although in the case of recommender systems it makes sense, in most application we want use the model to score some unseen data. For example, Imaging your are Netflix and want to update the user recommendations at the end of day based on their new watched list. We can schedule job that run the batch scoring at specific time at the end of the day.</p>\n<p>\\\nNow we can go ahead and generate the recommendations for each user. For this we find the top 20 items per users</p>\n<pre><code class=\"python language-python\">from pyspark.sql.window import Window\nfrom pyspark.sql.functions import col, split, row_number, collect_list\nfrom pyspark.sql.functions import col, collect_list, expr, lit, min, row_number, desc\n\n\nwindowSpec = Window.partitionBy(\"user_id\").orderBy(col(\"prediction\").desc())\n\ndf_top_20_items = df_scores.withColumn(\"rank\", row_number().over(windowSpec)).filter(col(\"rank\") &lt;= 20)\n\ndf_user_recs = df_top_20_items.groupBy(\"user_id\") \\\n    .agg(collect_list(col(\"item_id\").cast(\"double\")).alias(\"top_item_ids\"))\n</code></pre>\n<p>\\\nhere is how the result look like</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-j223470.jpeg\" alt=\"\" /></p>\n<p>Finally we can store the prediction as a delta label on our UC or publish them to a downstream systems Mongo DB or Azure Cosmos DB. We go with the firs option</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">df_user_recs.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{output_schema}.top20_item_recommendations\")\n</code></pre>\n<p>\\</p>\n<h3 id=\"streamingonlineinference\">Streaming/Online Inference</h3>\n<p>Now imagine a case in which we want to update our recommendations based on real-time user interactions. For this case we can use model serving.  When someone wants to use your model, they can send data to the server. The server then feeds that data to your deployed model, which goes into action, analyzes the data, and generates a prediction. They can be used in web applications, mobile apps, or even embedded systems. One of the application of this approach is to enable traffic routing for A/B testing.</p>\n<p>\\\nALS algorithm can’t be used directly for online inference since it requires the retraining the model using the whole data (old + new) to update the recommendations. Gradient Descent learning algorithms are examples of model that can be used for online updates. We might look at some of these algorithms in future post.</p>\n<p>\\\nHowever, just to illustrate how such a model would work, we are creating a (useless) model serving endpoint that predict movie rating based whenever a user rate a movies!</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">import requests\n\nmodel_path = f\"{catalog_name}.{model_schema}.{model_name}\"\nchampion_version = client.get_model_version_by_alias(model_path, \"champion\")\n\n# Set the name of the MLflow endpoint\nendpoint_name = config[\"model_serving_endpoint_name\"]\n\n# Name of the registered MLflow model\nmodel_name = model_path\n\n\n# Specify the type of compute (CPU, GPU_SMALL, GPU_MEDIUM, etc.)\nworkload_type = \"CPU\" \n\n# Specify the scale-out size of compute (Small, Medium, Large, etc.)\nworkload_size = \"Small\" \n\n# Get the latest version of the MLflow model\nmodel_version = int(champion_version.version)\n\n\n# Specify Scale to Zero(only supported for CPU endpoints)\nscale_to_zero = False \n\n# Get the API endpoint and token for the current notebook context\nAPI_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get() \nAPI_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\ndata = {\n    \"name\": endpoint_name,\n    \"config\": {\n        \"served_models\": [\n            {\n                \"model_name\": model_name,\n                \"model_version\": int(model_version),\n                \"workload_size\": workload_size,\n                \"scale_to_zero_enabled\": scale_to_zero,\n                \"workload_type\": workload_type,\n            }\n        ]\n    },\n}\n\nheaders = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n\nresponse = requests.post(\n    url=f\"{API_ROOT}/api/2.0/serving-endpoints\", json=data, headers=headers\n)\n</code></pre>\n<p>\\\nThis will create and lunch model serving cluster for us so it takes some time. Now if you open the <code>Serving</code> window you should see your endpoint.</p>\n<p>\\</p>\n<p>:::tip\nwe can use one endpoint to serve multiple model. Then we can use traffic routing for scenarios such as A/B testing or compare the performance of difference models in the production.</p>\n<p>:::</p>\n<h3 id=\"inferencetable\">Inference Table</h3>\n<p>Inference tables in Databricks Model Serving act as an automatic log for our deployed models. When enabled, they capture incoming requests (data sent for prediction), the corresponding model outputs (predictions), and some other metadata as a Delta table within Unity Catalog. We can use inference table for <strong>monitoring and debugging</strong>, <strong>lineage tracking</strong>, and a data collection procedure  for <strong>retraining</strong>  or <strong>fine-tune</strong> our models.</p>\n<p>\\\nWe can enable the <code>inference table</code> on our serving endpoint to monitor the model. We can do it by specifying the <code>auto_capture_config</code> properties in the payload when we first create the endpoint. Or we update our endpoint afterwards using the <code>put</code> command and the <code>config</code> endpoint URL as follows (more ==<a href=\"https://docs.databricks.com/en/machine-learning/model-serving/enable-model-serving-inference-tables.html\">here</a>)==</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">data = {\n        \"served_models\": [\n            {\n                \"model_name\": model_name,\n                \"model_version\": int(model_version),\n                \"workload_size\": workload_size,\n                \"scale_to_zero_enabled\": scale_to_zero,\n                \"workload_type\": workload_type,\n            }\n        ],\n        \"auto_capture_config\":{\n            \"catalog_name\": catalog_name,\n            \"schema_name\": model_schema,\n            \"table_name_prefix\": payload_table,\n    }\n}\n\nheaders = {\"Context-Type\": \"application/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n\nresponse = requests.put(url=f\"{API_ROOT}/api/2.0/serving-endpoints/{endpoint_name}/config\", json=data, headers=headers)\n\n\nprint(json.dumps(response.json(), indent=4))\n</code></pre>\n<p>\\\nnow let’s feed the endpoint with some dummy user interaction data</p>\n<pre><code class=\"python language-python\">import random\nimport time\n\nall_items = df_full_data.select(col(\"item_id\")).distinct()\nfor user_id in range(50,54):\n\n    items_not_rated_by_user = df_full_data.where(col(\"user_id\")==user_id).select(col(\"item_id\")).distinct()#collect()[0][0]\n    no_rated_items =  [item.item_id for item in all_items.subtract(items_not_rated_by_user).limit(4).collect()]\n    data = { \"dataframe_records\": [\n        {\"user_id\":user_id, \"item_id\":no_rated_items[0], \"rating\": random.randint(1, 5)},\n        {\"user_id\":user_id, \"item_id\":no_rated_items[1], \"rating\": random.randint(1, 5)},\n        {\"user_id\":user_id, \"item_id\":no_rated_items[2], \"rating\": random.randint(1, 5)},\n        {\"user_id\":user_id, \"item_id\":no_rated_items[2], \"rating\": random.randint(1, 5)},\n\n        ]\n            }\n    response = requests.post(\n     url=f\"{API_ROOT}/serving-endpoints/{endpoint_name}/invocations\", json=data, headers=headers\n )\n      # generate the data within the timespan of 1 to 8 minutes \n    time.sleep(random.randint(60*1, 60*8))\n</code></pre>\n<p>\\\nWe can check the endpoint logs in the <code>&lt;catalog&gt;.&lt;schema&gt;.&lt;payload_table&gt;</code> table. It takes around 10 minutes until you can see the data in the table.</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">table_path = f\"{catalog_name}.{model_schema}.{payload_table}\"\n# Read data from the inference table\ndf_inf_table = spark.read.table(table_path)\ndisplay(df_inf_table )\n</code></pre>\n<p>\\\nyou should see something like this your payload table</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-t2334jb.jpeg\" alt=\"Databricks model serving payload table\" /></p>\n<p>\\\nTo understand the schema of this inference table, check “Unity catalog inference table schema==” <a href=\"https://docs.databricks.com/en/machine-learning/model-serving/inference-tables.html\">here</a>.==</p>\n<p>\\</p>\n<h2 id=\"modelmonitoring\">Model Monitoring</h2>\n<p>Model and data monitoring a complex topic that requires a lot of time to master. Databricks Lakehouse Monitoring (DLM) reduces the overhead of building a proper monitoring system by providing standard and customizable templates for common use cases. However, mastering  DLM and model monitoring in general requires alot of experimentations.  I don’t want to give you an extensive overview of model monitoring here but rather give you a starting point. I might dedicate a blog to this topic in future.</p>\n<p>\\\n<strong>A short summary of DLM functionalities and features</strong></p>\n<p>Now that we have our model up and running, we can use inference table generated by our serving endpoint to monitor key metrics such a model performance and drift to detect any deviations or anomalies in our data or model over time. This proactive approach help us to take timely corrective actions, such as retraining the model or updating its features, to maintain optimal performance and alignment with business objectives.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-gv4341n.jpeg\" alt=\"Databricks Lakehouse Monitoring Data Architecture source: Databricks\" /></p>\n<p>\\\nDLM provides three type of analysis or <code>profile type</code>: <strong>Time Series</strong>, <strong>Snapshot</strong> and <strong>Inference</strong>. Since we are interested in analyzing our inference table, we focus on the latter one. To use a table for monitoring - our “<strong>primary table</strong>”, we should make sure that the table have the right structure. For the ==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-api.html#inferencelog-profile\">inference table</a>,== each row should correspond to a requests with following columns:</p>\n<ul>\n<li><p><strong>model features</strong></p></li>\n<li><p><strong>model prediction</strong></p></li>\n<li><p><strong>model id</strong></p></li>\n<li><p><strong>timestamp</strong>: timestamp of the inference request</p></li>\n<li><p><strong>ground truth</strong> (optional)</p>\n<p>\\</p></li>\n</ul>\n<p>The <strong>model id</strong> is important for cases when we serve multiple models and we want to track the performance of each model in one monitoring dashboard. If there are more than one model id available, DLM uses it to slice the data and compute metrics and statics for each slice separately.</p>\n<p>\\\nDLM computes each statistics and metrics for a specified time interval. For inference analysis, it used the <strong>timestamp</strong> column, plus a user defined window size to identify the time windows. more below.</p>\n<p>\\\nDLM supports two <code>problem type</code> for inference tables: “<strong>classification</strong>” or “<strong>regression</strong>”. It computes some of the relevant metrics and statistics based on the this specification.</p>\n<p>\\\nTo use DLM, we should create a monitor and attach it to a table. When we do this DLM create two <code>metric tables</code>:</p>\n<ul>\n<li><p><strong>profile metric table</strong>:  this table contains summary statistics such as min, max, percentage of null and zeros. It also contains additional metrics based on the problem type defined by the user. For example  <em>precision</em>, <em>recall</em> and <em>f1<em>score</em> for the classification models, and <em>mean</em>squared<em>error</em>  and <em>mean</em>average_error</em>  for regression models.</p></li>\n<li><p><strong>drift metric table</strong>: it contains statistic that measure how the distribution of data has changed <em>over time</em> or relative to a <em>baseline value (if provided)</em>. It compute measures such as Chi-square test, KS test.</p>\n<p>\\</p></li>\n</ul>\n<p>to see the list of complete metrics for each table check ==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/monitor-output.html\">Monitor metric table</a>== documentation page. It is also possible to create ==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html\">custom metrics</a>.==</p>\n<p>\\\nAn important aspect of building a monitoring system is to make sure that our monitoring dashboard has access to the latest inference data as they arrive. For that we can use ==<a href=\"https://docs.databricks.com/en/structured-streaming/delta-lake.html\">Delta table streaming</a>== to keep track of processed rows in the inference table. We use the model serving’s inference table as our source table (<code>readStream</code>&nbsp;), and the monitoring table as the sink table (<code>writeStream</code>). We also make sure the ==<a href=\"https://www.databricks.com/blog/2018/10/29/simplifying-change-data-capture-with-databricks-delta.html\">Change Data Capture</a>== (CDC) is enabled on both tables (it is enabled by default on the Inference Table).  This way we process only changes - insert/update/delete - in the source table rather than re-processing the entire table every refresh.</p>\n<h3 id=\"handson\">Hands-on</h3>\n<p>To enable the monitoring over our inference table we take the following steps:</p>\n<ol>\n<li>Read the inference table as a Streaming table</li>\n<li>Create a new delta table with the right schema by unpacking the inference table that is generated by our model serving endpoint.</li>\n<li>Prepare the baseline table (if any)</li>\n<li>Create a monitor over the resulting table and refresh the metric</li>\n<li>Schedule a workflow to  unpack the inference table to the right structure and refresh the metrics</li>\n</ol>\n<p>\\\nFirst we need to install the Lakehouse Monitoring API.  It should be already installed if you use Databricks rum time 15.3 LTS and above:</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">%pip install \"https://ml-team-public-read.s3.amazonaws.com/wheels/data-monitoring/a4050ef7-b183-47a1-a145-e614628e3146/databricks_lakehouse_monitoring-0.4.14-py3-none-any.whl\"\ndbutils.library.restartPython()\n</code></pre>\n<p>\\\nLet’s read the inference table as a streaming table</p>\n<pre><code class=\"python language-python\">requests_raw = spark.readStream\\\n        .format(\"delta\")\\\n        .table(inference_table_name)\n\nrequests_raw.isStreaming #-&gt; True \n</code></pre>\n<p>\\\nNext we have to put the table in right format as described above. This table should have one row for each prediction with relevant the features and prediction value. The inference table that we get from the model serving endpoint, store the endpoint requests and responses as a nested JSON format. Here is an example of the JSON payload for the request and response column.</p>\n<pre><code class=\"python language-python\">#requests\n{\"dataframe_records\": [\n            {\"user_id\": 1, \"item_id\": 346, \"rating\": 5},\n            {\"user_id\": 1, \"item_id\": 377, \"rating\": 2},\n          {\"user_id\": 1, \"item_id\": 302, \"rating\": 4}\n        ]\n}\n\n#reponse\n{\"predictions\": \n            [4.248899936676025, 1.1172138452529907, 4.279165744781494]\n}\n\n# --&gt; what we need\n| user_id | item_id | rating | prediction |\n|---------|---------|--------|------------|\n|    1    |   346   |    5   |   4.248900 |\n|    1    |   377   |    2   |   1.117214 |\n|    1    |   302   |    4   |   4.279166 |\n</code></pre>\n<p>\\\nTo unpack this table to the right schema we can use the following code that is adapted from Databricks documentation ==(<a href=\"https://docs.databricks.com/en/_extras/notebooks/source/monitoring/inference-table-monitor.html\">Inference table Lakehouse Monitoring starter notebook</a>).==</p>\n<p>\\</p>\n<pre><code class=\"python language-python\"># define the schema of the request and reponse fields in the inference tabel\nREQUEST_FIELDS = [StructField('user_id', IntegerType(), False),\\\n                    StructField('item_id', IntegerType(), False),\\\n                    StructField('rating', IntegerType(), False)\\\n                ]\n\nRESPONSE_FIELD = [T.StructField(\"predictions\", FloatType())]\n\ndef process_col_requests(json_str:str) -&gt; str:\n    \"\"\"\n    to proccess the JSON payload of request column in inference table\n    \"\"\"\n\n    request = json.loads(json_str)\n    dataframe_records = request.get(\"dataframe_records\", [])\n    return dataframe_records\n\ndef procces_col_response(json_str: str) -&gt; str:\n    \"\"\"\n    to proccess the JSON payload of reponse column in inference table\n    \"\"\"\n    reponse = json.loads(json_str)\n    output = [{prediction_col: round(prediction,4)} for prediction in reponse[\"predictions\"]]\n    return output\n\ndef get_model_id(endpoint_name: str) -&gt; str:\n    \"\"\"\n    create the model id by concatinating the model name and the model version.\n\n    note: the assumption is the endpoint serves only one model \n    \"\"\"\n    served_models = get_served_models(endpoint_name)\n    model_id = f\"{served_models[0]['model_name']}_{served_models[0]['model_version']}\"\n    return model_id\n\ndef process_requests(requests_raw: DataFrame, request_fields: List[T.StructField], response_field: T.StructField, endpoint_name: str) -&gt; DataFrame:\n    \"\"\"\n    Takes a stream of raw requests and processes them by:\n        - Unpacking JSON payloads for requests and responses\n        - Exploding batched requests into individual rows\n        - Converting Unix epoch millisecond timestamps to be Spark TimestampType\n\n    :param requests_raw: DataFrame containing raw requests. Assumed to contain the following columns:\n                            - `request`\n                            - `response`\n                            - `timestamp_ms`\n    :param request_fields: List of StructFields representing the request schema\n    :param response_field: A StructField representing the response schema\n    :return: A DataFrame containing processed requests\n    \"\"\"\n    # Convert the timestamp milliseconds to TimestampType for downstream processing.\n    requests_timestamped = requests_raw \\\n        .withColumn(timestamp_col, (F.col(\"timestamp_ms\") / 1000).cast(T.TimestampType())) \\\n        .drop(\"timestamp_ms\")\n\n    # create the model identifier column\n    model_id = get_model_id(endpoint_name)\n    # Convert the model name and version columns into a model identifier column.\n    requests_identified = requests_timestamped \\\n        .withColumn(model_id_col, F.lit(model_id))\n\n    # Rename the date column to avoid collisions with features.\n    requests_dated = requests_identified.withColumnRenamed(\"date\", date_col)\n\n    # Consolidate and unpack JSON.\n    request_schema = T.ArrayType(T.StructType(request_fields))\n    response_schema = T.ArrayType(T.StructType(response_field))\n    # w\n    udf_request = F.udf(process_col_requests, request_schema)\n    udf_reponse = F.udf(procces_col_response, response_schema)\n\n    requests_unpacked =  requests_dated.withColumn(\"request\", udf_request(\"request\")).\\\n    withColumn(\"response\", udf_reponse(\"response\")) \n\n    # Explode batched requests into individual rows.\n    DB_PREFIX = \"__db\"\n    requests_exploded = requests_unpacked \\\n        .withColumn(f\"{DB_PREFIX}_request_response\", F.arrays_zip(F.col(\"request\"), F.col(\"response\"))) \\\n        .withColumn(f\"{DB_PREFIX}_request_response\", F.explode(F.col(f\"{DB_PREFIX}_request_response\"))) \\\n        .select(F.col(\"*\"), F.col(f\"{DB_PREFIX}_request_response.request.*\"), F.col(f\"{DB_PREFIX}_request_response.response.*\")) \\\n        .drop(f\"{DB_PREFIX}_request_response\", \"request\", \"response\")\n\n    requests_cleaned = requests_exploded.drop(\"status_code\", \"sampling_fraction\", \"client_request_id\", \"databricks_request_id\", \"request_metadata\")\n    return requests_cleaned\n</code></pre>\n<p>\\\nThe resulting table would look like this:</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-eg534jl.jpeg\" alt=\"Payload table unpacked\" /></p>\n<p>Next we should initialize our sink table</p>\n<pre><code class=\"python language-python\">dt_builder = DeltaTable.createIfNotExists(spark) \\\n    .tableName(unpacked_requests_table_name) \\\n    .addColumns(schema) \\\n    .partitionedBy(requests_cleaned.schema) \\\n    .property(\"delta.enableChangeDataFeed\", \"true\") \\\n\n dt_builder.execute()\n</code></pre>\n<p>\\\nand write the results</p>\n<pre><code class=\"python language-python\">checkpoint_path = f\"dbfs:/payload-logging/{endpoint_name}/checkpoint\"\n\nrequests_stream = requests_cleaned.writeStream \\\n    .trigger(once=True) \\\n    .format(\"delta\") \\\n    .partitionBy(date_col) \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", checkpoint_path) \\\n    .toTable(unpacked_requests_table_name) \\\n</code></pre>\n<p>\\\nFinally, we create our baseline table. DLM uses this table to compute the drifts by comparing the distribution of similar columns of baseline and primary models. The baseline table should have the same feature column as the primary column as well as the same model identification column. For baseline table we use the prediction table of our <em>validation dataset</em> that we store earlier after we trained our model using he best hyperparameter. To compute the drift metric, Databricks compute the profile metrics for both primary and the baseline table. Here you can read about the ==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/index.html\">Primary table and baseline table</a>.==</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">#read the prediction table\ndf_base_table = spark.table(f\"{catalog_name}.{model_schema}.predictions\")\n\n# create the model id and add it to the table\nmodel_id = get_model_id(endpoint_name)\ndf_base_table = df_base_table.withColumn(model_id_col, F.lit(model_id))\n\n#write the new table and enable the CDC on it \noutput_base_table_name = f\"{catalog_name}.{model_schema}.{base_table_prefix}_{model_name}\"\ndf_base_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(output_base_table_name)\nspark.sql(f\"ALTER TABLE {output_base_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n</code></pre>\n<p>\\\nNow we are read to create our monitoring dashboard. We can do it either using the ==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-ui.html\">UI</a>==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-ui.html\"> </a>or the Lakehouse Monitoring API. Here we use the second option:</p>\n<pre><code class=\"python language-python\">#  This is where we store the metric tables. \noutput_schema_name = f\"{catalog_name}.{model_schema}\"\n\ntry:\n    info = lm.create_monitor(\n        table_name=unpacked_requests_table_name,\n        profile_type=lm.InferenceLog(\n            timestamp_col=timestamp_col,\n            granularities=granularities,#the aggregation window\n            model_id_col=model_id_col,\n            prediction_col=prediction_col,\n            label_col=label_col,\n            problem_type=problem_type,\n        ),\n        output_schema_name=output_schema_name,\n        schedule=None,  # We will refresh the metrics on-demand in this notebook\n        baseline_table_name=output_base_table_name,\n    )\n    print(info)\nexcept Exception as e:\n    # Ensure the exception was expected\n    assert \"RESOURCE_ALREADY_EXISTS\" in str(e), f\"Unexpected error: {e}\"\n\n    # Update the monitor if any parameters of this notebook have changed.\n    lm.update_monitor(\n        table_name=unpacked_requests_table_name,\n        updated_params=dict(\n            profile_type=lm.InferenceLog(\n                timestamp_col=timestamp_col,\n                granularities=granularities,\n                model_id_col=model_id_col,\n                prediction_col=prediction_col,\n                label_col=label_col,\n                problem_type=problem_type,\n            ),\n            output_schema_name=output_schema_name,\n            schedule=None,\n            baseline_table_name=output_base_table_name,\n        )\n    )\n\n    # Refresh metrics calculated on the requests table.\n    refresh_info = lm.run_refresh(table_name=unpacked_requests_table_name)\n    print(refresh_info)\n</code></pre>\n<p>\\\nafter we run the code it takes some time until Databricks calculate all the metric. To see the dashboard go to the <code>Quality</code> tab of your sink table (i.e. <code>unpacked_requests_table_name</code>). You should see a page as follow.</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-u96344s.jpeg\" alt=\"Databricks Model Monitoring view\" /></p>\n<p>\\\nIf you click on the view <code>refresh history</code> you see your running, pending and past refreshes. click on the <code>View Dashboard</code> to open your dashboard.</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-ds73460.gif.webp\" alt=\"Databricks Model Monitoring Dashboard\" /></p>\n<p>\\\n\\\nso we start with the inference table (<code>my_endpoint_payload</code> ), process it and save the result to <code>my_endpoint_payload_unpacked</code> and pass this table along with our baseline table (<code>base_table_als</code>) to our monitoring API. The DLM compute the profile metrics for each table (<code>my_endpoint_payload_unpacked_profile_metric</code> ) and use the them to compute the drift metrics (<code>my_endpoint_payload_unpacked_drift_metrics</code>)</p>\n<p>\\\nThere you go! you have everything you need to serve and monitor you model!</p>\n<p>\\\nIn the next part I’ll show you how to automate this process using <strong>Databricks Assets Bundle</strong> and <strong>Gitlab</strong>!</p>\n<p>In the ==<a href=\"https://hackernoon.com/mlops-with-databricks-and-spark-part-1\">first part of this tutorial</a>== series, we took the first steps for building an end-to-end MLOps pipeline using Databricks and Spark, guided by Databricks' reference architecture. Here's a recap of the key steps we covered:</p>\n<p>\\</p>\n<ul>\n<li><p><strong>Setting Up the Unity Catalog for Medallion Architecture</strong>: We organized our data into bronze, silver, and gold layers within the Unity Catalog, establishing a structured and efficient data management system.</p></li>\n<li><p><strong>Ingesting Data into Unity Catalog</strong>: We demonstrated how to import raw data into the system, ensuring consistency and quality for subsequent processing stages.</p></li>\n<li><p><strong>Training the Model</strong>: Utilizing Databricks, we trained a machine learning model tailored to our dataset, following best practices for scalable and effective model development.</p></li>\n<li><p><strong>Hyperparameter Tuning with HyperOpt</strong>: To enhance model performance, we employed HyperOpt to automate the search for optimal hyperparameters, improving accuracy and efficiency.</p></li>\n<li><p><strong>Experiment Tracking with Databricks MLflow</strong>: We utilized MLflow to log and monitor our experiments, maintaining a comprehensive record of model versions, metrics, and parameters for easy comparison and reproducibility.</p>\n<p>\\</p></li>\n</ul>\n<p>With these foundational steps completed, your model is now primed for deployment. In this second part, we'll focus on integrating two critical components into our system:</p>\n<p>\\</p>\n<ol>\n<li><strong>Batch Inference</strong>: Implementing batch processing to generate predictions on large datasets, suitable for applications like bulk scoring and periodic reporting.</li>\n<li><strong>Online Inference (Model Serving)</strong>: Setting up real-time model serving to provide immediate predictions, essential for interactive applications and services.</li>\n<li><strong>Model Monitoring:</strong> to ensure your deployed models maintain optimal performance and reliability over time.</li>\n</ol>\n<p>\\\nLet's get into it!</p>\n<h2 id=\"modeldeployment-1\">Model Deployment</h2>\n<p>The departure point of the last blog was model evaluation. Now imagine we did the comparison and found that our model shows a higher performance compare to this production model. As we want (assume) to use the model in production, we want to take advantage of all the data that we have. The next step is to train and test the model using the full dataset. Then persist our model for later use by deploying it as our champion model. Since this is the final model that we want to use for inference, we use the Feature Engineering client to train the model. This way we are not only track the model lineage easier, but also offload the schema validation and feature transformation (if any) to the client.</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">with mlflow.start_run(run_name=\"ALS_best_model\") as run:\n        als = ALS()\n        # Now we set the parameters for the method\n        als.setMaxIter(MAX_ITER)\\\n        .setSeed(SEED)\\\n        .setRegParam(best_params[\"REG_PARAM\"])\\\n        .setUserCol(COL_USER)\\\n        .setItemCol(COL_ITEM)\\\n        .setRatingCol(COL_LABEL)\\\n        .setRank(best_params[\"RANK\"])\n\n        mlflow.log_param(\"MAX_ITER\", MAX_ITER)\n        mlflow.log_param(\"RANK\", best_params[\"RANK\"])\n        mlflow.log_param(\"REG_PARAM\", best_params[\"REG_PARAM\"])\n\n        # Create the model with these parameters.\n        model = als.fit(df_full_data)\n        #drop predictions where users and products from the test test and didn't make it into the training set. in this case, the prediction is NaN\n        model.setColdStartStrategy('drop') \n        predictions = model.transform(df_full_data)\n        signature = infer_signature(model_input = df_full_data, model_output = predictions.select(COL_LABEL))\n\n        #log the model\n        mlflow.spark.log_model(model, model_name,\n                               sample_input=df_full_data.limit(3),\n                               signature = signature,\n                               conda_env=mlflow.spark.get_default_conda_env(),\n                               registered_model_name=f\"{catalog_name}.{model_schema}.{model_name}\")\n\n        evaluator = RegressionEvaluator(predictionCol=COL_PRED, labelCol=COL_LABEL)\n        rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n        mlflow.log_metric('rmse', rmse)\n</code></pre>\n<p>\\\nwe can also use the ==<a href=\"https://docs.databricks.com/en/machine-learning/feature-store/train-models-with-feature-store.html\">Feature Store or Feature Engineering APIs</a>== to train and log the models</p>\n<pre><code class=\"python language-python\">model_info = fe.log_model(model=model, \n                          artifact_path = model_name,\n                          flavor=mlflow.spark,\n                          training_set=fe_full_data,\n                          conda_env=mlflow.spark.get_default_conda_env(),\n                          registered_model_name= f\"{catalog_name}.{model_schema}.{model_name}\"\n                        )\n</code></pre>\n<p>\\\nwhen we use the feature engineering API we can view the model’s lineage in Catalog Explorer</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-ay034p9.jpeg\" alt=\"data lineage in Dataticks Unity Catalog\" /></p>\n<p>\\\nNow lets update the model description and assign a Champion label to it.</p>\n<pre><code class=\"python language-python\">import time\nfrom mlflow.tracking.client import MlflowClient\nfrom mlflow.entities.model_registry.model_version_status import ModelVersionStatus\n\nclient = MlflowClient()\n\n#find the latest model version\nmodel_name_path = f\"{catalog_name}.{model_schema}.{model_name}\"\nmodel_version_infos = client.search_model_versions(f\"name ='{model_name_path}'\")\nnew_model_version = max([int(model_version_info.version) for model_version_info in model_version_infos])\n\n#add the model and model version descirption\nclient.update_registered_model(\n  name=model_name_path,\n  description=\"collaborative filtering using Spark mllib ALS. This model use rating table\"\n)\n\nclient.update_model_version(\n  name=model_name_path,\n  version=new_model_version,\n  description=\"this model is optimized Rank and REG_PARAM with Hyperopt and rmse as a loss function. trained on the full dataset\"\n)\n\n# assign alias\nclient.set_registered_model_alias(model_name_path, \"Champion\", new_model_version)\n</code></pre>\n<p>\\\nNow go ahead and check the schema that you registered the model. you should see all your updates as follows</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-rg1342y.jpeg\" alt=\"Model registry in Databricks Unity Catalog\" /></p>\n<p>:::tip\n<strong>Model stages</strong>: If you use workspace for model registry you should stages to manage your models. Using aliases won’t work. Check out ==<a href=\"https://docs.databricks.com/en/mlflow/workspace-model-registry-example.html\">here</a>==<a href=\"https://docs.databricks.com/en/mlflow/workspace-model-registry-example.html\"> </a>to see how it works</p>\n<p>:::</p>\n<h2 id=\"modelinference-1\">Model Inference</h2>\n<h3 id=\"batchscoring-1\">Batch Scoring</h3>\n<p>Now imagine we want to use our model in production for inference. In this step we load the champion model and use it to generate 20 movie recommendations for each users.</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">from mlflow.spark import load_model as spark_load_model\nfrom mlflow.tracking.client import MlflowClient\nfrom create_training_set import split_data\n\n#-- set UC as model registray \nmlflow.set_registry_uri(\"databricks-uc\")\n\n#-- initate mlflow client\nclient = MlflowClient()\n\n# -- read the config file\nwith open('config.json') as config_file:\n    config = json.load(config_file)\n\ncatalog_name = config[\"catalog_name\"]\ngold_layer = config[\"gold_layer_name\"]\nsilver_layer = config[\"silver_layer_name\"]\nuser_item_table_name = config[\"user_item_table_name\"]\nft_user_item_name = config[\"ft_user_item_name\"]\nmodel_name = config[\"model_name\"]\nmodel_schema = config[\"model_schema\"]\n\n#-- create the model uri\nmodel_path = f\"{catalog_name}.{model_schema}.{model_name}\"\n# --create the model_uri: there are two ways to do this\n# 1: using the alias (we use this*)\nmodel_version_uri = f\"models:/{model_uri}@champion\"\n\n# 2: using model version\n#champion_version = client.get_model_version_by_alias(model_uri, \"champion\")\n#model_version_uri = f\"models:/{model_uri}/{champion_version.version}\"\n\n# -- load the model pipline and exctract the model\nmodel_pipeline = spark_load_model(model_version_uri)\nmodel = model_pipeline.stages[0]\n\n# -- batch scoring using the the model\nfe_full_data, df_full_data, df_train, df_test = split_data()\ndf_batch_input = df_full_data.drop(\"rating\")\ndf_scores = model.transform(df_batch_input)\n\n\n# --- in case you used Feature Engineering to train and register model \n#from databricks.feature_engineering import FeatureEngineeringClient\n#fe = FeatureEngineeringClient()\n# fe.score_batch(model_uri=f\"{model_version_uri}\",df = df_batch_input)\n</code></pre>\n<p>\\\nand you can see we used the same training data for batch scoring. Although in the case of recommender systems it makes sense, in most application we want use the model to score some unseen data. For example, Imaging your are Netflix and want to update the user recommendations at the end of day based on their new watched list. We can schedule job that run the batch scoring at specific time at the end of the day.</p>\n<p>\\\nNow we can go ahead and generate the recommendations for each user. For this we find the top 20 items per users</p>\n<pre><code class=\"python language-python\">from pyspark.sql.window import Window\nfrom pyspark.sql.functions import col, split, row_number, collect_list\nfrom pyspark.sql.functions import col, collect_list, expr, lit, min, row_number, desc\n\n\nwindowSpec = Window.partitionBy(\"user_id\").orderBy(col(\"prediction\").desc())\n\ndf_top_20_items = df_scores.withColumn(\"rank\", row_number().over(windowSpec)).filter(col(\"rank\") &lt;= 20)\n\ndf_user_recs = df_top_20_items.groupBy(\"user_id\") \\\n    .agg(collect_list(col(\"item_id\").cast(\"double\")).alias(\"top_item_ids\"))\n</code></pre>\n<p>\\\nhere is how the result look like</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-j223470.jpeg\" alt=\"\" /></p>\n<p>Finally we can store the prediction as a delta label on our UC or publish them to a downstream systems Mongo DB or Azure Cosmos DB. We go with the firs option</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">df_user_recs.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{output_schema}.top20_item_recommendations\")\n</code></pre>\n<p>\\</p>\n<h3 id=\"streamingonlineinference-1\">Streaming/Online Inference</h3>\n<p>Now imagine a case in which we want to update our recommendations based on real-time user interactions. For this case we can use model serving.  When someone wants to use your model, they can send data to the server. The server then feeds that data to your deployed model, which goes into action, analyzes the data, and generates a prediction. They can be used in web applications, mobile apps, or even embedded systems. One of the application of this approach is to enable traffic routing for A/B testing.</p>\n<p>\\\nALS algorithm can’t be used directly for online inference since it requires the retraining the model using the whole data (old + new) to update the recommendations. Gradient Descent learning algorithms are examples of model that can be used for online updates. We might look at some of these algorithms in future post.</p>\n<p>\\\nHowever, just to illustrate how such a model would work, we are creating a (useless) model serving endpoint that predict movie rating based whenever a user rate a movies!</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">import requests\n\nmodel_path = f\"{catalog_name}.{model_schema}.{model_name}\"\nchampion_version = client.get_model_version_by_alias(model_path, \"champion\")\n\n# Set the name of the MLflow endpoint\nendpoint_name = config[\"model_serving_endpoint_name\"]\n\n# Name of the registered MLflow model\nmodel_name = model_path\n\n\n# Specify the type of compute (CPU, GPU_SMALL, GPU_MEDIUM, etc.)\nworkload_type = \"CPU\" \n\n# Specify the scale-out size of compute (Small, Medium, Large, etc.)\nworkload_size = \"Small\" \n\n# Get the latest version of the MLflow model\nmodel_version = int(champion_version.version)\n\n\n# Specify Scale to Zero(only supported for CPU endpoints)\nscale_to_zero = False \n\n# Get the API endpoint and token for the current notebook context\nAPI_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get() \nAPI_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n\ndata = {\n    \"name\": endpoint_name,\n    \"config\": {\n        \"served_models\": [\n            {\n                \"model_name\": model_name,\n                \"model_version\": int(model_version),\n                \"workload_size\": workload_size,\n                \"scale_to_zero_enabled\": scale_to_zero,\n                \"workload_type\": workload_type,\n            }\n        ]\n    },\n}\n\nheaders = {\"Context-Type\": \"text/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n\nresponse = requests.post(\n    url=f\"{API_ROOT}/api/2.0/serving-endpoints\", json=data, headers=headers\n)\n</code></pre>\n<p>\\\nThis will create and lunch model serving cluster for us so it takes some time. Now if you open the <code>Serving</code> window you should see your endpoint.</p>\n<p>\\</p>\n<p>:::tip\nwe can use one endpoint to serve multiple model. Then we can use traffic routing for scenarios such as A/B testing or compare the performance of difference models in the production.</p>\n<p>:::</p>\n<h3 id=\"inferencetable-1\">Inference Table</h3>\n<p>Inference tables in Databricks Model Serving act as an automatic log for our deployed models. When enabled, they capture incoming requests (data sent for prediction), the corresponding model outputs (predictions), and some other metadata as a Delta table within Unity Catalog. We can use inference table for <strong>monitoring and debugging</strong>, <strong>lineage tracking</strong>, and a data collection procedure  for <strong>retraining</strong>  or <strong>fine-tune</strong> our models.</p>\n<p>\\\nWe can enable the <code>inference table</code> on our serving endpoint to monitor the model. We can do it by specifying the <code>auto_capture_config</code> properties in the payload when we first create the endpoint. Or we update our endpoint afterwards using the <code>put</code> command and the <code>config</code> endpoint URL as follows (more ==<a href=\"https://docs.databricks.com/en/machine-learning/model-serving/enable-model-serving-inference-tables.html\">here</a>)==</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">data = {\n        \"served_models\": [\n            {\n                \"model_name\": model_name,\n                \"model_version\": int(model_version),\n                \"workload_size\": workload_size,\n                \"scale_to_zero_enabled\": scale_to_zero,\n                \"workload_type\": workload_type,\n            }\n        ],\n        \"auto_capture_config\":{\n            \"catalog_name\": catalog_name,\n            \"schema_name\": model_schema,\n            \"table_name_prefix\": payload_table,\n    }\n}\n\nheaders = {\"Context-Type\": \"application/json\", \"Authorization\": f\"Bearer {API_TOKEN}\"}\n\nresponse = requests.put(url=f\"{API_ROOT}/api/2.0/serving-endpoints/{endpoint_name}/config\", json=data, headers=headers)\n\n\nprint(json.dumps(response.json(), indent=4))\n</code></pre>\n<p>\\\nnow let’s feed the endpoint with some dummy user interaction data</p>\n<pre><code class=\"python language-python\">import random\nimport time\n\nall_items = df_full_data.select(col(\"item_id\")).distinct()\nfor user_id in range(50,54):\n\n    items_not_rated_by_user = df_full_data.where(col(\"user_id\")==user_id).select(col(\"item_id\")).distinct()#collect()[0][0]\n    no_rated_items =  [item.item_id for item in all_items.subtract(items_not_rated_by_user).limit(4).collect()]\n    data = { \"dataframe_records\": [\n        {\"user_id\":user_id, \"item_id\":no_rated_items[0], \"rating\": random.randint(1, 5)},\n        {\"user_id\":user_id, \"item_id\":no_rated_items[1], \"rating\": random.randint(1, 5)},\n        {\"user_id\":user_id, \"item_id\":no_rated_items[2], \"rating\": random.randint(1, 5)},\n        {\"user_id\":user_id, \"item_id\":no_rated_items[2], \"rating\": random.randint(1, 5)},\n\n        ]\n            }\n    response = requests.post(\n     url=f\"{API_ROOT}/serving-endpoints/{endpoint_name}/invocations\", json=data, headers=headers\n )\n      # generate the data within the timespan of 1 to 8 minutes \n    time.sleep(random.randint(60*1, 60*8))\n</code></pre>\n<p>\\\nWe can check the endpoint logs in the <code>&lt;catalog&gt;.&lt;schema&gt;.&lt;payload_table&gt;</code> table. It takes around 10 minutes until you can see the data in the table.</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">table_path = f\"{catalog_name}.{model_schema}.{payload_table}\"\n# Read data from the inference table\ndf_inf_table = spark.read.table(table_path)\ndisplay(df_inf_table )\n</code></pre>\n<p>\\\nyou should see something like this your payload table</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-t2334jb.jpeg\" alt=\"Databricks model serving payload table\" /></p>\n<p>\\\nTo understand the schema of this inference table, check “Unity catalog inference table schema==” <a href=\"https://docs.databricks.com/en/machine-learning/model-serving/inference-tables.html\">here</a>.==</p>\n<p>\\</p>\n<h2 id=\"modelmonitoring-1\">Model Monitoring</h2>\n<p>Model and data monitoring a complex topic that requires a lot of time to master. Databricks Lakehouse Monitoring (DLM) reduces the overhead of building a proper monitoring system by providing standard and customizable templates for common use cases. However, mastering  DLM and model monitoring in general requires alot of experimentations.  I don’t want to give you an extensive overview of model monitoring here but rather give you a starting point. I might dedicate a blog to this topic in future.</p>\n<p>\\\n<strong>A short summary of DLM functionalities and features</strong></p>\n<p>Now that we have our model up and running, we can use inference table generated by our serving endpoint to monitor key metrics such a model performance and drift to detect any deviations or anomalies in our data or model over time. This proactive approach help us to take timely corrective actions, such as retraining the model or updating its features, to maintain optimal performance and alignment with business objectives.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-gv4341n.jpeg\" alt=\"Databricks Lakehouse Monitoring Data Architecture source: Databricks\" /></p>\n<p>\\\nDLM provides three type of analysis or <code>profile type</code>: <strong>Time Series</strong>, <strong>Snapshot</strong> and <strong>Inference</strong>. Since we are interested in analyzing our inference table, we focus on the latter one. To use a table for monitoring - our “<strong>primary table</strong>”, we should make sure that the table have the right structure. For the ==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-api.html#inferencelog-profile\">inference table</a>,== each row should correspond to a requests with following columns:</p>\n<ul>\n<li><p><strong>model features</strong></p></li>\n<li><p><strong>model prediction</strong></p></li>\n<li><p><strong>model id</strong></p></li>\n<li><p><strong>timestamp</strong>: timestamp of the inference request</p></li>\n<li><p><strong>ground truth</strong> (optional)</p>\n<p>\\</p></li>\n</ul>\n<p>The <strong>model id</strong> is important for cases when we serve multiple models and we want to track the performance of each model in one monitoring dashboard. If there are more than one model id available, DLM uses it to slice the data and compute metrics and statics for each slice separately.</p>\n<p>\\\nDLM computes each statistics and metrics for a specified time interval. For inference analysis, it used the <strong>timestamp</strong> column, plus a user defined window size to identify the time windows. more below.</p>\n<p>\\\nDLM supports two <code>problem type</code> for inference tables: “<strong>classification</strong>” or “<strong>regression</strong>”. It computes some of the relevant metrics and statistics based on the this specification.</p>\n<p>\\\nTo use DLM, we should create a monitor and attach it to a table. When we do this DLM create two <code>metric tables</code>:</p>\n<ul>\n<li><p><strong>profile metric table</strong>:  this table contains summary statistics such as min, max, percentage of null and zeros. It also contains additional metrics based on the problem type defined by the user. For example  <em>precision</em>, <em>recall</em> and <em>f1<em>score</em> for the classification models, and <em>mean</em>squared<em>error</em>  and <em>mean</em>average_error</em>  for regression models.</p></li>\n<li><p><strong>drift metric table</strong>: it contains statistic that measure how the distribution of data has changed <em>over time</em> or relative to a <em>baseline value (if provided)</em>. It compute measures such as Chi-square test, KS test.</p>\n<p>\\</p></li>\n</ul>\n<p>to see the list of complete metrics for each table check ==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/monitor-output.html\">Monitor metric table</a>== documentation page. It is also possible to create ==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html\">custom metrics</a>.==</p>\n<p>\\\nAn important aspect of building a monitoring system is to make sure that our monitoring dashboard has access to the latest inference data as they arrive. For that we can use ==<a href=\"https://docs.databricks.com/en/structured-streaming/delta-lake.html\">Delta table streaming</a>== to keep track of processed rows in the inference table. We use the model serving’s inference table as our source table (<code>readStream</code>&nbsp;), and the monitoring table as the sink table (<code>writeStream</code>). We also make sure the ==<a href=\"https://www.databricks.com/blog/2018/10/29/simplifying-change-data-capture-with-databricks-delta.html\">Change Data Capture</a>== (CDC) is enabled on both tables (it is enabled by default on the Inference Table).  This way we process only changes - insert/update/delete - in the source table rather than re-processing the entire table every refresh.</p>\n<h3 id=\"handson-1\">Hands-on</h3>\n<p>To enable the monitoring over our inference table we take the following steps:</p>\n<ol>\n<li>Read the inference table as a Streaming table</li>\n<li>Create a new delta table with the right schema by unpacking the inference table that is generated by our model serving endpoint.</li>\n<li>Prepare the baseline table (if any)</li>\n<li>Create a monitor over the resulting table and refresh the metric</li>\n<li>Schedule a workflow to  unpack the inference table to the right structure and refresh the metrics</li>\n</ol>\n<p>\\\nFirst we need to install the Lakehouse Monitoring API.  It should be already installed if you use Databricks rum time 15.3 LTS and above:</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">%pip install \"https://ml-team-public-read.s3.amazonaws.com/wheels/data-monitoring/a4050ef7-b183-47a1-a145-e614628e3146/databricks_lakehouse_monitoring-0.4.14-py3-none-any.whl\"\ndbutils.library.restartPython()\n</code></pre>\n<p>\\\nLet’s read the inference table as a streaming table</p>\n<pre><code class=\"python language-python\">requests_raw = spark.readStream\\\n        .format(\"delta\")\\\n        .table(inference_table_name)\n\nrequests_raw.isStreaming #-&gt; True \n</code></pre>\n<p>\\\nNext we have to put the table in right format as described above. This table should have one row for each prediction with relevant the features and prediction value. The inference table that we get from the model serving endpoint, store the endpoint requests and responses as a nested JSON format. Here is an example of the JSON payload for the request and response column.</p>\n<pre><code class=\"python language-python\">#requests\n{\"dataframe_records\": [\n            {\"user_id\": 1, \"item_id\": 346, \"rating\": 5},\n            {\"user_id\": 1, \"item_id\": 377, \"rating\": 2},\n          {\"user_id\": 1, \"item_id\": 302, \"rating\": 4}\n        ]\n}\n\n#reponse\n{\"predictions\": \n            [4.248899936676025, 1.1172138452529907, 4.279165744781494]\n}\n\n# --&gt; what we need\n| user_id | item_id | rating | prediction |\n|---------|---------|--------|------------|\n|    1    |   346   |    5   |   4.248900 |\n|    1    |   377   |    2   |   1.117214 |\n|    1    |   302   |    4   |   4.279166 |\n</code></pre>\n<p>\\\nTo unpack this table to the right schema we can use the following code that is adapted from Databricks documentation ==(<a href=\"https://docs.databricks.com/en/_extras/notebooks/source/monitoring/inference-table-monitor.html\">Inference table Lakehouse Monitoring starter notebook</a>).==</p>\n<p>\\</p>\n<pre><code class=\"python language-python\"># define the schema of the request and reponse fields in the inference tabel\nREQUEST_FIELDS = [StructField('user_id', IntegerType(), False),\\\n                    StructField('item_id', IntegerType(), False),\\\n                    StructField('rating', IntegerType(), False)\\\n                ]\n\nRESPONSE_FIELD = [T.StructField(\"predictions\", FloatType())]\n\ndef process_col_requests(json_str:str) -&gt; str:\n    \"\"\"\n    to proccess the JSON payload of request column in inference table\n    \"\"\"\n\n    request = json.loads(json_str)\n    dataframe_records = request.get(\"dataframe_records\", [])\n    return dataframe_records\n\ndef procces_col_response(json_str: str) -&gt; str:\n    \"\"\"\n    to proccess the JSON payload of reponse column in inference table\n    \"\"\"\n    reponse = json.loads(json_str)\n    output = [{prediction_col: round(prediction,4)} for prediction in reponse[\"predictions\"]]\n    return output\n\ndef get_model_id(endpoint_name: str) -&gt; str:\n    \"\"\"\n    create the model id by concatinating the model name and the model version.\n\n    note: the assumption is the endpoint serves only one model \n    \"\"\"\n    served_models = get_served_models(endpoint_name)\n    model_id = f\"{served_models[0]['model_name']}_{served_models[0]['model_version']}\"\n    return model_id\n\ndef process_requests(requests_raw: DataFrame, request_fields: List[T.StructField], response_field: T.StructField, endpoint_name: str) -&gt; DataFrame:\n    \"\"\"\n    Takes a stream of raw requests and processes them by:\n        - Unpacking JSON payloads for requests and responses\n        - Exploding batched requests into individual rows\n        - Converting Unix epoch millisecond timestamps to be Spark TimestampType\n\n    :param requests_raw: DataFrame containing raw requests. Assumed to contain the following columns:\n                            - `request`\n                            - `response`\n                            - `timestamp_ms`\n    :param request_fields: List of StructFields representing the request schema\n    :param response_field: A StructField representing the response schema\n    :return: A DataFrame containing processed requests\n    \"\"\"\n    # Convert the timestamp milliseconds to TimestampType for downstream processing.\n    requests_timestamped = requests_raw \\\n        .withColumn(timestamp_col, (F.col(\"timestamp_ms\") / 1000).cast(T.TimestampType())) \\\n        .drop(\"timestamp_ms\")\n\n    # create the model identifier column\n    model_id = get_model_id(endpoint_name)\n    # Convert the model name and version columns into a model identifier column.\n    requests_identified = requests_timestamped \\\n        .withColumn(model_id_col, F.lit(model_id))\n\n    # Rename the date column to avoid collisions with features.\n    requests_dated = requests_identified.withColumnRenamed(\"date\", date_col)\n\n    # Consolidate and unpack JSON.\n    request_schema = T.ArrayType(T.StructType(request_fields))\n    response_schema = T.ArrayType(T.StructType(response_field))\n    # w\n    udf_request = F.udf(process_col_requests, request_schema)\n    udf_reponse = F.udf(procces_col_response, response_schema)\n\n    requests_unpacked =  requests_dated.withColumn(\"request\", udf_request(\"request\")).\\\n    withColumn(\"response\", udf_reponse(\"response\")) \n\n    # Explode batched requests into individual rows.\n    DB_PREFIX = \"__db\"\n    requests_exploded = requests_unpacked \\\n        .withColumn(f\"{DB_PREFIX}_request_response\", F.arrays_zip(F.col(\"request\"), F.col(\"response\"))) \\\n        .withColumn(f\"{DB_PREFIX}_request_response\", F.explode(F.col(f\"{DB_PREFIX}_request_response\"))) \\\n        .select(F.col(\"*\"), F.col(f\"{DB_PREFIX}_request_response.request.*\"), F.col(f\"{DB_PREFIX}_request_response.response.*\")) \\\n        .drop(f\"{DB_PREFIX}_request_response\", \"request\", \"response\")\n\n    requests_cleaned = requests_exploded.drop(\"status_code\", \"sampling_fraction\", \"client_request_id\", \"databricks_request_id\", \"request_metadata\")\n    return requests_cleaned\n</code></pre>\n<p>\\\nThe resulting table would look like this:</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-eg534jl.jpeg\" alt=\"Payload table unpacked\" /></p>\n<p>Next we should initialize our sink table</p>\n<pre><code class=\"python language-python\">dt_builder = DeltaTable.createIfNotExists(spark) \\\n    .tableName(unpacked_requests_table_name) \\\n    .addColumns(schema) \\\n    .partitionedBy(requests_cleaned.schema) \\\n    .property(\"delta.enableChangeDataFeed\", \"true\") \\\n\n dt_builder.execute()\n</code></pre>\n<p>\\\nand write the results</p>\n<pre><code class=\"python language-python\">checkpoint_path = f\"dbfs:/payload-logging/{endpoint_name}/checkpoint\"\n\nrequests_stream = requests_cleaned.writeStream \\\n    .trigger(once=True) \\\n    .format(\"delta\") \\\n    .partitionBy(date_col) \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", checkpoint_path) \\\n    .toTable(unpacked_requests_table_name) \\\n</code></pre>\n<p>\\\nFinally, we create our baseline table. DLM uses this table to compute the drifts by comparing the distribution of similar columns of baseline and primary models. The baseline table should have the same feature column as the primary column as well as the same model identification column. For baseline table we use the prediction table of our <em>validation dataset</em> that we store earlier after we trained our model using he best hyperparameter. To compute the drift metric, Databricks compute the profile metrics for both primary and the baseline table. Here you can read about the ==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/index.html\">Primary table and baseline table</a>.==</p>\n<p>\\</p>\n<pre><code class=\"python language-python\">#read the prediction table\ndf_base_table = spark.table(f\"{catalog_name}.{model_schema}.predictions\")\n\n# create the model id and add it to the table\nmodel_id = get_model_id(endpoint_name)\ndf_base_table = df_base_table.withColumn(model_id_col, F.lit(model_id))\n\n#write the new table and enable the CDC on it \noutput_base_table_name = f\"{catalog_name}.{model_schema}.{base_table_prefix}_{model_name}\"\ndf_base_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(output_base_table_name)\nspark.sql(f\"ALTER TABLE {output_base_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n</code></pre>\n<p>\\\nNow we are read to create our monitoring dashboard. We can do it either using the ==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-ui.html\">UI</a>==<a href=\"https://docs.databricks.com/en/lakehouse-monitoring/create-monitor-ui.html\"> </a>or the Lakehouse Monitoring API. Here we use the second option:</p>\n<pre><code class=\"python language-python\">#  This is where we store the metric tables. \noutput_schema_name = f\"{catalog_name}.{model_schema}\"\n\ntry:\n    info = lm.create_monitor(\n        table_name=unpacked_requests_table_name,\n        profile_type=lm.InferenceLog(\n            timestamp_col=timestamp_col,\n            granularities=granularities,#the aggregation window\n            model_id_col=model_id_col,\n            prediction_col=prediction_col,\n            label_col=label_col,\n            problem_type=problem_type,\n        ),\n        output_schema_name=output_schema_name,\n        schedule=None,  # We will refresh the metrics on-demand in this notebook\n        baseline_table_name=output_base_table_name,\n    )\n    print(info)\nexcept Exception as e:\n    # Ensure the exception was expected\n    assert \"RESOURCE_ALREADY_EXISTS\" in str(e), f\"Unexpected error: {e}\"\n\n    # Update the monitor if any parameters of this notebook have changed.\n    lm.update_monitor(\n        table_name=unpacked_requests_table_name,\n        updated_params=dict(\n            profile_type=lm.InferenceLog(\n                timestamp_col=timestamp_col,\n                granularities=granularities,\n                model_id_col=model_id_col,\n                prediction_col=prediction_col,\n                label_col=label_col,\n                problem_type=problem_type,\n            ),\n            output_schema_name=output_schema_name,\n            schedule=None,\n            baseline_table_name=output_base_table_name,\n        )\n    )\n\n    # Refresh metrics calculated on the requests table.\n    refresh_info = lm.run_refresh(table_name=unpacked_requests_table_name)\n    print(refresh_info)\n</code></pre>\n<p>\\\nafter we run the code it takes some time until Databricks calculate all the metric. To see the dashboard go to the <code>Quality</code> tab of your sink table (i.e. <code>unpacked_requests_table_name</code>). You should see a page as follow.</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-u96344s.jpeg\" alt=\"Databricks Model Monitoring view\" /></p>\n<p>\\\nIf you click on the view <code>refresh history</code> you see your running, pending and past refreshes. click on the <code>View Dashboard</code> to open your dashboard.</p>\n<p><img src=\"https://cdn.hackernoon.com/images/a98nFEjuohShlIKwuarlXIkY1ID2-ds73460.gif.webp\" alt=\"Databricks Model Monitoring Dashboard\" /></p>\n<p>\\\n\\\nso we start with the inference table (<code>my_endpoint_payload</code> ), process it and save the result to <code>my_endpoint_payload_unpacked</code> and pass this table along with our baseline table (<code>base_table_als</code>) to our monitoring API. The DLM compute the profile metrics for each table (<code>my_endpoint_payload_unpacked_profile_metric</code> ) and use the them to compute the drift metrics (<code>my_endpoint_payload_unpacked_drift_metrics</code>)</p>\n<p>\\\nThere you go! you have everything you need to serve and monitor you model!</p>\n<p>\\\nIn the next part I’ll show you how to automate this process using <strong>Databricks Assets Bundle</strong> and <strong>Gitlab</strong>!</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The Top 7 Robotics Stories of 2024","url":"https://spectrum.ieee.org/top-robotics-stories-2024","date":1735480802,"author":"Evan Ackerman","unread":true,"desc":"","content":"<p>A new Atlas, Figure's bonkers funding round, and the last voyage of a helicopter on Mars</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4MzY0OC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4NTQ4MzExMX0.3bca1xoiyaOOgYmcqPZ7K3Izu2FimblAkRoTgP96l7M/image.jpg?width=600","enclosureMime":""},{"title":"Permira’s Brian Ruder talks AI, Squarespace acquisition, and the value of co-leadership","url":"https://techcrunch.com/2024/12/29/permiras-brian-ruder-talks-ai-squarespace-acquisition-and-the-value-of-co-leadership/","date":1735480800,"author":"Paul Sawers","unread":true,"desc":"","content":"<p>It has been a busy year in the private equity realm, with countless big-money acquisitions unfolding. The take-private space specifically has seen some sizable transactions, with private equity firms spearheading more than a dozen billion-dollar deals for public tech companies. London-headquartered Permira was a key protagonist, joining Blackstone to acquire European online classifieds group Adevinta [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Anger at Health Insurance Prompts the Public to Fund a 9-Year-Old's Bionic Arm","url":"https://science.slashdot.org/story/24/12/29/0213248/anger-at-health-insurance-prompts-the-public-to-fund-a-9-year-olds-bionic-arm?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735475640,"author":"EditorDavid","unread":true,"desc":"","content":"A 9-year-old girl born without a left hand had \"started asking for a robotic arm to help her feel more confident,\" her mother told the Washington Post. So her parents met with a consultant from Open Bionics, which fits people with lightweight, 3D-printed prostheses that function more like a natural arm and hand &mdash; known as Hero Arms.\n\n\nThe bionic arms are manufactured in Britain and cost about $24,000, but the Batemans were hopeful that their health insurance company, Select Health, would pay for one for [their 9-year-old daughter] Remi. Remi said she tried using one of the robotic arms for a few days in Colorado and was thrilled to cut her food with a knife and fork for the first time and carry plates with two hands. \"I loved it so much &mdash; I could function like a full human,\" she said. \"I was able to steal my dad's hat. When they fit me for my arm, I told them I wanted it to be pink.\" \n\nOn Oct. 1, the Batemans sent a prescription for the robotic arm and office notes from Remi's pediatrician to Select Health for approval. One week later, their request was denied, Jami Bateman said. \"They sent us a letter saying it was not medically necessary for Remi to have a Hero Arm and that it was for cosmetic use only,\" she said. \"We appealed twice and were again denied.\" \n\n\"It was very upsetting, and Remi cried when I told her, because we'd all been so hopeful,\" Bateman added. \"It broke our hearts.\" In mid-December, a frustrated Jami Bateman tried an approach she'd seen other people use when their health insurance failed them: She started a GoFundMe for her daughter, hoping to purchase a robotic arm through the kindness of strangers.... Bateman was stunned when friends and strangers chipped in more than $30,000 in just a few days, surpassing the family's $24,000 goal. People who donated understood the Batemans' predicament, and many were furious on their behalf. \n\nAs donations poured in, the Batemans received a call from somebody else who wanted to help. Andy Schoonover is the CEO of CrowdHealth, a subscriber-based resource that helps people negotiate lower costs for medical bills. He told the family on Dec. 16 that his company wanted to pay the entire cost of Remi's bionic arm. \"We were looking for some ways to help people during the holiday season, and I stumbled upon Remi's story on social media,\" Schoonover said. \"We were honored to help her out....\" \n\nRemi quickly came up with an idea. \"She came to me and said, 'Mom, I know how it feels to have one hand. Is there someone else we can help?\" Bateman recalled. She said she contacted Open Bionics and learned there was a long list of children who had been turned down for Hero Arms by their health insurance companies for the same reason Remi was denied... \n\nSomewhere in Maryland, the mother of a 9-year-old boy born without a left hand suddenly got a surprise phone call explaining Remi's decision. \"I was so proud of Remi that I immediately started crying,\" she said. \"She wanted to give my son an opportunity that I was unable to give him. It just touched my heart.\" \n\nThey had been trying to raise money by running a lemonade stand. But yesterday Remi's GoFundMe page posted an update. The 9-year-old boy's arm had now been paid for. \n\n\"And maybe, if more donations roll in we can help a third child!\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Anger+at+Health+Insurance+Prompts+the+Public+to+Fund+a+9-Year-Old's+Bionic+Arm%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F0213248%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F0213248%2Fanger-at-health-insurance-prompts-the-public-to-fund-a-9-year-olds-bionic-arm%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/29/0213248/anger-at-health-insurance-prompts-the-public-to-fund-a-9-year-olds-bionic-arm?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563755&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Ubuntu's Great Year From 24.04 LTS To Focusing More On Performance Optimizations","url":"https://www.phoronix.com/news/Ubuntu-2024-Great-Year","date":1735473669,"author":"Michael Larabel","unread":true,"desc":"","content":"From my independent monitoring, Ubuntu Linux had a pretty great year. Ubuntu 24.04 LTS shipped and has been well received across enterprises, Canonical engineers have been focusing more on performance optimizations for Ubuntu, and there has been other interesting changes like their new commitment to always ship the latest upstream Linux kernel version as of Ubuntu release time. Plus they have continued with various GNOME desktop improvements, Ubuntu on servers continues with steady traction, and all-around was a pretty exciting year for the Ubuntu camp...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Fish Shell Outlines Their Successes & Challenges Going From C++ To Rust","url":"https://www.phoronix.com/news/Fish-Shell-Rust-Challenges","date":1735473213,"author":"Michael Larabel","unread":true,"desc":"","content":"Earlier this month the Fish Shell 4.0 went into beta with the C++ code ported to Rust. Now with most of the Fish Shell code transitioned to Rust, the project put out a blog post this weekend outlining the successes and challenges they have encountered in porting their large C++ codebase to Rust...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Apple DWI Backlight Linux Driver Updated For Various iPhones, iPods & iPads","url":"https://www.phoronix.com/news/Apple-DWI-Backlight-Linux-v4","date":1735472940,"author":"Michael Larabel","unread":true,"desc":"","content":"While Linux 6.13 is introducing basic support for various Apple iPads and iPhones using A-series SoCs, the support is just that: basic. Various feature limitations remain for those dreaming over the prospects of running Linux on older Apple mobile devices. One of various feature limitations remaining are around backlight control for different models and for that there is the Apple DWI backlight driver for Linux that continues to be hacked on...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Linux 6.13-rc5 To See Fix For Intel TDX CoCo VMs Potentially Leaking Decrypted Memory","url":"https://www.phoronix.com/news/Linux-6.13-Fixing-TDX-CoCo-Leak","date":1735471447,"author":"Michael Larabel","unread":true,"desc":"","content":"The x86 fixes pull request was sent out this morning ahead of the Linux 6.13-rc5 kernel being released later today. Both x86 fixes this week pertain to Intel bits: a self-test issue on upcoming Intel FRED (Flexible Return and Event Delivery) systems and also an issue of Intel TDX confidential computing VM guests potentially leaking decrypted memory within the unrecoverable error handling...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The TechBeat: RootstockCollective In-Depth: Empowering Bitcoin Builders (12/29/2024)","url":"https://hackernoon.com/12-29-2024-techbeat?source=rss","date":1735456253,"author":"Techbeat","unread":true,"desc":"","content":"<p>How are you, hacker? \n 🪐<strong>Want to know what's trending right now?:</strong>\n <a href=\"https://hackernoon.com/homepage-has-a-new-baby\">The Techbeat by HackerNoon </a> has got you covered with fresh content from our trending stories of the day! Set email preference <a href=\"https://app.hackernoon.com/profile/email-settings\">here</a>.\n ## <strong><a href=\"https://hackernoon.com/rootstockcollective-in-depth-empowering-bitcoin-builders\">RootstockCollective In-Depth: Empowering Bitcoin Builders</a></strong> <img src=\"https://cdn.hackernoon.com/images/InxBRjRIs6M1kdhuWcyNHiiUrxm1-j5034qi.png\" alt=\"\" />\n By <a href=\"https://hackernoon.com/u/rootstock_io\">@rootstock_io</a> [ 7 Min read ] \n Empowering Bitcoin builders with RootstockCollective DAO: Rewarding innovation, stakers, and developers in the Bitcoin sidechain ecosystem. <a href=\"https://hackernoon.com/rootstockcollective-in-depth-empowering-bitcoin-builders\">Read More.</a></p>\n<h2 id=\"joinlumozzkverifiernodeminingandshare25billionmozrewardshttpshackernooncomjoinlumozzkverifiernodeminingandshare25billionmozrewardshttpscdnhackernooncomimageswls6ttjolgmbl8akwqlibcyfjqf2cx03ymqpng\"><strong><a href=\"https://hackernoon.com/join-lumoz-zkverifier-node-mining-and-share-25-billion-moz-rewards\">Join Lumoz zkVerifier Node Mining and Share 2.5 Billion MOZ Rewards</a></strong> <img src=\"https://cdn.hackernoon.com/images/Wls6TtjOLGMbl8aKwQlIbcyfjQF2-cx03ymq.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/lumoz\">@lumoz</a> [ 4 Min read ] \n Lumoz Node Network &amp; MOZ staking are live! Join the zkVerifier network, stake MOZ, or run nodes to share 25% of $90M in rewards. Act now! <a href=\"https://hackernoon.com/join-lumoz-zkverifier-node-mining-and-share-25-billion-moz-rewards\">Read More.</a></p>\n<h2 id=\"quicklybulkloadimagetoecommercesiteswiththisguidehttpshackernooncomquicklybulkloadimagetoecommercesiteswiththisguidehttpscdnhackernooncomimages0sm1kikmbzhu9ad2gar4ckywon439l035i7png\"><strong><a href=\"https://hackernoon.com/quickly-bulk-load-image-to-e-commerce-sites-with-this-guide\">Quickly Bulk Load Image to E-commerce Sites With This Guide</a></strong> <img src=\"https://cdn.hackernoon.com/images/0sm1KIKmbZhu9AD2GAr4cKywoN43-9l035i7.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/filestack\">@filestack</a> [ 10 Min read ] \n Manually updating each image one by one is not only time-consuming but also inefficient. This is where bulk quick image upload is helpful. <a href=\"https://hackernoon.com/quickly-bulk-load-image-to-e-commerce-sites-with-this-guide\">Read More.</a></p>\n<h2 id=\"thestanfordgradwhoforgothowtothinkhttpshackernooncomthestanfordgradwhoforgothowtothinkhttpscdnhackernooncomimagesarobotperformingbrainsurgeryzdwlilmtvqn3wi11haupx3bepng\"><strong><a href=\"https://hackernoon.com/the-stanford-grad-who-forgot-how-to-think\">The Stanford Grad Who Forgot How To Think</a></strong> <img src=\"https://cdn.hackernoon.com/images/a-robot-performing-brain-surgery-zdwlilmtvqn3wi11haupx3be.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/scottdclary\">@scottdclary</a> [ 8 Min read ] \n Overusing AI like ChatGPT is reshaping human cognition. Learn how to maintain your brain’s independence and avoid the hidden costs of digital dependence.</p>\n<ol start=\"4\">\n<li>TL <a href=\"https://hackernoon.com/the-stanford-grad-who-forgot-how-to-think\">Read More.</a></li>\n</ol>\n<h2 id=\"thecryptoindustryisoverlookingacriticalfactorinensuringitssuccesshttpshackernooncomthecryptoindustryisoverlookingacriticalfactorinensuringitssuccesshttpscdnhackernooncomimages58y6hvcgyjoqohikudze6tnxncn2u0039u8jpeg\"><strong><a href=\"https://hackernoon.com/the-crypto-industry-is-overlooking-a-critical-factor-in-ensuring-its-success\">The Crypto Industry is Overlooking a Critical Factor in Ensuring Its Success</a></strong> <img src=\"https://cdn.hackernoon.com/images/58y6HVCGyJOQoHIkUdze6TnxnCN2-u0039u8.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/kategrizik\">@kategrizik</a> [ 4 Min read ] \n To turn people into cryptocurrency adopters, it’s not enough to just develop technology and use traditional marketing techniques. <a href=\"https://hackernoon.com/the-crypto-industry-is-overlooking-a-critical-factor-in-ensuring-its-success\">Read More.</a></p>\n<h2 id=\"segapromisestotakeontekkenwithvirtuafighterannouncementhttpshackernooncomsegapromisestotakeontekkenwithvirtuafighterannouncementhttpscdnhackernooncomimages9iwtzqgrqiuqi3nricdxbir6rcn1uw036pyjpeg\"><strong><a href=\"https://hackernoon.com/sega-promises-to-take-on-tekken-with-virtua-fighter-announcement\">Sega Promises to Take on Tekken With Virtua Fighter Announcement</a></strong> <img src=\"https://cdn.hackernoon.com/images/9IwtzQGRqIUqi3nriCDxBIR6rCn1-uw036py.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/jabrilgoodner\">@jabrilgoodner</a> [ 3 Min read ] \n Sega promises to return Virtua Fighter to the fighting game space.\n <a href=\"https://hackernoon.com/sega-promises-to-take-on-tekken-with-virtua-fighter-announcement\">Read More.</a></p>\n<h2 id=\"isanthropicsalignmentfakingasignificantaisafetyresearchhttpshackernooncomisanthropicsalignmentfakingasignificantaisafetyresearchhttpscdnhackernooncomimagesc5rkpn8owpnzalu41ht0jwmb4pw1yo03p2bjpeg\"><strong><a href=\"https://hackernoon.com/is-anthropics-alignment-faking-a-significant-ai-safety-research\">Is Anthropic's Alignment Faking a Significant AI Safety Research?</a></strong> <img src=\"https://cdn.hackernoon.com/images/c5rKpn8oWPNzALU41hT0JwMb4pw1-yo03p2b.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/step\">@step</a> [ 4 Min read ] \n How the mind works [of human and of AI] is not by labels, like induction or deduction, but by components, their interactions, and features.  <a href=\"https://hackernoon.com/is-anthropics-alignment-faking-a-significant-ai-safety-research\">Read More.</a></p>\n<h2 id=\"thedevelopersguidetobuildinglinuxkernel6xhttpshackernooncomthedevelopersguidetobuildinglinuxkernel6xhttpscdnhackernooncomimagesrzmce2upbzcfw27emzbfuxkv7tv1je02u45jpeg\"><strong><a href=\"https://hackernoon.com/the-developers-guide-to-building-linux-kernel-6x\">The Developer's Guide to Building Linux Kernel 6.x</a></strong> <img src=\"https://cdn.hackernoon.com/images/rZMCE2UpBzcfW27eMzbfUXKV7Tv1-je02u45.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/atulthosar\">@atulthosar</a> [ 3 Min read ] \n This blog post explains how to build the linux kernel version 6.x for x86_64 platforms. <a href=\"https://hackernoon.com/the-developers-guide-to-building-linux-kernel-6x\">Read More.</a></p>\n<h2 id=\"programmablebitcoinishereaturingcompletebridgelessbitcoinexecutionlayerhttpshackernooncomprogrammablebitcoinishereaturingcompletebridgelessbitcoinexecutionlayerhttpscdnhackernooncomimagesmkz8c4ofl4mny75bkkxixwonmju1ba035mkpng\"><strong><a href=\"https://hackernoon.com/programmable-bitcoin-is-here-a-turing-complete-bridgeless-bitcoin-execution-layer\">Programmable Bitcoin Is Here: A Turing-complete Bridgeless Bitcoin Execution Layer</a></strong> <img src=\"https://cdn.hackernoon.com/images/MkZ8C4ofL4Mny75bKkXIXWoNMJu1-ba035mk.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/omnity\">@omnity</a> [ 9 Min read ] \n The Runes Exchange Environment (REE) is a decentralized execution layer on Bitcoin for builders to create BTCFi exchanges without forks, bridges, or opcodes. <a href=\"https://hackernoon.com/programmable-bitcoin-is-here-a-turing-complete-bridgeless-bitcoin-execution-layer\">Read More.</a></p>\n<h2 id=\"harnessingsharedsecurityforsecurecrosschaininteroperabilityhttpshackernooncomharnessingsharedsecurityforsecurecrosschaininteroperabilityhttpscdnhackernooncomimageswsqjfcsxoxwphtnq7snevvhdwgu1l903680jpeg\"><strong><a href=\"https://hackernoon.com/harnessing-shared-security-for-secure-cross-chain-interoperability\">Harnessing Shared Security For Secure Cross-Chain Interoperability</a></strong> <img src=\"https://cdn.hackernoon.com/images/WSQJfCSXOxWphTNQ7sneVvhdWGu1-l903680.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/2077research\">@2077research</a> [ 47 Min read ] \n A deep dive on shared security and the role of shared security infrastructure in building robust and secure cross-chain interoperability solutions for users. <a href=\"https://hackernoon.com/harnessing-shared-security-for-secure-cross-chain-interoperability\">Read More.</a></p>\n<h2 id=\"thecyberresilienceactafieldguideforctosandcisoshttpshackernooncomthecyberresilienceactafieldguideforctosandcisoshttpscdnhackernooncomimagesrhanbxrxjsyoximtykjflecfjyc3ln1340wjpeg\"><strong><a href=\"https://hackernoon.com/the-cyber-resilience-act-a-field-guide-for-ctos-and-cisos\">The Cyber Resilience Act: A Field Guide for CTOs and CISOs</a></strong> <img src=\"https://cdn.hackernoon.com/images/RHANbxrXjsYoxIMTyKJFleCFJyC3-ln1340w.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/salkimmich\">@salkimmich</a> [ 9 Min read ] \n The Cyber Resilience Act mandates cybersecurity for digital products in the EU. Essential for CTOs &amp; CISOs, it defines secure by design and compliance strategy. <a href=\"https://hackernoon.com/the-cyber-resilience-act-a-field-guide-for-ctos-and-cisos\">Read More.</a></p>\n<h2 id=\"netflixfinallycomesforsportsstreaminghttpshackernooncomnetflixfinallycomesforsportsstreaminghttpscdnhackernooncomimagesntolwvce2kpryow11aq31ts22ky19b03dj5jpeg\"><strong><a href=\"https://hackernoon.com/netflix-finally-comes-for-sports-streaming\">Netflix Finally Comes For Sports Streaming</a></strong> <img src=\"https://cdn.hackernoon.com/images/nTolwVCe2KPryOw11aq31tS22Ky1-9b03dj5.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/davidjdeal\">@davidjdeal</a> [ 4 Min read ] \n The company did not make a serious stab at sports until 2023 with the airing of the Netflix Cup, a novelty golf event. <a href=\"https://hackernoon.com/netflix-finally-comes-for-sports-streaming\">Read More.</a></p>\n<h2 id=\"howtoincreaseyourintelligenceevenifyourenotgeneticallygiftedhttpshackernooncomhowtoincreaseyourintelligenceevenifyourenotgeneticallygiftedhttpscdnhackernooncomimagesx21vpriqhyayrjebiymkn7uuoth2gq0360cjpeg\"><strong><a href=\"https://hackernoon.com/how-to-increase-your-intelligence-even-if-youre-not-genetically-gifted\">How to Increase Your Intelligence (even if You're Not Genetically Gifted)</a></strong> <img src=\"https://cdn.hackernoon.com/images/x21VprIQHYaYrJEbiyMkN7uuOTH2-gq0360c.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/praisejames\">@praisejames</a> [ 4 Min read ] \n The true test of intelligence is getting what you want out of life. <a href=\"https://hackernoon.com/how-to-increase-your-intelligence-even-if-youre-not-genetically-gifted\">Read More.</a></p>\n<h2 id=\"psanvidiasaitrainingcoursesarefreerightnowforalimitedtimeonlyhttpshackernooncompsanvidiasaitrainingcoursesarefreerightnowforalimitedtimeonlyhttpscdnhackernooncomimagesibh9rmtufzxwh5bugcm1d400bu73av139gtjpeg\"><strong><a href=\"https://hackernoon.com/psa-nvidias-ai-training-courses-are-free-right-now-for-a-limited-time-only\">PSA: Nvidia's AI Training Courses Are Free Right Now For a Limited Time Only</a></strong> <img src=\"https://cdn.hackernoon.com/images/iBh9rMtuFZXWH5bUGCM1D400BU73-av139gt.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/giorgiofazio\">@giorgiofazio</a> [ 4 Min read ] \n Don’t miss out! NVIDIA is offering premium AI courses worth up to $90 for free, but only for a limited time. Learn, innovate, and grow today! <a href=\"https://hackernoon.com/psa-nvidias-ai-training-courses-are-free-right-now-for-a-limited-time-only\">Read More.</a></p>\n<h2 id=\"thesenocodetoolspromisetoboostyourdeveloperworkflownotechexpertiseneededhttpshackernooncomthesenocodetoolspromisetoboostyourdeveloperworkflownotechexpertiseneededhttpscdnhackernooncomimagesgaztnviyrwbjikqm85r8ixm2voy23l034nyjpeg\"><strong><a href=\"https://hackernoon.com/these-no-code-tools-promise-to-boost-your-developer-workflowno-tech-expertise-needed\">These No-code Tools Promise to Boost Your Developer Workflow—No Tech Expertise Needed</a></strong> <img src=\"https://cdn.hackernoon.com/images/gaZTNviyRwbJIkQm85R8IxM2vOY2-3l034ny.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/madzadev\">@madzadev</a> [ 5 Min read ] \n In this article, I've manually compiled 8 of my favorite no-code tools you can use in your workflow to boost productivity without a lot of tech expertise. <a href=\"https://hackernoon.com/these-no-code-tools-promise-to-boost-your-developer-workflowno-tech-expertise-needed\">Read More.</a></p>\n<h2 id=\"standoutfromotherdevelopersbycontributingtoopensourcehttpshackernooncomstandoutfromotherdevelopersbycontributingtoopensourcehttpscdnhackernooncomimagesnz9j3rbetvbgserqxpofkosigah1wj02ojvjpeg\"><strong><a href=\"https://hackernoon.com/stand-out-from-other-developers-by-contributing-to-open-source\">Stand Out From Other Developers By Contributing to Open Source</a></strong> <img src=\"https://cdn.hackernoon.com/images/NZ9j3rbETVbGsErqXpofkOsigAh1-wj02ojv.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/empiree361\">@empiree361</a> [ 7 Min read ] \n If you truly love programming and want to grow as a developer, strive to create something of your own — whether it’s a small library or a service. <a href=\"https://hackernoon.com/stand-out-from-other-developers-by-contributing-to-open-source\">Read More.</a></p>\n<h2 id=\"howiscryptoregulationshapingupinthe2ndlargestcontinentintheworldhttpshackernooncomhowiscryptoregulationshapingupinthe2ndlargestcontinentintheworldhttpscdnhackernooncomimagessbgdf0dbxxnwhs0pfrpmgivvot32q6234ugjpeg\"><strong><a href=\"https://hackernoon.com/how-is-crypto-regulation-shaping-up-in-the-2nd-largest-continent-in-the-world\">How Is Crypto Regulation Shaping Up In The 2nd Largest Continent in the World?</a></strong> <img src=\"https://cdn.hackernoon.com/images/sBGdF0DbxXNwhS0pFRPmgiVVOt32-q6234ug.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/ilinskii\">@ilinskii</a> [ 5 Min read ] \n Africa's crypto regulation landscape is diverse and dynamic with 54 countries, six major languages, and over 1 billion people.  <a href=\"https://hackernoon.com/how-is-crypto-regulation-shaping-up-in-the-2nd-largest-continent-in-the-world\">Read More.</a></p>\n<h2 id=\"mailbirdexpandstomacmanageallyourinboxesandfavoriteappsinoneplacehttpshackernooncommailbirdexpandstomacmanageallyourinboxesandfavoriteappsinoneplacehttpscdnhackernooncomimageshq098u52dzpm2y4uitqcqxtlrak2g40300spng\"><strong><a href=\"https://hackernoon.com/mailbird-expands-to-mac-manage-all-your-inboxes-and-favorite-apps-in-one-place\">Mailbird Expands to Mac: Manage All Your Inboxes and Favorite Apps in One Place</a></strong> <img src=\"https://cdn.hackernoon.com/images/hQ098u52DzPm2Y4UITQcQXtLRAk2-g40300s.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/pressreleases\">@pressreleases</a> [ 2 Min read ] \n Mailbird, the email client trusted by millions of Windows users worldwide, is now available for Mac. <a href=\"https://hackernoon.com/mailbird-expands-to-mac-manage-all-your-inboxes-and-favorite-apps-in-one-place\">Read More.</a></p>\n<h2 id=\"restitsmorethanjustcrudoverhttphttpshackernooncomrestitsmorethanjustcrudoverhttphttpscdnhackernooncomimagesinxbrjris6m1kdhuwcynhiiurxm1m9034gpwebp\"><strong><a href=\"https://hackernoon.com/rest-its-more-than-just-crud-over-http\">ReST: It’s More Than Just CRUD Over HTTP</a></strong> <img src=\"https://cdn.hackernoon.com/images/InxBRjRIs6M1kdhuWcyNHiiUrxm1-m9034gp.webp\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/justc\">@justc</a> [ 5 Min read ] \n Discover why ReST API design goes beyond CRUD operations. Learn how intent-based approaches create meaningful user-system dialogues &amp; align with business goals. <a href=\"https://hackernoon.com/rest-its-more-than-just-crud-over-http\">Read More.</a></p>\n<h2 id=\"bitcoinenthusiastsarelettingaltcoinspassbyhttpshackernooncombitcoinenthusiastsarelettingaltcoinspassbyhttpscdnhackernooncomimages2jqchkrv03exbugklrdzibfm99q2xw02suujpeg\"><strong><a href=\"https://hackernoon.com/bitcoin-enthusiasts-are-letting-altcoins-pass-by\">Bitcoin Enthusiasts Are Letting Altcoins Pass by</a></strong> <img src=\"https://cdn.hackernoon.com/images/2jqChkrv03exBUgkLrDzIbfM99q2-xw02suu.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/sergeigorshunov\">@sergeigorshunov</a> [ 2 Min read ] \n Bitcoin, which is trying to settle above the psychologically important $100,000 level, gets the most media attention. <a href=\"https://hackernoon.com/bitcoin-enthusiasts-are-letting-altcoins-pass-by\">Read More.</a> \n 🧑‍💻 What happened in your world this week? It's been said that <a href=\"https://hackernoon.com/developers-the-why-and-how-to-writing-technical-articles-54e824789ef6\">writing can help consolidate technical knowledge</a>, <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\">establish credibility</a>,<a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\"> and contribute to emerging community standards</a>. Feeling stuck? We got you covered ⬇️⬇️⬇️\n <a href=\"https://app.hackernoon.com/mobile/lZx3fmlPdlPJpVBIdble\">ANSWER THESE GREATEST INTERVIEW QUESTIONS OF ALL TIME</a>\n We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.\n See you on Planet Internet! With love, \n The HackerNoon Team ✌️\n <img src=\"https://cdn.hackernoon.com/images/ezgif.com-gif-maker%20(44).gif\" alt=\"\" /></p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"How a Retrocomputing Enthusiast Got a 30-Year-Old Clamshell Computer Online","url":"https://tech.slashdot.org/story/24/12/29/0454215/how-a-retrocomputing-enthusiast-got-a-30-year-old-clamshell-computer-online?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735450440,"author":"EditorDavid","unread":true,"desc":"","content":"It had a 4.8-inch display. Introduced in 1991, Hewlett-Packard's (DOS-based) HP 95LX Palmtop PC &mdash; a collaboration with Lotus &mdash; was finally discontinued back in 2003. \n\nBut one found its way to long-time Slashdot reader Shayde (who in November repaired a 48-year-old handheld videogame console from Mattel). \"I really wanted to get this HP95LX talking to the internet at large,\" they told Slashdot, \" but network stacks for DOS in 1991 were pretty limited, and this machine didn't even have the hardware for a network connection. \n\n\"It did have a serial port though &mdash; a flat 4-pin custom interface. I did a bunch of research and learned how to custom-build an RS-232 hookup for this port, and using an external Wifi module, got it online &mdash; and talking to the retrocomputing BBS!\" \nThere's a video documenting the whole experience. (Along the way he uses 20-gauge hook-up wire from Amazon, a zip tie, solder cups, and an internet modem (the WiFi232 Hayes modem emulator). The whole thing is powered by two AA batteries &mdash; it has 512K of memory, and about half a meg of storage. My favorite technical detail? \n\n\"Conveniently, the HP 95 [Palmtop PC] uses the exact same pinout as the HP 48GX handheld graphing calculator. So looking up on the Internet, we can determine what pins we need to map from the HP unit over to what would be a DB25 serial port...\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=How+a+Retrocomputing+Enthusiast+Got+a+30-Year-Old+Clamshell+Computer+Online%3A+https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F29%2F0454215%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F29%2F0454215%2Fhow-a-retrocomputing-enthusiast-got-a-30-year-old-clamshell-computer-online%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://tech.slashdot.org/story/24/12/29/0454215/how-a-retrocomputing-enthusiast-got-a-30-year-old-clamshell-computer-online?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563817&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Will AI Transform Online Dating?","url":"https://slashdot.org/story/24/12/28/0652238/will-ai-transform-online-dating?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735439640,"author":"EditorDavid","unread":true,"desc":"","content":"\"Dating apps are on the cusp of a major transformation,\" argues CNN, suggesting AI-powered possibilities like \"personalized chatbots dating other chatbots on your behalf,\" as well as \"AI concierges fielding questions about potential matches,\" and \"advanced algorithms predicting compatibility better than ever before.\"\n\n\nAt its investor day last week, executives from Match Group &mdash; the parent company of Match.com, Tinder, Hinge, OkCupid, Our Time and more &mdash; teased plans to use AI to improve user experiences and help make better connections. Justin McLeod, CEO of Hinge, outlined how the company intends to fully embrace AI next year: more personalized matching, smarter algorithms that adapt to users and better understand them over time and AI coaching for struggling daters. \"While AI is not going to be a panacea when it comes to the very deeply and personal problem of love, I can tell you that it is going to transform the dating app experience, taking it from a do-it-yourself platform to an expertly guided journey that leads to far better outcomes and much better value to our daters,\" he told investors.... \n\nIt's already starting to play a bigger role. Tinder, for example, uses AI to help users select their best profile photos. Meanwhile, Bumble's recently enhanced \"For You\" roundup uses advanced AI when delivering its daily set of four curated profiles based on a user's preferences and past matches. Bumble also uses AI in safety features like its Private Detector &mdash; an AI-powered tool that blurs explicit images &mdash; and Deception Detector, which identifies spam, scams and fake profiles. Similarly, Match Group offers tools like buttons that say \"Are You Sure?\" to detect harmful language and \"Does This Bother You?\" to prompt users to report inappropriate behavior.... \n\n According to Liesel Sharabi, an associate professor at Arizona State University's Hugh Downs School of Human Communication, the dating industry is still \"very much in the early stages\" of embracing AI. \"The platforms are still figuring out its role in the online dating experience, but it really does have the potential to transform this space....\" Bumble founder Whitney Wolfe Herd previously said she envisions AI functioning as a dating concierge, helping users navigate matches, set up dates and respond to messages. Startups such as Volar and Rizz have already experimented with chatbots that help respond to messages. On Rizz, users upload screenshots of conversations they're having on other dating apps, and the platform helps create flirty replies. (Volar, a standalone dating app that trains on users' preferences and automatically responds to other chatbots, shut down in September due to lack of funding.) While the concept of chatbots dating on your behalf may seem strange, it could reduce the tedious early-stage communication by focusing more on highly compatible matches, Sharabi said... \n\nDuring Match Group's investor day, Hinge's McLeod announced plans to build the \"world's most knowledgeable dating coach\" using years of insights from the dating process... McLeod said Hinge has already seen a higher number of matches and subscription renewals with its improved AI algorithm among early test groups. It plans to roll this out globally in March. \nAnd of course, some users are already using ChatGPT to write online dating profiles or respond to messages, the article points out...<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Will+AI+Transform+Online+Dating%3F%3A+https%3A%2F%2Fslashdot.org%2Fstory%2F24%2F12%2F28%2F0652238%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fslashdot.org%2Fstory%2F24%2F12%2F28%2F0652238%2Fwill-ai-transform-online-dating%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://slashdot.org/story/24/12/28/0652238/will-ai-transform-online-dating?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563209&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"LEAP 71 Hot-Fires Advanced Aerospike Rocket Engine Designed by AI","url":"https://science.slashdot.org/story/24/12/28/2312227/leap-71-hot-fires-advanced-aerospike-rocket-engine-designed-by-ai?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735428840,"author":"EditorDavid","unread":true,"desc":"","content":"Long-time Slashdot reader schwit1 writes: The Dubai-based startup LEAP71, focused on using AI software to quickly develop rocket engine designs it can then 3D print, has successfully test fired a prototype aerospike engine on December 18, 2024 during a static fire test campaign conducted in the United Kingdom. \n\nAlong the way they tackled a problem with bell-shaped rocket nozzles, writes New Atlas. \"A rocket that works very well on liftoff will work less well as it rises in the atmosphere and the air pressure decreases. This is why second- and third-stage rocket engines are different from those of the first stage.\"\n\nIdeally, engineers want an engine that can adjust itself automatically to changes in air pressure. An aerospike does this by shaping the engine into a spike or plug with a curve like that of the inside of a rocket bell. As the combustion gases flow from the engine over the spike, the curve acts as one side of the bell and the surrounding air as the outside curve. As the air pressure changes, so does the shape of the virtual bell. There have been a number of aerospike engines developed since the 1950s and one has actually gone airborne, but there's still a long way to go when it comes to turning a promising idea into a practical space engine. \n\nLEAP 71's contribution to the effort is to apply its Noyron Large Computational Engineering Model to the problem. It's an AI programmed and trained by aerospace experts to take a given set of input parameters and use them to create a design that meets those parameters by inferring physical interactions of various factors, including thermal behaviors and projected performance. The results of this are then fed back into the AI model to fine tune it as it presents computed performance parameters, the geometry of the engine, the parameters of the manufacturing process, and other details. \n\n\"Despite their clear advantages, Aerospikes are not used in space access today,\" LEAP 71's co-founder said in a statement. \"We want to change that. Noyron allows us to radically cut the time we need to re-engineer and iterate after a test and enables us to converge rapidly on an optimal design.\" \n\nAerospikes \"are more compact and significantly more efficient across various atmospheric pressures, including the vacuum of space,\" the company said this week &mdash; announcing the successful hot-firing of their Aerospike engine, and calling it \"one of the most advanced and elusive rocket engines ever created...\"\n\n By leveraging the power of Noyron's computational AI, the thruster was developed in a matter of weeks, manufactured as a monolithic piece of copper through industrial 3D printing, and put on the test stand, where it worked successfully on the first attempt... \n\nThe Aerospike was fired on December 18th, 2024, as part of a four-engines-in-four-days campaign conducted by LEAP&#226;71 at Airborne Engineering in Westcott, UK. The company will process the collected data to fine-tune Noyron for the next iteration of engines and continue testing in 2025, with the goal of making Aerospikes a viable option for modern spacecraft.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=LEAP+71+Hot-Fires+Advanced+Aerospike+Rocket+Engine+Designed+by+AI%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F28%2F2312227%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F28%2F2312227%2Fleap-71-hot-fires-advanced-aerospike-rocket-engine-designed-by-ai%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/28/2312227/leap-71-hot-fires-advanced-aerospike-rocket-engine-designed-by-ai?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563667&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Communications of the ACM Asks: Is It Ethical To Work For Big Tech?","url":"https://tech.slashdot.org/story/24/12/28/2030223/communications-of-the-acm-asks-is-it-ethical-to-work-for-big-tech?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735425240,"author":"EditorDavid","unread":true,"desc":"","content":"Long-time Slashdot reader theodp writes:\n\n\n\nBack in January, Rice University professor and former CACM Editor-in-Chief Moshe Y. Vardi wrote of the unintended consequences of social media and mobile computing in \"Computing, You Have Blood on Your Hands!\" To close out the year, Vardi addresses the role tech workers play in enabling dubious Big Tech business models &mdash; including now-powered-by-AI Big Tech Surveillance Capitalism &mdash; in an opinion piece titled \"I Was Wrong about the Ethics Crisis.\" Vardi writes: \"The belief in the magical power of the free market always to serve the public good has no theoretical basis. In fact, our current climate crisis is a demonstrated market failure. To take an extreme example, Big Tobacco surely does not support the public good, and most of us would agree that it is unethical to work for Big Tobacco. The question, thus, is whether Big Tech is supporting the public good, and if not, what should Big Tech workers do about it. Of course, there is no simple answer to such a question, and the only reasonable answer to the question of whether it is ethical to work for Big Tech is, 'It depends.' [...] It is difficult to get a man to understand something, when his salary depends on his not understanding it, said the writer and political activist Upton Sinclair. By and large, Big Tech workers do not seem to be asking themselves hard questions, I believe, hence my conclusion that we do indeed suffer from an ethics crisis.\"\n\n<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Communications+of+the+ACM+Asks%3A+Is+It+Ethical+To+Work+For+Big+Tech%3F%3A+https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F28%2F2030223%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F28%2F2030223%2Fcommunications-of-the-acm-asks-is-it-ethical-to-work-for-big-tech%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://tech.slashdot.org/story/24/12/28/2030223/communications-of-the-acm-asks-is-it-ethical-to-work-for-big-tech?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563567&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Google CEO says AI model Gemini will be the company’s ‘biggest focus’ in 2025","url":"https://techcrunch.com/2024/12/28/google-ceo-says-ai-model-gemini-will-the-companys-biggest-focus-in-2025/","date":1735423241,"author":"Anthony Ha","unread":true,"desc":"","content":"<p>CEO Sundar Pichai reportedly told Google employees that 2025 will be a “critical” year for the company. CNBC reports that it obtained audio from a December 18 strategy meeting where Pichai and other executives put on ugly holiday sweaters and laid out their priorities for the coming year. “I think 2025 will be critical,” Pichai [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"How Blockchain Contracts Ensure Fairness, Flexibility, and Compensation for Option Holders","url":"https://hackernoon.com/how-blockchain-contracts-ensure-fairness-flexibility-and-compensation-for-option-holders?source=rss","date":1735423211,"author":"EScholar: Electronic Academic Papers for Scholars","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<ol>\n<li><p><a href=\"http://hackernoon.com/preview/Um9xbWbUFDMzkXakWgDh\">Abstract and Introduction</a></p></li>\n<li><p><a href=\"https://hackernoon.com/preview/dKHrehhbzuaTHtgJFbvy\">Preliminaries</a></p></li>\n<li><p><a href=\"https://hackernoon.com/preview/oyJxVXND1XBuIkqdQbUB\">Overview</a> </p></li>\n<li><p>Protocol</p>\n<p><a href=\"http://hackernoon.com/preview/zubShKzeJXXu82JolWP1\">4.1 Efficient Option Transfer Protocol</a></p>\n<p><a href=\"https://hackernoon.com/preview/DpuaIvNLjK32Qu0Ww6rb\">4.2 Holder Collateral-Free Cross-Chain Options</a> </p></li>\n<li><p>Security Analysis</p>\n<p><a href=\"https://hackernoon.com/preview/tvEU9LSYD1n72BFcPYRT\">5.1 Option Transfer Properties</a></p>\n<p><a href=\"https://hackernoon.com/preview/VAh9cc7JIcgJxxRg7Fpi\">5.2 Option Properties</a> </p></li>\n<li><p><a href=\"http://hackernoon.com/preview/EFqsX8ZQC0xIihPVO1wn\">Implementation</a></p></li>\n<li><p><a href=\"http://hackernoon.com/preview/QiR69gdekeOcmxVXfMmx\">Related Work</a> </p></li>\n<li><p><a href=\"http://hackernoon.com/preview/8Xg0iL7cvxWaswxK1nyU\">Conclusion and Discussion, and References</a></p></li>\n</ol>\n<p>\\\n<strong>A. Codes</strong></p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/6y2VWpQDepDSmB2qOM4S\">A.1 Robust and Efficient Transfer Protocol</a></li>\n<li><a href=\"http://hackernoon.com/preview/VefKnK0kRsutojZNjvDf\">A.2 Holder Collateral-Free Cross-Chain Options</a></li>\n</ul>\n<p><strong>B. Proofs</strong></p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/jPnJi8X5LswRotYNUva5\">B.1 Transfer Protocol Proofs</a></li>\n<li><a href=\"https://hackernoon.com/preview/R3bOBSwg5ZdKJAP8Bw5b\">B.2 Option</a></li>\n</ul>\n<h2 id=\"b2option\">B.2 Option</h2>\n<p>Theorem 6. Protocol 4.2 satisfies option correctness: If both the Alice and Bob are conforming, then if Alice does not exercise the right, Alice doesn’t lose the 𝐴𝑠𝑠𝑒𝑡𝐴 and Bob doesn’t lose the 𝐴𝑠𝑠𝑒𝑡𝐺 and 𝐴𝑠𝑠𝑒𝑡𝐵; or if Alice exercise the right, then Alice will receive 𝐴𝑠𝑠𝑒𝑡𝐵 and Bob will receive 𝐴𝑠𝑠𝑒𝑡𝐴 and 𝐴𝑠𝑠𝑒𝑡𝐺 .</p>\n<p>\\\nProof. According to Protocol 4.2, it is evident that if Alice escrows her collateral in the 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐴 contract and calls 𝑒𝑥𝑒𝑟𝑐𝑖𝑠𝑒 (), then a conforming Bob will reveal the pre-image 𝐵 in 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐴 to reclaim the guarantee 𝐴𝑠𝑠𝑒𝑡𝐺 and Alice’s collateral 𝐴𝑠𝑠𝑒𝑡𝐴. Subsequently, Alice can use 𝐵 to obtain 𝐴𝑠𝑠𝑒𝑡𝐵. If Alice does not escrow the collateral, Bob will not reveal 𝐵. After the option expires at 𝑇𝐸 +2Δ, Bob can call𝑐𝑙𝑎𝑖𝑚() and 𝑟𝑒 𝑓 𝑢𝑛𝑑 () on the respective chains to reclaim 𝐴𝑠𝑠𝑒𝑡𝐺 and 𝐴𝑠𝑠𝑒𝑡𝐵.</p>\n<p>\\\nTheorem 7. <em>Protocol 4.2 satisfies exercisablity: During the transfer from Bob to Dave, the option remains active, allowing Alice to exercise the option without any delays.</em></p>\n<p>\\\nProof. According to Protocol 4.2.1, during the transfer from Bob to Dave, Alice can make a deposit and exercise her option at any time. If the transfer is in the Setup Phase, Bob will need to reveal 𝐵 to fulfill his obligation and revoke the transfer. It is important to note that Dave can use 𝐵 to reclaim 𝑇 𝑟𝑎𝑛𝑠𝑊 . If the transfer is in the Attempt Phase and Bob acts maliciously by using 𝐵 to take 𝐴𝑠𝑠𝑒𝑡𝐺 , Alice can use 𝐵 to obtain 𝐴𝑠𝑠𝑒𝑡𝐵. Dave will need to use 𝐵 on 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐵 to withdraw the transfer. Otherwise, when Dave uses 𝜎𝑚 to change the writer and the hash lock, he will reveal a new preimage secret 𝐷, which Alice can then use to obtain 𝐴𝑠𝑠𝑒𝑡𝐵.</p>\n<p>\\\nTheorem 8. <em>Protocol 4.2 satisfies failure compensation: Before expiration, Alice can exercise the option successfully, or if the exercise fails, she is compensated with the guarantee deposited by Bob.</em></p>\n<p>\\\nProof. By Theorem 6, if Alice successfully exercises her option, she will receive Bob’s collateral. Otherwise, after Alice makes a deposit and calls 𝑒𝑥𝑒𝑟𝑐𝑖𝑠𝑒 (), 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐴 can invoke 𝑖𝑠𝐷𝑒𝑝𝑜𝑠𝑖𝑡𝑒𝑑 () to determine if the exercise has occurred. If Bob does not fulfill his obligation within a period of Δ, Alice can call 𝑐𝑙𝑎𝑖𝑚() to obtain 𝐴𝑠𝑠𝑒𝑡𝐺 as compensation, and Bob will lose his guarantee.</p>\n<p>\\</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Zifan Peng, The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, Guangdong, China (zpengao@connect.hkust-gz.edu.cn);</p>\n<p>(2) Yingjie Xue, The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, Guangdong, China (yingjiexue@hkust-gz.edu.cn);</p>\n<p>(3) Jingyu Liu, The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, Guangdong, China (jliu514@connect.hkust-gz.edu.cn).</p>\n<p>:::</p>\n<hr />\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2410.15724\">available on arxiv</a> under CC BY 4.0 license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"'Universal Basic Income' Isn't a Silver Bullet, Says Lead Researcher on Sam Altman's Study","url":"https://yro.slashdot.org/story/24/12/28/0535244/universal-basic-income-isnt-a-silver-bullet-says-lead-researcher-on-sam-altmans-study?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735421640,"author":"EditorDavid","unread":true,"desc":"","content":" Business Insider reports:\n\nThe lead researcher for Sam Altman's basic-income study says guaranteed no-strings payments are not a silver bullet for issues facing lower-income Americans. Elizabeth Rhodes, the research director for the Basic Income Project at Open Research, told Business Insider that while basic-income payments are \"beneficial in many ways,\" the programs also have \"clear limitations....\" \n\nRhodes headed up one of the largest studies in the space, which focused specifically on those on low incomes rather than making universal payments to adults across all economic demographics. The three-year experiment, backed by OpenAI boss Altman, provided 1,000 low-income participants with $1,000 a month without any stipulations for how they could spend it.... The initial findings, released in July, found that recipients put the bulk of their extra spending toward basic needs such as rent, transportation, and food. They also worked less on average but remained engaged in the workforce and were more deliberate in their job searches compared with a control group. But Rhodes says the research reinforced how difficult it is to solve complex issues such as poverty or economic insecurity, and that there is \"a lot more work to do.\" \n\nThe Altman-backed study is still reporting results. New findings released in December showed recipients valued work more after receiving the recurring monthly payments &mdash; a result that may challenge one of the main arguments against basic income payments. Participants also reported significant reductions in stress, mental distress, and food insecurity during the first year, though those effects faded by the second and third years of the program. \"Poverty and economic insecurity are incredibly difficult problems to solve,\" Rhodes said. \"The findings that we've had thus far are quite nuanced.\" \n\nShe added: \"There's not a clear through line in terms of, this helps everyone, or this does that. It reinforced to me the idea that these are really difficult problems that, maybe, there isn't a singular solution.\" \n\nIn an earlier article coauthor David Broockman told Business Insider that the study's results might offer insights into how future programs could be successful &mdash; but said that the study's results didn't necessarily confirm the fears or hopes expressed by skeptics or supporters of a basic income. \n\n\nThanks to Slashdot reader jjslash for sharing the news.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status='Universal+Basic+Income'+Isn't+a+Silver+Bullet%2C+Says+Lead+Researcher+on+Sam+Altman's+Study%3A+https%3A%2F%2Fyro.slashdot.org%2Fstory%2F24%2F12%2F28%2F0535244%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fyro.slashdot.org%2Fstory%2F24%2F12%2F28%2F0535244%2Funiversal-basic-income-isnt-a-silver-bullet-says-lead-researcher-on-sam-altmans-study%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://yro.slashdot.org/story/24/12/28/0535244/universal-basic-income-isnt-a-silver-bullet-says-lead-researcher-on-sam-altmans-study?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563177&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"How Cross-Chain Transfer Protocols Ensure Safe and Smooth Transactions","url":"https://hackernoon.com/how-cross-chain-transfer-protocols-ensure-safe-and-smooth-transactions?source=rss","date":1735421412,"author":"EScholar: Electronic Academic Papers for Scholars","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<ol>\n<li><p><a href=\"http://hackernoon.com/preview/Um9xbWbUFDMzkXakWgDh\">Abstract and Introduction</a></p></li>\n<li><p><a href=\"https://hackernoon.com/preview/dKHrehhbzuaTHtgJFbvy\">Preliminaries</a></p></li>\n<li><p><a href=\"https://hackernoon.com/preview/oyJxVXND1XBuIkqdQbUB\">Overview</a> </p></li>\n<li><p>Protocol</p>\n<p><a href=\"http://hackernoon.com/preview/zubShKzeJXXu82JolWP1\">4.1 Efficient Option Transfer Protocol</a></p>\n<p><a href=\"https://hackernoon.com/preview/DpuaIvNLjK32Qu0Ww6rb\">4.2 Holder Collateral-Free Cross-Chain Options</a> </p></li>\n<li><p>Security Analysis</p>\n<p><a href=\"https://hackernoon.com/preview/tvEU9LSYD1n72BFcPYRT\">5.1 Option Transfer Properties</a></p>\n<p><a href=\"https://hackernoon.com/preview/VAh9cc7JIcgJxxRg7Fpi\">5.2 Option Properties</a> </p></li>\n<li><p><a href=\"http://hackernoon.com/preview/EFqsX8ZQC0xIihPVO1wn\">Implementation</a></p></li>\n<li><p><a href=\"http://hackernoon.com/preview/QiR69gdekeOcmxVXfMmx\">Related Work</a> </p></li>\n<li><p><a href=\"http://hackernoon.com/preview/8Xg0iL7cvxWaswxK1nyU\">Conclusion and Discussion, and References</a></p></li>\n</ol>\n<p>\\\n<strong>A. Codes</strong></p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/6y2VWpQDepDSmB2qOM4S\">A.1 Robust and Efficient Transfer Protocol</a></li>\n<li><a href=\"http://hackernoon.com/preview/VefKnK0kRsutojZNjvDf\">A.2 Holder Collateral-Free Cross-Chain Options</a></li>\n</ul>\n<p><strong>B. Proofs</strong></p>\n<ul>\n<li><a href=\"http://hackernoon.com/preview/jPnJi8X5LswRotYNUva5\">B.1 Transfer Protocol Proofs</a></li>\n<li><a href=\"https://hackernoon.com/preview/R3bOBSwg5ZdKJAP8Bw5b\">B.2 Option</a></li>\n</ul>\n<h2 id=\"b1transferprotocolproofs\">B.1 Transfer Protocol Proofs</h2>\n<p>Lemma 9. <em>The holder transfer procedure of Protocol 4.2.1 does not require Bob’s participation.</em></p>\n<p>\\\nProof. It is evident that, Alice does not own the exercise secret, holder’s transfer is required to replace the holder address and transfer public key, and the inconsistency of two chains will not harm the interest of Bob. According to the Protocol 4.1, Bob cannot use the transfer private key of Alice, i.e. 𝑠𝑘𝐴 to claim assets. Therefore, during the reveal phase and consistency phase, Bob is not required to participate and not allowed to make any change on 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐴 and 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐵.</p>\n<p>\\\nLemma 10. <em>If Bob and Dave are conforming, then the writer transfer procedure of Protocol 4.2.1 does not require Alice’s participation.</em></p>\n<p>\\\nProof. Obviously, honest Bob will not leak two signatures or 𝑠𝑘𝐵 and honest Dave will submit signature 𝜎𝑚 on both 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐴 and 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐵, Alice only needs to make operations when there is any dishonest party.</p>\n<p>\\\nTheorem 2. <em>Protocol 4.2.1 satisfies liveness: If Alice, Bob, and Carol/Dave are conforming, then Alice/Bob will obtain Carol/Dave’s collateral, Carol/Dave will obtain Alice/Bob’s position, and Bob/Alice will retain their original position.</em></p>\n<p>\\\nProof. By Lemma 9, Bob’s participation is not required during the holder transfer. If Alice and Carol are conforming, Carol will create 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐶 contract and lock her collateral using signature of Alice before 𝑇𝐻 − 3Δ. Alice will then reveal signature by 𝑠𝑘𝐴 and call𝑟𝑒𝑣𝑒𝑎𝑙() on𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐶 at𝑇𝐻 −2Δ. An honest Carol will forward the signature, setting the holder to Carol. Alice can then wait for 3Δ withdrawl delayed period to obtain the collateral, while the writers of𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐴 and𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐵 are still Bob, Bob maintains the writer’s position. During the process where Bob transfers his position to Dave, if both parties are conforming, Bob will not expose two different signatures. After 𝑇𝑊 + Δ, Bob will not be obstructed and will surely obtain Dave’s collateral. Meanwhile, Dave can submit 𝜎𝑚 between𝑇𝑊 −Δ and𝑇𝑊 to 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐴 and 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐵 to change the writer, Alice retaining the holder position.</p>\n<p>\\\nTheorem 3. <em>Protocol 4.2.1 satisfies unobstructibility: Alice/Bob can transfer the position to another party even if Bob/Alice is adversarial.</em></p>\n<p>\\\nProof. By Lemma 9, Bob’s participation is not required, it is evident that Bob cannot block the process of transferring a holder’s position. During Bob’s transfer to Dave, Alice can only obtain Bob’s collateral by two different messages signed with 𝑠𝑘𝐵 or the exercise secret. If Bob is honest, he will neither leak 𝑠𝐵, sign multiple messages nor leak exercise secret. Consequently, Alice cannot interrupt the transfer process.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-hi034rc.png\" alt=\"\" /></p>\n<p>\\\nProof. After 𝐴𝑙𝑖𝑐𝑒𝑖 transfers to 𝐴𝑙𝑖𝑐𝑒𝑖+1, the holder in the current option’s 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐴 and 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐵 is updated to Alice𝑖+1, and the transfer key is known only to 𝐴𝑙𝑖𝑐𝑒𝑖+1. Therefore, after a holder transfer, 𝐴𝑙𝑖𝑐𝑒𝑖+1 can transfer the position to 𝐴𝑙𝑖𝑐𝑒𝑖+2 by re-performing Protocol 4.2.1 with the transfer key of 𝐴𝑙𝑖𝑐𝑒𝑖+2. Similarly, after 𝐵𝑜𝑏𝑗 transfers to 𝐵𝑜𝑏𝑗+1 (holder Alice does not contest within Δ), the writer in the current option’s 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐴 and 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐵 is updated to Bob𝑗+1. At this point, only 𝑠𝑘 𝑗+1 𝐵 or its signatures can be used for the next transfer. 𝐵𝑜𝑏𝑗+1 can also transfer the position by re-performing Protocol 4.2.1 with the new transfer key.</p>\n<p>\\\nLemma 11. Protocols 4.2.1 satisfy atomicity: If conforming Alice/Bob loses their position, she/he will be able to obtain Carol/Dave’s collateral.</p>\n<p>\\\nProof. Following Theorem 2, in transferring the holder position, after Carol correctly escrows the collateral, Alice temporarily locks the holder position in both contracts using 𝐻(𝐶). If Carol uses 𝐶 to obtain the position before 𝑇𝐻 , then Alice will obtain Carol’s collateral at 𝑇𝐻 + Δ. If Carol does not reveal 𝐶 before 𝑇𝐻 , Alice will not receive Carol’s collateral. Similarly, in transferring the writer position, if Bob does not reveal his signature honestly, then Bob will lose the position and Dave can retrieve and will not lose the collateral. If honest Bob signs for a buyer Dave, the honest Dave will use the signature to obtain Bob’s position at 𝑇𝑊 . Bob will then obtain Dave’s collateral at 𝑇𝑊 + Δ.</p>\n<p>\\\nB.1.1 <em>Safety</em>.</p>\n<p>\\\nProof. In the 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐴, the following elements are defined:</p>\n<p>\\\n• 𝑇𝐸: The expiration time of this option. </p>\n<p>\\\n• exercise_hashlock: The hash lock of this option, which is the hash of a secret value known only to the writer. </p>\n<p>\\\n• old<em>exercise</em>hashlock: The hash lock of this option, which is the hash of a secret value known only to the writer. </p>\n<p>\\\n• holder: The holder can call 𝑒𝑥𝑒𝑟𝑐𝑖𝑠𝑒 () to exercise the option before 𝑇𝐸. </p>\n<p>\\\n• guarantee: The writer’s asset, i.e. 𝐴𝑠𝑠𝑒𝑡𝐺 , which can be any asset mutually agreed upon by the holder and writer as guarantee. This can include tokens, NFTs, or any other type of asset. </p>\n<p>\\\n• writer: The writer can use the secret value to call 𝑟𝑒 𝑓 𝑢𝑛𝑑 () to retrieve the guarantee or retrieve it directly after 𝑇𝐸 + 2Δ. </p>\n<p>\\\n• collateral: The collateral that Alice must deposit if she decides to exercise the option to purchase Bob’s asset. </p>\n<p>\\\n• holder<em>transfer</em>public_key: the transfer key of Alice, 𝑝𝑘𝐴, used for verify the transfer signature of Alice to Carol. </p>\n<p>\\\n• writer<em>transfer</em>public_key: New transfer key of Dave, 𝑝𝑘𝐷 , used for verify the transfer signature of Dave to others. </p>\n<p>\\\n• old<em>writer</em>transfer<em>public</em>key: Old transfer key of Bob, 𝑝𝑘𝐵, used for verify the transfer signature of Bob to Dave, Within the period of one Δ, during which the transfer signature must be submitted to this contract, we still need to record the old transfer public key in case of Bob’s misbehavior. </p>\n<p>\\\n• writer<em>transfer</em>time: The writer transfer time, used for Alice to claim assets if there exits misbehavior of Bob.</p>\n<p>\\\nIn the 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐵, there are other additional items:</p>\n<p>\\\n• collateral: The writer’s collateral, i.e.𝐴𝑠𝑠𝑒𝑡𝐵, it can be claimed by holder with preimage of hashlock. </p>\n<p>\\\n• holder: The holder can call 𝑒𝑥𝑒𝑟𝑐𝑖𝑠𝑒 () to exercise the option before 𝑇𝐸. </p>\n<p>\\\n• writer: The writer can call𝑟𝑒 𝑓 𝑢𝑛𝑑 () to retrieve the guarantee or retrieve it directly after 𝑇𝐸 + 2Δ.</p>\n<p>\\\nIn the 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐷 , the following elements are defined:</p>\n<p>\\\n• T_W: The deadline for seller to reveal signature. </p>\n<p>\\\n• buyer: writer position buyer, i.e. Dave. </p>\n<p>\\\n• seller: writer position seller, i.e. Bob. </p>\n<p>\\\n• old<em>exercise</em>hashlock: The hashlock of exercise, if Bob reveals during the transfer, Dave is able to reclaim with preimage. </p>\n<p>\\\n• exercise_hashlock: The new hashlock of exercise, generated by Dave. </p>\n<p>\\\n• old<em>writer</em>transfer<em>public</em>key: Bob’s transfer public key, used for verify the signature of Bob. </p>\n<p>\\\n• writer<em>transfer</em>public_key: New transfer public key generated by Dave, used for replacing Bob’s key. </p>\n<p>\\\n• transfer_time: Used for record the time of transfer (the time reveal signature) and calculate the withdrawal delayed period.</p>\n<p>\\\nTake Bob transferring his position to Dave as an example, since Bob deposit 𝐴𝑠𝑠𝑒𝑡𝐺 and 𝐴𝑠𝑠𝑒𝑡𝐵 into the contracts, which is more complex. Transferring Alice’s position to Carol is more simple.</p>\n<p>\\\nBy Lemma 11, if compliant Bob loses his position, he will at least obtain Dave’s collateral during the writer transfer process.</p>\n<p>\\\nIf Dave is conforming, then if Bob acts maliciously on his own, Bob provides two different signatures to different buyers, Dave can reclaim the transfer fee with extracted 𝑠𝑘𝐵 since 𝐷 records old<em>writer</em>transfer<em>public</em>key i.e. 𝑝𝑘𝐵. If Bob reveals 𝐵 at the same time during transfer process, then Dave can use 𝐵 to reclaim 𝑊 𝑟𝑖𝑡𝑒𝑟𝐹𝑒𝑒 since 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐶 records old<em>exercise</em>hashlock i.e. 𝐻(𝐵). If Alice and Bob collude, they can use 𝑠𝑘𝐵 or 𝐵 to withdraw 𝐴𝑠𝑠𝑒𝑡𝐺 and 𝐴𝑠𝑠𝑒𝑡𝐵. Then, Dave can observe 𝑠𝑘𝐵 or 𝐵 and withdraw 𝑊 𝑟𝑖𝑡𝑒𝑟𝐹𝑒𝑒 during withdrawal delay period since 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐷 records transfer_time.</p>\n<p>\\\nIf Alice is conforming, then If Bob provides two different signatures to different buyers, Alice can extract 𝑠𝑘𝐵 and submit it to obtain 𝐴𝑠𝑠𝑒𝑡𝐺 and 𝐴𝑠𝑠𝑒𝑡𝐵. If Bob or Dave publishes one signature exclusively on either 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐴 or 𝐶𝑜𝑛𝑡𝑟𝑎𝑐𝑡𝐵, Alice can forward this signature to another chain to make sure the exercise secret hashlocks are consistent on two chains. If Bob and Dave collude, they use two signatures to change the hashlock. During the withdrawal delay period, Alice can obtain 𝐴𝑠𝑠𝑒𝑡𝐺 and 𝐴𝑠𝑠𝑒𝑡𝐵 using the extracted 𝑠𝑘𝐵.</p>\n<p>\\\nTransferring Alice’s position to Carol is simpler, as Alice does not deposit assets into the option contracts and cannot modify the exercise secret hashlock. Carol only needs to ensure consistency between the holders on the two chains. Otherwise, she can extract 𝑠𝑘𝐴 and refund the 𝐻𝑜𝑙𝑑𝑒𝑟𝐹𝑒𝑒 during the withdrawal delay period.</p>\n<p>\\\nTheorem 5. <em>Protocol 4.2.2 satisfies isolation: Alice and Bob can simultaneously and separately transfer their positions to Carol and Dave, respectively. This means that transferring holder and the transferring writer can proceed concurrently.</em></p>\n<p>\\\nProof. Suppose both Carol and Dave are interested in Alice’s and Bob’s positions, respectively. According to Lemma 9, Alice transferring to Carol does not require Bob’s involvement, hence Alice and Carol will not be interfered with. Similarly, it is known that during Bob’s transfer to Dave, by Lemma 10, if Bob and Dave are both compliant, Alice does not need to participate. Considering the case when Bob reveals two different signatures: (i) If Carol has already revealed the secret value 𝐶 of the transfer hash lock, then Carol becomes the new holder and can use two different signatures by 𝑠𝐵 to obtain 𝐴𝑠𝑠𝑒𝑡𝐵 and 𝐴𝑠𝑠𝑒𝑡𝐺 . (ii) If Carol has not revealed 𝐶 and will reveal it after Δ, Carol can simultaneously reveal𝐶 and call 𝑟𝑒𝑐𝑙𝑎𝑖𝑚() on both chains after Δ to obtain 𝐴𝑠𝑠𝑒𝑡𝐵 and 𝐴𝑠𝑠𝑒𝑡𝐺 . If Dave or Bob publishes 𝜎𝑚 on one single chain, Carol must forward 𝜎𝑚 to the other chain while revealing 𝐶.</p>\n<p>\\</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Zifan Peng, The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, Guangdong, China (zpengao@connect.hkust-gz.edu.cn);</p>\n<p>(2) Yingjie Xue, The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, Guangdong, China (yingjiexue@hkust-gz.edu.cn);</p>\n<p>(3) Jingyu Liu, The Hong Kong University of Science and Technology (Guangzhou) Guangzhou, Guangdong, China (jliu514@connect.hkust-gz.edu.cn).</p>\n<p>:::</p>\n<hr />\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2410.15724\">available on arxiv</a> under CC BY 4.0 license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"What Happens to Relicensed Open Source Projects and Their Forks?","url":"https://news.slashdot.org/story/24/12/28/2012251/what-happens-to-relicensed-open-source-projects-and-their-forks?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735418040,"author":"EditorDavid","unread":true,"desc":"","content":"A Linux Foundation project focused on understanding the health of the open source community just studied the outcomes for three projects that switched to \"more restrictive\" licenses and then faced community forks. \n\nThe data science director for the project &mdash; known as Community Health Analytics in Open Source Software (or CHAOSS) &mdash; is also an OpenUK board member, and describes the outcomes for OpenSearch, Redis with fork Valkey, and Terraform:\n\n\nThe relicensed project (Redis) had significant numbers of contributors who were not employed by the company, and the fork (Valkey) was created by those existing contributors as a foundation project... The Redis project differs from Elasticsearch and Terraform in the number of contributions to the Redis repository from people who were not employees of Redis. In the year leading up to the relicense, when Redis was still open source, there were substantial contributions from employees of other companies: Twice as many non-Redis employees made five or more commits, and about a dozen employees of other companies made almost twice as many commits as Redis employees made. \n\nIn the six months after the relicense, all of the external contributors from companies (including Amazon, Alibaba, Tencent, Huawei and Ericsson) who contributed over five commits to the Redis project in the year prior to the relicense stopped contributing. In sum, Redis had strong organizational diversity before the relicense, but only Redis employees made significant contributions afterward. \n\nValkey was forked from Redis 7.2.4 on March 28, 2024, as a Linux Foundation project under the BSD-3 license. The fork was driven by a group of people who previously contributed to Redis with public support from their employers. Within its first six months, the Valkey repository had 29 contributors employed at 10 companies, and 18 of those people previously contributed to Redis. Valkey has a diverse set of contributors from various companies, with Amazon having the most contributors. \nThe results weren't always so clear-cut. Because Terraform always had very few contributors outside of the company, \"there was no substantial impact on the contributor community from the relicensing event...\" (Although the OpenTofu fork &mdash; a Linux Foundation project &mdash; had 31 people at 11 organizations who made five or more contributions.) \n\nAnd both before and after Elasticsearch's relicensing, most contributors were Elastic employees, so \"the 2021 relicense had little to no impact on contributors.\" (But the OpenSearch fork &mdash; transferred in September to the Linux Foundation &mdash; shows a more varied contributor base, with just 63% of additions and 64% of deletions coming from Amazon employees who made 10 or more commits. Six people who didn't work for Amazon made 10 or more commits, making up 11% of additions and 13% of deletions.\") \n\nSo \"Looking at all of these projects together, we see that the forks from relicensed projects tend to have more organizational diversity than the original projects,\" they conclude, adding that in general \"projects with greater organizational diversity tend to be more sustainable...\" \n\n\"You can dive into the details about these six projects in the paper, presentation and data we shared at the recent OpenForum Academy Symposium.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=What+Happens+to+Relicensed+Open+Source+Projects+and+Their+Forks%3F+%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F28%2F2012251%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F28%2F2012251%2Fwhat-happens-to-relicensed-open-source-projects-and-their-forks%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/28/2012251/what-happens-to-relicensed-open-source-projects-and-their-forks?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563559&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Magnus Carlsen Quits Chess Tournament After Refusing to Change Out of Jeans","url":"https://games.slashdot.org/story/24/12/28/1759232/magnus-carlsen-quits-chess-tournament-after-refusing-to-change-out-of-jeans?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735414440,"author":"EditorDavid","unread":true,"desc":"","content":" Magnus Carlsen quit the World Rapid Chess Championship on Friday, reports CNN, \"after he refused to change out of the jeans he was wearing...\" \n\n\"Carlsen, the world champion from 2013 until 2023, allegedly replied, 'I'm out, f*** you,' after being informed that he would not be permitted to continue,\" reports the Hindustan Times. \n\nThe International Chess Federation (or FIDE) \"said in a statement that Carlsen breached the tournament's dress code by wearing jeans,\" reports CNN:\n\n\nAs a result, Carlsen would not have been paired for round nine, though he could have returned for the rest of the tournament had he not decided to walk away, per Chess.com. Since he had performed poorly in the earlier rounds, there was little chance that Carlsen could have defended his title regardless.... \n\nThe standoff became \"a matter of principle\" for Carlsen, he told chess channel Take Take Take. \"I haven't appealed, honestly I'm too old at this point to care too much, if this is what they want to do ... nobody wants to back down, if this is where we are, that's fine by me,\" he said. \"I'll probably head off to somewhere where the weather is a bit nicer than here and that's it.\" He explained that he had been at a lunch meeting before heading to the tournament's second day and \"barely had time to go the room, change, put on a shirt, jacket and honestly I didn't even think about the jeans.\" \n\n\nCarlsen was also fined $200, according to the article. He has now also withdrawn from the World Blitz Championship which follows this tournament.\n\n In a statement, the FIDE said their dress code and other regulations \"are designed to ensure professionalism and fairness for all participants,\" and that the federation \"remains committed to promoting chess and its values, including respect for the rules that all participants agree to follow.\" \n\nThe group's CEO added \"Rules are applicable to all the participants, and it would be unfair towards all players who respected the dress-code, and those who were previously fined.\" (They added that \"We gave Magnus more than enough time to change. But as he had stated himself in his interview &mdash; it became a matter of principle for him.\") \n\nCNN notes that Carlsen has already won five world rapid and seven world blitz titles in the last 10 years...<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Magnus+Carlsen+Quits+Chess+Tournament+After+Refusing+to+Change+Out+of+Jeans%3A+https%3A%2F%2Fgames.slashdot.org%2Fstory%2F24%2F12%2F28%2F1759232%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fgames.slashdot.org%2Fstory%2F24%2F12%2F28%2F1759232%2Fmagnus-carlsen-quits-chess-tournament-after-refusing-to-change-out-of-jeans%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://games.slashdot.org/story/24/12/28/1759232/magnus-carlsen-quits-chess-tournament-after-refusing-to-change-out-of-jeans?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563507&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"How vLLM Implements Decoding Algorithms","url":"https://hackernoon.com/how-vllm-implements-decoding-algorithms?source=rss","date":1735412415,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/UZfnXsE8xpuPm7xJGzSd\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Joq6DOP9CkD0S4F8MX1r\">2 Background and 2.1 Transformer-Based Large Language Models</a></p>\n<p><a href=\"http://hackernoon.com/preview/25g0m9nrxol991ZY9qci\">2.2 LLM Service & Autoregressive Generation</a></p>\n<p><a href=\"http://hackernoon.com/preview/IQ9Fd8hlh5MpHVMFA27O\">2.3 Batching Techniques for LLMs</a></p>\n<p><a href=\"https://hackernoon.com/preview/ytSMq2pxVtKIRC7iS3kK\">3 Memory Challenges in LLM Serving</a></p>\n<p><a href=\"http://hackernoon.com/preview/6BZmg60VAii9DNx0L1qc\">3.1 Memory Management in Existing Systems</a></p>\n<p><a href=\"http://hackernoon.com/preview/ZzGN1QRkg16N6zN7hCJi\">4 Method and 4.1 PagedAttention</a></p>\n<p><a href=\"http://hackernoon.com/preview/4Da8juHWnURn6g4urrl4\">4.2 KV Cache Manager</a></p>\n<p><a href=\"http://hackernoon.com/preview/GieTLdxmIGxSHDideqV5\">4.3 Decoding with PagedAttention and vLLM</a></p>\n<p><a href=\"http://hackernoon.com/preview/7g3isP8BzgyEsTasNUr2\">4.4 Application to Other Decoding Scenarios</a></p>\n<p><a href=\"http://hackernoon.com/preview/TEjiBAga2TDQ1ZZp5Vbg\">4.5 Scheduling and Preemption</a></p>\n<p><a href=\"http://hackernoon.com/preview/OjYLeVPi5jM2NSWKvipy\">4.6 Distributed Execution</a></p>\n<p><a href=\"http://hackernoon.com/preview/yqADDmfsCggYCcSdA6Nj\">5 Implementation</a></p>\n<p><a href=\"http://hackernoon.com/preview/S6sF02nTOzlXKZE1iQcP\">6 Evaluation and 6.1 Experimental Setup</a></p>\n<p><a href=\"http://hackernoon.com/preview/K3TeA7MNOi0g142ICu5W\">6.2 Basic Sampling</a></p>\n<p><a href=\"http://hackernoon.com/preview/cJxKQHExtKQrburnndpo\">6.3 Parallel Sampling and Beam Search</a></p>\n<p><a href=\"http://hackernoon.com/preview/JEOm7WvIfRfxEjepPqXW\">6.4 Shared prefix</a></p>\n<p><a href=\"http://hackernoon.com/preview/DWk5D3KHLfKhdQC02ayZ\">6.5 Chatbot</a></p>\n<p><a href=\"http://hackernoon.com/preview/l7VvLlkEDSBJ5GzMlwJf\">7 Ablation Studies</a></p>\n<p><a href=\"http://hackernoon.com/preview/PJb0S41IDQAbwYp0e6RZ\">8 Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/5d2sL9hRMUBNmVCZWMBC\">9 Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/HL77hmYOoM9MPB5fKgmq\">10 Conclusion, Acknowledgement and References</a></p>\n<h2 id=\"5implementation\">5 Implementation</h2>\n<p>vLLM is an end-to-end serving system with a FastAPI [15] frontend and a GPU-based inference engine. The frontend extends the OpenAI API [34] interface, allowing users to customize sampling parameters for each request, such as the maximum sequence length and the beam width 𝑘. The vLLM engine is written in 8.5K lines of Python and 2K lines of C++/CUDA code. We develop control-related components including the scheduler and the block manager in Python while developing custom CUDA kernels for key operations such as PagedAttention. For the model executor, we implement popular LLMs such as GPT [5], OPT [62], and LLaMA [52] using</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-zt830gd.png\" alt=\"Figure 11. Input and output length distributions of the (a) ShareGPT and (b) Alpaca datasets.\" /></p>\n<p>\\\nPyTorch [39] and Transformers [58]. We use NCCL [32] for tensor communication across the distributed GPU workers.</p>\n<h3 id=\"51kernelleveloptimization\">5.1 Kernel-level Optimization</h3>\n<p>Since PagedAttention introduces memory access patterns that are not efficiently supported by existing systems, we develop several GPU kernels for optimizing it. (1) Fused reshape and block write. In every Transformer layer, the new KV cache are split into blocks, reshaped to a memory layout optimized for block read, then saved at positions specified by the block table. To minimize kernel launch overheads, we fuse them into a single kernel. (2) Fusing block read and attention. We adapt the attention kernel in FasterTransformer [31] to read KV cache according to the block table and perform attention operations on the fly. To ensure coalesced memory access, we assign a GPU warp to read each block. Moreover, we add support for variable sequence lengths within a request batch. (3) Fused block copy. Block copy operations, issued by the copy-on-write mechanism, may operate on discontinuous blocks. This can lead to numerous invocations of small data movements if we use the cudaMemcpyAsync API. To mitigate the overhead, we implement a kernel that batches the copy operations for different blocks into a single kernel launch.</p>\n<h3 id=\"52supportingvariousdecodingalgorithms\">5.2 Supporting Various Decoding Algorithms</h3>\n<p>vLLM implements various decoding algorithms using three key methods: fork, append, and free. The fork method creates a new sequence from an existing one. The append method appends a new token to the sequence. Finally, the free method deletes the sequence. For instance, in parallel sampling, vLLM creates multiple output sequences from the single input sequence using the fork method. It then adds new tokens to these sequences in every iteration with append, and deletes sequences that meet a stopping condition using free. The same strategy is also applied in beam search and prefix sharing by vLLM. We believe future decoding algorithms can also be supported by combining these methods.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2309.06180\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Woosuk Kwon, UC Berkeley with Equal contribution;</p>\n<p>(2) Zhuohan Li, UC Berkeley with Equal contribution;</p>\n<p>(3) Siyuan Zhuang, UC Berkeley;</p>\n<p>(4) Ying Sheng, UC Berkeley and Stanford University;</p>\n<p>(5) Lianmin Zheng, UC Berkeley;</p>\n<p>(6) Cody Hao Yu, Independent Researcher;</p>\n<p>(7) Cody Hao Yu, Independent Researcher;</p>\n<p>(8) Joseph E. Gonzalez, UC Berkeley;</p>\n<p>(9) Hao Zhang, UC San Diego;</p>\n<p>(10) Ion Stoica, UC Berkeley.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New York Passes Law Making Fossil Fuel Companies Pay $75 Billion for 'Climate Superfund'","url":"https://news.slashdot.org/story/24/12/28/0613231/new-york-passes-law-making-fossil-fuel-companies-pay-75-billion-for-climate-superfund?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735410840,"author":"EditorDavid","unread":true,"desc":"","content":"Thursday New York's governor signed new legislation \"to hold polluters responsible for the damage done to our environment\" by establishing a Climate Superfund that's paid for by big fossil-fuel companies. \n\nThe money will be used for \"climate change adaptation,\" according to New York state senator Liz Krueger, who notes that the legislation follows \"the polluter-pays model\" used in America's already-existing federal and state superfund laws. Spread out over 25 years, the legislation collects an average of $3 billion each year &mdash; or $75 billion &mdash; \"from the parties most responsible for causing the climate crisis &mdash; big oil and gas companies.\" \n\n\"The Climate Change Superfund Act is now law, and New York has fired a shot that will be heard round the world: the companies most responsible for the climate crisis will be held accountable,\" said Senator Krueger. \"Too often over the last decade, courts have dismissed lawsuits against the oil and gas industry by saying that the issue of climate culpability should be decided by legislatures. Well, the Legislature of the State of New York &mdash; the 10th largest economy in the world &mdash; has accepted the invitation, and I hope we have made ourselves very clear: the planet's largest climate polluters bear a unique responsibility for creating the climate crisis, and they must pay their fair share to help regular New Yorkers deal with the consequences. \n\n\"And there's no question that those consequences are here, and they are serious,\" Krueger continued. \"Repairing from and preparing for extreme weather caused by climate change will cost more than half a trillion dollars statewide by 2050. That's over $65,000 per household, and that's on top of the disruption, injury, and death that the climate crisis is causing in every corner of our state. The Climate Change Superfund Act is a critical piece of affordability legislation that will deliver billions of dollars every year to ease the burden on regular New Yorkers....\" \n\nStarting in the 1970s, scientists working for Exxon made \"remarkably accurate projections of just how much burning fossil fuels would warm the planet.\" Yet for years, \"the oil giant publicly cast doubt on climate science, and cautioned against any drastic move away from burning fossil fuels, the main driver of climate change.\"\n \n\"The oil giant Saudi Aramco of Saudi Arabia could be slapped with the largest annual assessment of any company &mdash; $640 million a year &mdash; for emitting 31,269 million tons of greenhouse gases from 2000 to 2020,\" notes the New York Post. \n\nAnd \"The law will also standardize the number of emissions tied to the fuel produced by companies,\" reports the Times Union newspaper. \"[F]or every 1 million pounds of coal, for example, the program assigns over 942 metric tons of carbon dioxide. For every 1 million barrels of crude oil, an entity is considered to have produced 432,180 metric tons of carbon dioxide.\"\n\nAmong the infrastructure programs the superfund program aims to pay for: coastal wetlands restoration, energy efficient cooling systems in buildings, including schools and new housing developments, and stormwater drainage upgrades. \n\n\n New York is now the second U.S. state with a \"climate Superfund\" law, according to Bloomberg Law, with New York following the lead of Vermont. \"Maryland, Massachusetts, and California are also considering climate Superfund laws to manage mounting infrastructure costs.\"\n\nThe American Petroleum Institute, which represents about 600 members of the industry, condemned the law. \"This type of legislation represents nothing more than a punitive new fee on American energy, and we are evaluating our options moving forward,\" an API spokesperson said in an emailed statement... The bills &mdash; modeled after the federal Comprehensive Environmental Response, Compensation, and Liability Act, known as Superfund &mdash; would almost certainly spur swift litigation from fossil fuel companies upon enactment, legal educators say.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=New+York+Passes+Law+Making+Fossil+Fuel+Companies+Pay+%2475+Billion+for+'Climate+Superfund'%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F28%2F0613231%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F28%2F0613231%2Fnew-york-passes-law-making-fossil-fuel-companies-pay-75-billion-for-climate-superfund%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/28/0613231/new-york-passes-law-making-fossil-fuel-companies-pay-75-billion-for-climate-superfund?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563195&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"LLaVA-Phi: The Training We Put It Through","url":"https://hackernoon.com/llava-phi-the-training-we-put-it-through?source=rss","date":1735410611,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"https://hackernoon.com/preview/0xvVYI3OvJLvwRUYFnhV\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Jo6taCeMb5To49SvqmUy\">2. Related Work</a></p>\n<p><a href=\"https://hackernoon.com/preview/8re4SLMd5eLulCS8byQf\">3. LLaVA-Phi and 3.1. Training</a></p>\n<p><a href=\"https://hackernoon.com/preview/XmeEoU4AZVnfg5r6ymbU\">3.2. Qualitative Results</a></p>\n<p><a href=\"https://hackernoon.com/preview/bk1VFYQ8qOylIqzav05R\">4. Experiments</a></p>\n<p><a href=\"https://hackernoon.com/preview/WwxpDAUyfNGjRGKmxiF4\">5. Conclusion, Limitation, and Future Works and References</a></p>\n<h2 id=\"3llavaphi\">3. LLaVA-Phi</h2>\n<p>Our overall network architecture is similar to LLaVA-1.5. We use the pre-trained CLIP ViT-L/14 with a resolution of 336x336 as the visual encoder. A two-layer MLP is adopted to improve the connection of the visual encoder and LLM.</p>\n<h2 id=\"31training\">3.1. Training</h2>\n<p><strong>Supervised fine-tuning on Phi-2.</strong> The publicly released Phi-2 model has not undergone fine-tuning. Previous research indicates that even a small amount of high-quality data can significantly enhance performance in areas such as mathematics, language reasoning, and coding tasks. In light of this, we employed supervised fine-tuning to further train Phi-2 using a select set of premium data. This data was organized in the Vicuna format. For our Supervised Fine-Tuning (SFT) data, we utilized ShareGPT from an open-source platform. The training was conducted over two epochs, beginning with an initial learning rate of 3e-5, which was linearly decreased over time. Our findings suggest that while this step might be optional, applying SFT to Phi-2 does result in modest improvements across most benchmarks.</p>\n<p><img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-1283xx7.png\" alt=\"\" /></p>\n<p><img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-u793xy7.png\" alt=\"Figure 3. LLaVA-Phi is capable of performing accurate OCR on mathematical equations and solving them correspondingly.\" /></p>\n<p><strong>Training LLaVA-Phi.</strong> Our training approach follows the pipeline used for LLaVA1.5, consisting of a pretraining stage and a subsequent instruction tuning phase. Initially, we kept the vision encoder and Phi-2 static, focusing exclusively on training the efficient projector. This step is followed by a comprehensive fine-tuning of both the projector and the language model (LLM), aiming to enhance their capabilities in visual comprehension and language processing.</p>\n<p>\\\nFor pre-training, we utilize a filtered subset of the CC-595K dataset [24] over one epoch, applying an initial learning rate of 1e-3 and a batch size of 256. Then, we finetune the model on LLaVA-Instruct-150K dataset for 1 epoch at a learning rate of 2e-5 and a batch size of 256. We implement a weight decay of 0.1 and utilize the Adam optimizer, characterized by momentum parameters of 0.9 and 0.98, and an epsilon value of 1e-7. We fine-tune all parameters in LLM instead of using LoRA.</p>\n<p>\\\n<strong>Computational Cost.</strong> Similar to LLaVA1.5, our training process is structured in two stages. For LLaVA-Phi, the pretraining phase takes 1.5 hours, followed by 8 hours dedicated to visual instruction tuning, utilizing 8 A100 GPUs. The integration of techniques such as LoRA [15] and QLoRA [9] has the potential to significantly reduce training time, a possibility we plan to explore in future work.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2401.02330\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Yichen Zhu, Midea Group;</p>\n<p>(2) Minjie Zhu, Midea Group and East China Normal University;</p>\n<p>(3) Ning Liu, Midea Group;</p>\n<p>(4) Zhicai Ou, Midea Group;</p>\n<p>(5) Xiaofeng Mou, Midea Group.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The Distributed Execution of vLLM","url":"https://hackernoon.com/the-distributed-execution-of-vllm?source=rss","date":1735407910,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/UZfnXsE8xpuPm7xJGzSd\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Joq6DOP9CkD0S4F8MX1r\">2 Background and 2.1 Transformer-Based Large Language Models</a></p>\n<p><a href=\"http://hackernoon.com/preview/25g0m9nrxol991ZY9qci\">2.2 LLM Service & Autoregressive Generation</a></p>\n<p><a href=\"http://hackernoon.com/preview/IQ9Fd8hlh5MpHVMFA27O\">2.3 Batching Techniques for LLMs</a></p>\n<p><a href=\"https://hackernoon.com/preview/ytSMq2pxVtKIRC7iS3kK\">3 Memory Challenges in LLM Serving</a></p>\n<p><a href=\"http://hackernoon.com/preview/6BZmg60VAii9DNx0L1qc\">3.1 Memory Management in Existing Systems</a></p>\n<p><a href=\"http://hackernoon.com/preview/ZzGN1QRkg16N6zN7hCJi\">4 Method and 4.1 PagedAttention</a></p>\n<p><a href=\"http://hackernoon.com/preview/4Da8juHWnURn6g4urrl4\">4.2 KV Cache Manager</a></p>\n<p><a href=\"http://hackernoon.com/preview/GieTLdxmIGxSHDideqV5\">4.3 Decoding with PagedAttention and vLLM</a></p>\n<p><a href=\"http://hackernoon.com/preview/7g3isP8BzgyEsTasNUr2\">4.4 Application to Other Decoding Scenarios</a></p>\n<p><a href=\"http://hackernoon.com/preview/TEjiBAga2TDQ1ZZp5Vbg\">4.5 Scheduling and Preemption</a></p>\n<p><a href=\"http://hackernoon.com/preview/OjYLeVPi5jM2NSWKvipy\">4.6 Distributed Execution</a></p>\n<p><a href=\"http://hackernoon.com/preview/yqADDmfsCggYCcSdA6Nj\">5 Implementation</a></p>\n<p><a href=\"http://hackernoon.com/preview/S6sF02nTOzlXKZE1iQcP\">6 Evaluation and 6.1 Experimental Setup</a></p>\n<p><a href=\"http://hackernoon.com/preview/K3TeA7MNOi0g142ICu5W\">6.2 Basic Sampling</a></p>\n<p><a href=\"http://hackernoon.com/preview/cJxKQHExtKQrburnndpo\">6.3 Parallel Sampling and Beam Search</a></p>\n<p><a href=\"http://hackernoon.com/preview/JEOm7WvIfRfxEjepPqXW\">6.4 Shared prefix</a></p>\n<p><a href=\"http://hackernoon.com/preview/DWk5D3KHLfKhdQC02ayZ\">6.5 Chatbot</a></p>\n<p><a href=\"http://hackernoon.com/preview/l7VvLlkEDSBJ5GzMlwJf\">7 Ablation Studies</a></p>\n<p><a href=\"http://hackernoon.com/preview/PJb0S41IDQAbwYp0e6RZ\">8 Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/5d2sL9hRMUBNmVCZWMBC\">9 Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/HL77hmYOoM9MPB5fKgmq\">10 Conclusion, Acknowledgement and References</a></p>\n<h2 id=\"46distributedexecution\">4.6 Distributed Execution</h2>\n<p>Many LLMs have parameter sizes exceeding the capacity of a single GPU [5, 9]. Therefore, it is necessary to partition them across distributed GPUs and execute them in a model parallel fashion [28, 63]. This calls for a memory manager capable of handling distributed memory. vLLM is effective in distributed settings by supporting the widely used Megatron-LM style tensor model parallelism strategy on Transformers [47]. This strategy adheres to an SPMD (Single Program Multiple Data) execution schedule, wherein the linear layers are partitioned</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-s3830ln.png\" alt=\"Table 1. Model sizes and server configurations.\" /></p>\n<p>\\\nto perform block-wise matrix multiplication, and the the GPUs constantly synchronize intermediate results via an all-reduce operation. Specifically, the attention operator is split on the attention head dimension, each SPMD process takes care of a subset of attention heads in multi-head attention.</p>\n<p>\\\nWe observe that even with model parallel execution, each model shard still processes the same set of input tokens, thus requiring the KV Cache for the same positions. Therefore, vLLM features a single KV cache manager within the centralized scheduler, as in Fig. 4. Different GPU workers share the manager, as well as the mapping from logical blocks to physical blocks. This common mapping allows GPU workers to execute the model with the physical blocks provided by the scheduler for each input request. Although each GPU worker has the same physical block IDs, a worker only stores a portion of the KV cache for its corresponding attention heads.</p>\n<p>\\\nIn each step, the scheduler first prepares the message with input token IDs for each request in the batch, as well as the block table for each request. Next, the scheduler broadcasts this control message to the GPU workers. Then, the GPU workers start to execute the model with the input token IDs. In the attention layers, the GPU workers read the KV cache according to the block table in the control message. </p>\n<p>\\\nDuring execution, the GPU workers synchronize the intermediate results with the all-reduce communication primitive without the coordination of the scheduler, as in [47]. In the end, the GPU workers send the sampled tokens of this iteration back to the scheduler. In summary, GPU workers do not need to synchronize on memory management as they only need to receive all the memory management information at the beginning of each decoding iteration along with the step inputs.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2309.06180\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Woosuk Kwon, UC Berkeley with Equal contribution;</p>\n<p>(2) Zhuohan Li, UC Berkeley with Equal contribution;</p>\n<p>(3) Siyuan Zhuang, UC Berkeley;</p>\n<p>(4) Ying Sheng, UC Berkeley and Stanford University;</p>\n<p>(5) Lianmin Zheng, UC Berkeley;</p>\n<p>(6) Cody Hao Yu, Independent Researcher;</p>\n<p>(7) Cody Hao Yu, Independent Researcher;</p>\n<p>(8) Joseph E. Gonzalez, UC Berkeley;</p>\n<p>(9) Hao Zhang, UC San Diego;</p>\n<p>(10) Ion Stoica, UC Berkeley.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"How vLLM Prioritizes a Subset of Requests","url":"https://hackernoon.com/how-vllm-prioritizes-a-subset-of-requests?source=rss","date":1735407010,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/UZfnXsE8xpuPm7xJGzSd\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Joq6DOP9CkD0S4F8MX1r\">2 Background and 2.1 Transformer-Based Large Language Models</a></p>\n<p><a href=\"http://hackernoon.com/preview/25g0m9nrxol991ZY9qci\">2.2 LLM Service & Autoregressive Generation</a></p>\n<p><a href=\"http://hackernoon.com/preview/IQ9Fd8hlh5MpHVMFA27O\">2.3 Batching Techniques for LLMs</a></p>\n<p><a href=\"https://hackernoon.com/preview/ytSMq2pxVtKIRC7iS3kK\">3 Memory Challenges in LLM Serving</a></p>\n<p><a href=\"http://hackernoon.com/preview/6BZmg60VAii9DNx0L1qc\">3.1 Memory Management in Existing Systems</a></p>\n<p><a href=\"http://hackernoon.com/preview/ZzGN1QRkg16N6zN7hCJi\">4 Method and 4.1 PagedAttention</a></p>\n<p><a href=\"http://hackernoon.com/preview/4Da8juHWnURn6g4urrl4\">4.2 KV Cache Manager</a></p>\n<p><a href=\"http://hackernoon.com/preview/GieTLdxmIGxSHDideqV5\">4.3 Decoding with PagedAttention and vLLM</a></p>\n<p><a href=\"http://hackernoon.com/preview/7g3isP8BzgyEsTasNUr2\">4.4 Application to Other Decoding Scenarios</a></p>\n<p><a href=\"http://hackernoon.com/preview/TEjiBAga2TDQ1ZZp5Vbg\">4.5 Scheduling and Preemption</a></p>\n<p><a href=\"http://hackernoon.com/preview/OjYLeVPi5jM2NSWKvipy\">4.6 Distributed Execution</a></p>\n<p><a href=\"http://hackernoon.com/preview/yqADDmfsCggYCcSdA6Nj\">5 Implementation</a></p>\n<p><a href=\"http://hackernoon.com/preview/S6sF02nTOzlXKZE1iQcP\">6 Evaluation and 6.1 Experimental Setup</a></p>\n<p><a href=\"http://hackernoon.com/preview/K3TeA7MNOi0g142ICu5W\">6.2 Basic Sampling</a></p>\n<p><a href=\"http://hackernoon.com/preview/cJxKQHExtKQrburnndpo\">6.3 Parallel Sampling and Beam Search</a></p>\n<p><a href=\"http://hackernoon.com/preview/JEOm7WvIfRfxEjepPqXW\">6.4 Shared prefix</a></p>\n<p><a href=\"http://hackernoon.com/preview/DWk5D3KHLfKhdQC02ayZ\">6.5 Chatbot</a></p>\n<p><a href=\"http://hackernoon.com/preview/l7VvLlkEDSBJ5GzMlwJf\">7 Ablation Studies</a></p>\n<p><a href=\"http://hackernoon.com/preview/PJb0S41IDQAbwYp0e6RZ\">8 Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/5d2sL9hRMUBNmVCZWMBC\">9 Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/HL77hmYOoM9MPB5fKgmq\">10 Conclusion, Acknowledgement and References</a></p>\n<h2 id=\"45schedulingandpreemption\">4.5 Scheduling and Preemption</h2>\n<p>When the request traffic surpasses the system’s capacity, vLLM must prioritize a subset of requests. In vLLM, we adopt the first-come-first-serve (FCFS) scheduling policy for all requests, ensuring fairness and preventing starvation. When vLLM needs to preempt requests, it ensures that the earliest arrived requests are served first and the latest requests are preempted first.</p>\n<p>\\\nLLM services face a unique challenge: the input prompts for an LLM can vary significantly in length, and the resulting output lengths are not known a priori, contingent on both the input prompt and the model. As the number of requests and their outputs grow, vLLM can run out of the GPU’s physical blocks to store the newly generated KV cache. There are two classic questions that vLLM needs to answer in this context: (1) Which blocks should it evict? (2) How to recover evicted blocks if needed again? </p>\n<p>\\\nTypically, eviction policies use heuristics to predict which block will be accessed furthest in the future and evict that block. Since in our case we know that all blocks of a sequence are accessed together, we implement an all-or-nothing eviction policy, i.e., either evict all or none of the blocks of a sequence. Furthermore, multiple sequences within one request (e.g., beam candidates in one beam search request) are gang-scheduled as a sequence group. The sequences within one sequence group are always preempted or rescheduled together due to potential memory sharing across those sequences. To answer the second question of how to recover an evicted block, we consider two techniques:</p>\n<p>\\\n<strong>Swapping</strong>. This is the classic technique used by most virtual memory implementations which copy the evicted pages to a swap space on the disk. In our case, we copy evicted blocks to the CPU memory. As shown in Fig. 4, besides the GPU block allocator, vLLM includes a CPU block allocator to manage the physical blocks swapped to CPU RAM. When vLLM exhausts free physical blocks for new tokens, it selects a set of sequences to evict and transfer their KV cache to the CPU. </p>\n<p>\\\nOnce it preempts a sequence and evicts its blocks, vLLM stops accepting new requests until all preempted sequences are completed. Once a request completes, its blocks are freed from memory, and the blocks of a preempted sequence are brought back in to continue the processing of that sequence. Note that with this design, the number of blocks swapped to the CPU RAM never exceeds the number of total physical blocks in the GPU RAM, so the swap space on the CPU RAM is bounded by the GPU memory allocated for the KV cache.</p>\n<p>\\\n<strong>Recomputation</strong>. In this case, we simply recompute the KV cache when the preempted sequences are rescheduled. Note that recomputation latency can be significantly lower than the original latency, as the tokens generated at decoding can be concatenated with the original user prompt as a new prompt—their KV cache at all positions can be generated in one prompt phase iteration.</p>\n<p>\\\nThe performances of swapping and recomputation depend on the bandwidth between CPU RAM and GPU memory and the computation power of the GPU. We examine the speeds of swapping and recomputation in §7.3.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2309.06180\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Woosuk Kwon, UC Berkeley with Equal contribution;</p>\n<p>(2) Zhuohan Li, UC Berkeley with Equal contribution;</p>\n<p>(3) Siyuan Zhuang, UC Berkeley;</p>\n<p>(4) Ying Sheng, UC Berkeley and Stanford University;</p>\n<p>(5) Lianmin Zheng, UC Berkeley;</p>\n<p>(6) Cody Hao Yu, Independent Researcher;</p>\n<p>(7) Cody Hao Yu, Independent Researcher;</p>\n<p>(8) Joseph E. Gonzalez, UC Berkeley;</p>\n<p>(9) Hao Zhang, UC San Diego;</p>\n<p>(10) Ion Stoica, UC Berkeley.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"LLaVA-Phi: Related Work to Get You Caught Up","url":"https://hackernoon.com/llava-phi-related-work-to-get-you-caught-up?source=rss","date":1735406114,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"https://hackernoon.com/preview/0xvVYI3OvJLvwRUYFnhV\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Jo6taCeMb5To49SvqmUy\">2. Related Work</a></p>\n<p><a href=\"https://hackernoon.com/preview/8re4SLMd5eLulCS8byQf\">3. LLaVA-Phi and 3.1. Training</a></p>\n<p><a href=\"https://hackernoon.com/preview/XmeEoU4AZVnfg5r6ymbU\">3.2. Qualitative Results</a></p>\n<p><a href=\"https://hackernoon.com/preview/bk1VFYQ8qOylIqzav05R\">4. Experiments</a></p>\n<p><a href=\"https://hackernoon.com/preview/WwxpDAUyfNGjRGKmxiF4\">5. Conclusion, Limitation, and Future Works and References</a></p>\n<h2 id=\"2relatedwork\">2. Related Work</h2>\n<p>The rapid advancements in Large Language Models (LLMs) have significantly propelled the development of vision-language models based on LLMs. These models, representing a departure from the capabilities of the preLLM era, are equipped with advanced question-answering and visual comprehension skills. This progress is enabled by using LLMs as language encoding modules. Notable research in this domain includes the LLaVA-family [24, 25, 26, 32], the BLIP-family [8, 20], MiniGPT-4 [37], and others. Each has demonstrated significant advancements in managing visual-centric dialogues. However, a common limitation of these open-sourced Vision-Language Models (VLMs) is their substantial computational demands, typically ranging from 7B to 65B parameters. This requirement poses challenges for deployment on edge or mobile devices, especially in real-time applications. Gemini [33], a leader in this field, has released three versions of visionlanguage models, including the compact Gemini-Nano with 1.8B/3.25B parameters, tailored for smartphones. However, their models and data are not open-sourced. Another initiative, MobileVLM [6], has developed mobileLLaMA with 2.7B parameters to facilitate smaller vision-language models. Our paper explores and demonstrates the effectiveness of integrating vision-language models with open-sourced, smaller language models, assessing their potential and efficiency in a variety of applications.</p>\n<p><img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-0h83x51.png\" alt=\"Figure 1. LLaVA-Phi is adept at identifying and responding to complex questions with empathetic reasoning.\" /></p>\n<p><img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-jz93xue.png\" alt=\"Figure 2. LLaVA-Phi can generate useful codes based on visual input and commands.\" /></p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2401.02330\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Yichen Zhu, Midea Group;</p>\n<p>(2) Minjie Zhu, Midea Group and East China Normal University;</p>\n<p>(3) Ning Liu, Midea Group;</p>\n<p>(4) Zhicai Ou, Midea Group;</p>\n<p>(5) Xiaofeng Mou, Midea Group.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"How vLLM Can Be Applied to Other Decoding Scenarios","url":"https://hackernoon.com/how-vllm-can-be-applied-to-other-decoding-scenarios?source=rss","date":1735406110,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/UZfnXsE8xpuPm7xJGzSd\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Joq6DOP9CkD0S4F8MX1r\">2 Background and 2.1 Transformer-Based Large Language Models</a></p>\n<p><a href=\"http://hackernoon.com/preview/25g0m9nrxol991ZY9qci\">2.2 LLM Service & Autoregressive Generation</a></p>\n<p><a href=\"http://hackernoon.com/preview/IQ9Fd8hlh5MpHVMFA27O\">2.3 Batching Techniques for LLMs</a></p>\n<p><a href=\"https://hackernoon.com/preview/ytSMq2pxVtKIRC7iS3kK\">3 Memory Challenges in LLM Serving</a></p>\n<p><a href=\"http://hackernoon.com/preview/6BZmg60VAii9DNx0L1qc\">3.1 Memory Management in Existing Systems</a></p>\n<p><a href=\"http://hackernoon.com/preview/ZzGN1QRkg16N6zN7hCJi\">4 Method and 4.1 PagedAttention</a></p>\n<p><a href=\"http://hackernoon.com/preview/4Da8juHWnURn6g4urrl4\">4.2 KV Cache Manager</a></p>\n<p><a href=\"http://hackernoon.com/preview/GieTLdxmIGxSHDideqV5\">4.3 Decoding with PagedAttention and vLLM</a></p>\n<p><a href=\"http://hackernoon.com/preview/7g3isP8BzgyEsTasNUr2\">4.4 Application to Other Decoding Scenarios</a></p>\n<p><a href=\"http://hackernoon.com/preview/TEjiBAga2TDQ1ZZp5Vbg\">4.5 Scheduling and Preemption</a></p>\n<p><a href=\"http://hackernoon.com/preview/OjYLeVPi5jM2NSWKvipy\">4.6 Distributed Execution</a></p>\n<p><a href=\"http://hackernoon.com/preview/yqADDmfsCggYCcSdA6Nj\">5 Implementation</a></p>\n<p><a href=\"http://hackernoon.com/preview/S6sF02nTOzlXKZE1iQcP\">6 Evaluation and 6.1 Experimental Setup</a></p>\n<p><a href=\"http://hackernoon.com/preview/K3TeA7MNOi0g142ICu5W\">6.2 Basic Sampling</a></p>\n<p><a href=\"http://hackernoon.com/preview/cJxKQHExtKQrburnndpo\">6.3 Parallel Sampling and Beam Search</a></p>\n<p><a href=\"http://hackernoon.com/preview/JEOm7WvIfRfxEjepPqXW\">6.4 Shared prefix</a></p>\n<p><a href=\"http://hackernoon.com/preview/DWk5D3KHLfKhdQC02ayZ\">6.5 Chatbot</a></p>\n<p><a href=\"http://hackernoon.com/preview/l7VvLlkEDSBJ5GzMlwJf\">7 Ablation Studies</a></p>\n<p><a href=\"http://hackernoon.com/preview/PJb0S41IDQAbwYp0e6RZ\">8 Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/5d2sL9hRMUBNmVCZWMBC\">9 Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/HL77hmYOoM9MPB5fKgmq\">10 Conclusion, Acknowledgement and References</a></p>\n<h2 id=\"44applicationtootherdecodingscenarios\">4.4 Application to Other Decoding Scenarios</h2>\n<p>§4.3 shows how PagedAttention and vLLM handle basic decoding algorithms, such as greedy decoding and sampling, that take one user prompt as input and generate a single output sequence. In many successful LLM applications [18, 34], an LLM service must offer more complex decoding scenarios that exhibit complex accessing patterns and more opportunities for memory sharing. We show the general applicability of vLLM on them in this section.</p>\n<p>\\\n<strong>Parallel sampling.</strong> In LLM-based program assistants [6, 18], an LLM generates multiple sampled outputs for a single input prompt; users can choose a favorite output from various candidates. So far we have implicitly assumed that a request</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-4x830j1.png\" alt=\"Figure 8. Parallel sampling example.\" /></p>\n<p>\\\ngenerates a single sequence. In the remainder of this paper, we assume the more general case in which a request generates multiple sequences. In parallel sampling, one request includes multiple samples sharing the same input prompt, allowing the KV cache of the prompt to be shared as well. Via its PagedAttention and paged memory management, vLLM can realize this sharing easily and save memory.</p>\n<p>\\\nFig. 8 shows an example of parallel decoding for two outputs. Since both outputs share the same prompt, we only reserve space for one copy of the prompt’s state at the prompt phase; the logical blocks for the prompts of both sequences are mapped to the same physical blocks: the logical block 0 and 1 of both sequences are mapped to physical blocks 7 and 1, respectively. Since a single physical block can be mapped to multiple logical blocks, we introduce a reference count for each physical block. In this case, the reference counts for physical blocks 7 and 1 are both 2. At the generation phase, the two outputs sample different output tokens and need separate storage for KV cache. vLLM implements a copy-onwrite mechanism at the block granularity for the physical blocks that need modification by multiple sequences, similar to the copy-on-write technique in OS virtual memory (e.g., when forking a process). Specifically, in Fig. 8, when sample A1 needs to write to its last logical block (logical block 1), vLLM recognizes that the reference count of the corresponding physical block (physical block 1) is greater than 1; it allocates a new physical block (physical block 3), instructs the block engine to copy the information from physical block 1, and decreases the reference count to 1. Next, when sample A2 writes to physical block 1, the reference count is already reduced to 1; thus A2 directly writes its newly generated KV cache to physical block 1.</p>\n<p>\\\nIn summary, vLLM enables the sharing of most of the space used to store the prompts’ KV cache across multiple output samples, with the exception of the final logical block, which is managed by a copy-on-write mechanism. By sharing physical blocks across multiple samples, memory usage can be greatly reduced, especially for long input prompts.</p>\n<p>\\\n<strong>Beam search.</strong> In LLM tasks like machine translation [59], the users expect the top-𝑘 most appropriate translations output by the LLM. Beam search [49] is widely used to decode the most probable output sequence from an LLM, as it mitigates the computational complexity of fully traversing the</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-26930hy.png\" alt=\"Figure 9. Beam search example.\" /></p>\n<p>\\\nsample space. The algorithm relies on the beam width parameter 𝑘, which determines the number of top candidates retained at every step. During decoding, beam search expands each candidate sequence in the beam by considering all possible tokens, computes their respective probabilities using the LLM, and retains the top-𝑘 most probable sequences out of 𝑘 · |𝑉 | candidates, where |𝑉 | is the vocabulary size.</p>\n<p>\\\nUnlike parallel decoding, beam search facilities sharing not only the initial prompt blocks but also other blocks across different candidates, and the sharing patterns dynamically change as the decoding process advances, similar to the process tree in the OS created by compound forks. Fig. 9 shows how vLLM manages the KV blocks for a beam search example with 𝑘 = 4. Prior to the iteration illustrated as the dotted line, each candidate sequence has used 4 full logical blocks. All beam candidates share the first block 0 (i.e., prompt). Candidate 3 digresses from others from the second block. Candidates 0-2 share the first 3 blocks and diverge at the fourth block. At subsequent iterations, the top-4 probable candidates all originate from candidates 1 and 2. As the original candidates 0 and 3 are no longer among the top candidates, their logical blocks are freed, and the reference counts of corresponding physical blocks are reduced. vLLM frees all physical blocks whose reference counts reach 0 (blocks 2, 4, 5, 8). Then, vLLM allocates new physical blocks (blocks 9-12) to store the new KV cache from the new candidates. Now, all candidates share blocks 0, 1, 3; candidates 0 and 1 share block 6, and candidates 2 and 3 further share block 7.</p>\n<p>\\\nPrevious LLM serving systems require frequent memory copies of the KV cache across the beam candidates. For example, in the case shown in Fig. 9, after the dotted line, candidate 3 would need to copy a large portion of candidate 2’s KV cache to continue generation. This frequent memory copy overhead is significantly reduced by vLLM’s physical block sharing. In vLLM, most blocks of different beam candidates can be shared. The copy-on-write mechanism is applied only when the newly generated tokens are within an old shared block, as in parallel decoding. This involves only copying one block of data.</p>\n<p>\\\n<strong>Shared prefix.</strong> Commonly, the LLM user provides a (long) description of the task including instructions and example inputs and outputs, also known as system prompt [36]. The description is concatenated with the actual task input to form the prompt of the request. The LLM generates outputs based</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-6ca30va.png\" alt=\"Figure 10. Shared prompt example for machine translation. The examples are adopted from [5].\" /></p>\n<p>\\\non the full prompt. Fig. 10 shows an example. Moreover, the shared prefix can be further tuned, via prompt engineering, to improve the accuracy of the downstream tasks [26, 27].</p>\n<p>\\\nFor this type of application, many user prompts share a prefix, thus the LLM service provider can store the KV cache of the prefix in advance to reduce the redundant computation spent on the prefix. In vLLM, this can be conveniently achieved by reserving a set of physical blocks for a set of predefined shared prefixes by the LLM service provider, as how OS handles shared library across processes. A user input prompt with the shared prefix can simply map its logical blocks to the cached physical blocks (with the last block marked copy-on-write). The prompt phase computation only needs to execute on the user’s task input.</p>\n<p>\\\n<strong>Mixed decoding methods.</strong> The decoding methods discussed earlier exhibit diverse memory sharing and accessing patterns. Nonetheless, vLLM facilitates the simultaneous processing of requests with different decoding preferences, which existing systems cannot efficiently do. This is because vLLM conceals the complex memory sharing between different sequences via a common mapping layer that translates logical blocks to physical blocks. The LLM and its execution kernel only see a list of physical block IDs for each sequence and do not need to handle sharing patterns across sequences. Compared to existing systems, this approach broadens the batching opportunities for requests with different sampling requirements, ultimately increasing the system’s overall throughput.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2309.06180\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Woosuk Kwon, UC Berkeley with Equal contribution;</p>\n<p>(2) Zhuohan Li, UC Berkeley with Equal contribution;</p>\n<p>(3) Siyuan Zhuang, UC Berkeley;</p>\n<p>(4) Ying Sheng, UC Berkeley and Stanford University;</p>\n<p>(5) Lianmin Zheng, UC Berkeley;</p>\n<p>(6) Cody Hao Yu, Independent Researcher;</p>\n<p>(7) Cody Hao Yu, Independent Researcher;</p>\n<p>(8) Joseph E. Gonzalez, UC Berkeley;</p>\n<p>(9) Hao Zhang, UC San Diego;</p>\n<p>(10) Ion Stoica, UC Berkeley.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Trump asks Supreme Court to pause imminent TikTok ban","url":"https://techcrunch.com/2024/12/28/trump-asks-supreme-court-to-pause-imminent-tiktok-ban/","date":1735405241,"author":"Anthony Ha","unread":true,"desc":"","content":"<p>Attorneys representing President-elect Donald Trump have asked the Supreme Court to pause a law that would force TikTok-owner ByteDance to sell the short-form video app or see it banned from the United States. If the app isn’t sold, the ban is set to take effect in just a few weeks, on January 19. ByteDance is [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Decoding With PagedAttention and vLLM","url":"https://hackernoon.com/decoding-with-pagedattention-and-vllm?source=rss","date":1735405216,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/UZfnXsE8xpuPm7xJGzSd\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Joq6DOP9CkD0S4F8MX1r\">2 Background and 2.1 Transformer-Based Large Language Models</a></p>\n<p><a href=\"http://hackernoon.com/preview/25g0m9nrxol991ZY9qci\">2.2 LLM Service & Autoregressive Generation</a></p>\n<p><a href=\"http://hackernoon.com/preview/IQ9Fd8hlh5MpHVMFA27O\">2.3 Batching Techniques for LLMs</a></p>\n<p><a href=\"https://hackernoon.com/preview/ytSMq2pxVtKIRC7iS3kK\">3 Memory Challenges in LLM Serving</a></p>\n<p><a href=\"http://hackernoon.com/preview/6BZmg60VAii9DNx0L1qc\">3.1 Memory Management in Existing Systems</a></p>\n<p><a href=\"http://hackernoon.com/preview/ZzGN1QRkg16N6zN7hCJi\">4 Method and 4.1 PagedAttention</a></p>\n<p><a href=\"http://hackernoon.com/preview/4Da8juHWnURn6g4urrl4\">4.2 KV Cache Manager</a></p>\n<p><a href=\"http://hackernoon.com/preview/GieTLdxmIGxSHDideqV5\">4.3 Decoding with PagedAttention and vLLM</a></p>\n<p><a href=\"http://hackernoon.com/preview/7g3isP8BzgyEsTasNUr2\">4.4 Application to Other Decoding Scenarios</a></p>\n<p><a href=\"http://hackernoon.com/preview/TEjiBAga2TDQ1ZZp5Vbg\">4.5 Scheduling and Preemption</a></p>\n<p><a href=\"http://hackernoon.com/preview/OjYLeVPi5jM2NSWKvipy\">4.6 Distributed Execution</a></p>\n<p><a href=\"http://hackernoon.com/preview/yqADDmfsCggYCcSdA6Nj\">5 Implementation</a></p>\n<p><a href=\"http://hackernoon.com/preview/S6sF02nTOzlXKZE1iQcP\">6 Evaluation and 6.1 Experimental Setup</a></p>\n<p><a href=\"http://hackernoon.com/preview/K3TeA7MNOi0g142ICu5W\">6.2 Basic Sampling</a></p>\n<p><a href=\"http://hackernoon.com/preview/cJxKQHExtKQrburnndpo\">6.3 Parallel Sampling and Beam Search</a></p>\n<p><a href=\"http://hackernoon.com/preview/JEOm7WvIfRfxEjepPqXW\">6.4 Shared prefix</a></p>\n<p><a href=\"http://hackernoon.com/preview/DWk5D3KHLfKhdQC02ayZ\">6.5 Chatbot</a></p>\n<p><a href=\"http://hackernoon.com/preview/l7VvLlkEDSBJ5GzMlwJf\">7 Ablation Studies</a></p>\n<p><a href=\"http://hackernoon.com/preview/PJb0S41IDQAbwYp0e6RZ\">8 Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/5d2sL9hRMUBNmVCZWMBC\">9 Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/HL77hmYOoM9MPB5fKgmq\">10 Conclusion, Acknowledgement and References</a></p>\n<h2 id=\"43decodingwithpagedattentionandvllm\">4.3 Decoding with PagedAttention and vLLM</h2>\n<p>Next, we walk through an example, as in Fig. 6, to demonstrate how vLLM executes PagedAttention and manages the memory during the decoding process of a single input sequence: 1 As in OS’s virtual memory, vLLM does not require reserving the memory for the maximum possible generated sequence length initially. Instead, it reserves only the necessary KV blocks to accommodate the KV cache generated during prompt computation. </p>\n<p>\\\nIn this case, The prompt has 7 tokens, so vLLM maps the first 2 logical KV blocks (0 and 1) to 2 physical KV blocks (7 and 1, respectively). In the prefill step, vLLM generates the KV cache of the prompts and the first output token with a conventional self-attention algorithm (e.g., [13]). vLLM then stores the KV cache of the first 4 tokens in logical block 0 and the following 3 tokens in logical block 1. The remaining slot is reserved for the subsequent autoregressive generation phase. 2 In the first autoregressive decoding step, vLLM generates the new token with the PagedAttention algorithm on physical blocks 7 and 1. </p>\n<p>\\\nSince one slot remains available in the last logical block, the newly generated KV cache is stored there, and the block table’s #filled record is updated. 3 At the second decoding step, as the last logical block is full, vLLM stores the newly generated KV cache in a new logical block; vLLM allocates a new physical block (physical block 3) for it and stores this mapping in the block table.</p>\n<p>\\\nGlobally, for each decoding iteration, vLLM first selects a set of candidate sequences for batching (more in §4.5), and allocates the physical blocks for the newly required logical blocks. Then, vLLM concatenates all the input tokens of the current iteration (i.e., all tokens for prompt phase</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-gd830l8.png\" alt=\"Figure 7. Storing the KV cache of two requests at the same time in vLLM.\" /></p>\n<p>\\\nrequests and the latest tokens for generation phase requests) as one sequence and feeds it into the LLM. During LLM’s computation, vLLM uses the PagedAttention kernel to access the previous KV cache stored in the form of logical KV blocks and saves the newly generated KV cache into the physical KV blocks. Storing multiple tokens within a KV block (block size &gt; 1) enables the PagedAttention kernel to process the KV cache across more positions in parallel, thus increasing the hardware utilization and reducing latency. However, a larger block size also increases memory fragmentation. We study the effect of block size in §7.2.</p>\n<p>\\\nAgain, vLLM dynamically assigns new physical blocks to logical blocks as more tokens and their KV cache are generated. As all the blocks are filled from left to right and a new physical block is only allocated when all previous blocks are full, vLLM limits all the memory wastes for a request within one block, so it can effectively utilize all the memory, as shown in Fig. 2. </p>\n<p>\\\nThis allows more requests to fit into memory for batching—hence improving the throughput. Once a request finishes its generation, its KV blocks can be freed to store the KV cache of other requests. In Fig. 7, we show an example of vLLM managing the memory for two sequences. The logical blocks of the two sequences are mapped to different physical blocks within the space reserved by the block engine in GPU workers. The neighboring logical blocks of both sequences do not need to be contiguous in physical GPU memory and the space of physical blocks can be effectively utilized by both sequences.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2309.06180\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Woosuk Kwon, UC Berkeley with Equal contribution;</p>\n<p>(2) Zhuohan Li, UC Berkeley with Equal contribution;</p>\n<p>(3) Siyuan Zhuang, UC Berkeley;</p>\n<p>(4) Ying Sheng, UC Berkeley and Stanford University;</p>\n<p>(5) Lianmin Zheng, UC Berkeley;</p>\n<p>(6) Cody Hao Yu, Independent Researcher;</p>\n<p>(7) Cody Hao Yu, Independent Researcher;</p>\n<p>(8) Joseph E. Gonzalez, UC Berkeley;</p>\n<p>(9) Hao Zhang, UC San Diego;</p>\n<p>(10) Ion Stoica, UC Berkeley.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Introducing LLaVA-Phi: A Compact Vision-Language Assistant Powered By a Small Language Model","url":"https://hackernoon.com/introducing-llava-phi-a-compact-vision-language-assistant-powered-by-a-small-language-model?source=rss","date":1735405211,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"https://hackernoon.com/preview/0xvVYI3OvJLvwRUYFnhV\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Jo6taCeMb5To49SvqmUy\">2. Related Work</a></p>\n<p><a href=\"https://hackernoon.com/preview/8re4SLMd5eLulCS8byQf\">3. LLaVA-Phi and 3.1. Training</a></p>\n<p><a href=\"https://hackernoon.com/preview/XmeEoU4AZVnfg5r6ymbU\">3.2. Qualitative Results</a></p>\n<p><a href=\"https://hackernoon.com/preview/bk1VFYQ8qOylIqzav05R\">4. Experiments</a></p>\n<p><a href=\"https://hackernoon.com/preview/WwxpDAUyfNGjRGKmxiF4\">5. Conclusion, Limitation, and Future Works and References</a></p>\n<h2 id=\"abstract\">Abstract</h2>\n<p>In this paper, we introduce LLaVA-ϕ (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a notable advancement in the realm of compact multi-modal models. It demonstrates that even smaller language models, with as few as 2.7B parameters, can effectively engage in intricate dialogues that integrate both textual and visual elements, provided they are trained with high-quality corpora. Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception. Beyond its remarkable performance in multi-modal dialogue tasks, our model opens new avenues for applications in timesensitive environments and systems that require real-time interaction, such as embodied agents. It highlights the potential of smaller language models to achieve sophisticated levels of understanding and interaction, while maintaining greater resource efficiency. The project is available at https://github.com/zhuyiche/llava-phi.</p>\n<h2 id=\"1introduction\">1. Introduction</h2>\n<p>Large vision language models, including Flamingo [1], GPT-4V [30], and Gemini [33], have exhibited remarkable proficiency in executing instructions, engaging in multi-turn dialogues, and handling image-based question-answering tasks. The progression of open-source vision language models has been significantly propelled by the rapid advancement of open-source Large Language Models like LLaMA [34] and Vicuna [5]. These developments primarily focus on leveraging language models with a minimum of 7B parameters, integrated with a vision encoder to enhance visual comprehension. However, this approach often results in increased test time and reduced inference speed, which are less than ideal for time-sensitive or real-time interactive applications, such as autonomous driving and robotics. This leads to an important inquiry: How effectively can small vision-language assistants perform in comparison?</p>\n<p>\\\nGemini [33] has blazed a trail for multi-modal models in mobile technology. Its streamlined variant, Gemini-Nano, boasts 1.8/3.25 billion parameters and is deployable on mobile devices. However, details like the model architecture, training data, and training methodologies remain proprietary and inaccessible to the public. In the realm of small language models, there have been notable advancements: TinyGSM [23], with 2.6 billion parameters, achieves over 80% accuracy on the GSM8k [7] benchmark. Additionally, models such as Phi [13] have demonstrated capabilities in language understanding, commonsense reasoning, and code generation, rivaling larger language models like LLaMA2-7B. This progress underscores the significant strides being made in the efficiency and effectiveness of smaller-scale language models.</p>\n<p>\\\nIn this paper, we introduce LLaVA-Phi, a compact vision-language assistant powered by a small language model. Our work combines the powerful opensourced multi-modal model, LLaVA-1.5 [24], with the best-performing open-sourced small language models, Phi2 [21]. We follow a two-stage training pipeline and leverage high-quality visual instruction tuning data from LLaVA. LLaVA-Phi was evaluated across eight diverse benchmarks. Despite possessing only 3 billion parameters, it achieves performance comparable to, or even surpassing, some larger multi-modal models that are three times larger. </p>\n<p>\\\nNotably, LLaVA-Phi-3B demonstrates exceptional proficiency in ScienceQA [28], outperforming existing large multimodal models. Additionally, we qualitatively demonstrate LLaVA-Phi’s strong generalization ability in handling challenging questions, generating code based on instructions, and solving mathematical problems.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2401.02330\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n(1) Yichen Zhu, Midea Group;</p>\n<p>(2) Minjie Zhu, Midea Group and East China Normal University;</p>\n<p>(3) Ning Liu, Midea Group;</p>\n<p>(4) Zhicai Ou, Midea Group;</p>\n<p>(5) Xiaofeng Mou, Midea Group.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"KV Cache Manager: The Key Idea Behind It and How It Works","url":"https://hackernoon.com/kv-cache-manager-the-key-idea-behind-it-and-how-it-works?source=rss","date":1735404311,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/UZfnXsE8xpuPm7xJGzSd\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Joq6DOP9CkD0S4F8MX1r\">2 Background and 2.1 Transformer-Based Large Language Models</a></p>\n<p><a href=\"http://hackernoon.com/preview/25g0m9nrxol991ZY9qci\">2.2 LLM Service & Autoregressive Generation</a></p>\n<p><a href=\"http://hackernoon.com/preview/IQ9Fd8hlh5MpHVMFA27O\">2.3 Batching Techniques for LLMs</a></p>\n<p><a href=\"https://hackernoon.com/preview/ytSMq2pxVtKIRC7iS3kK\">3 Memory Challenges in LLM Serving</a></p>\n<p><a href=\"http://hackernoon.com/preview/6BZmg60VAii9DNx0L1qc\">3.1 Memory Management in Existing Systems</a></p>\n<p><a href=\"http://hackernoon.com/preview/ZzGN1QRkg16N6zN7hCJi\">4 Method and 4.1 PagedAttention</a></p>\n<p><a href=\"http://hackernoon.com/preview/4Da8juHWnURn6g4urrl4\">4.2 KV Cache Manager</a></p>\n<p><a href=\"http://hackernoon.com/preview/GieTLdxmIGxSHDideqV5\">4.3 Decoding with PagedAttention and vLLM</a></p>\n<p><a href=\"http://hackernoon.com/preview/7g3isP8BzgyEsTasNUr2\">4.4 Application to Other Decoding Scenarios</a></p>\n<p><a href=\"http://hackernoon.com/preview/TEjiBAga2TDQ1ZZp5Vbg\">4.5 Scheduling and Preemption</a></p>\n<p><a href=\"http://hackernoon.com/preview/OjYLeVPi5jM2NSWKvipy\">4.6 Distributed Execution</a></p>\n<p><a href=\"http://hackernoon.com/preview/yqADDmfsCggYCcSdA6Nj\">5 Implementation</a></p>\n<p><a href=\"http://hackernoon.com/preview/S6sF02nTOzlXKZE1iQcP\">6 Evaluation and 6.1 Experimental Setup</a></p>\n<p><a href=\"http://hackernoon.com/preview/K3TeA7MNOi0g142ICu5W\">6.2 Basic Sampling</a></p>\n<p><a href=\"http://hackernoon.com/preview/cJxKQHExtKQrburnndpo\">6.3 Parallel Sampling and Beam Search</a></p>\n<p><a href=\"http://hackernoon.com/preview/JEOm7WvIfRfxEjepPqXW\">6.4 Shared prefix</a></p>\n<p><a href=\"http://hackernoon.com/preview/DWk5D3KHLfKhdQC02ayZ\">6.5 Chatbot</a></p>\n<p><a href=\"http://hackernoon.com/preview/l7VvLlkEDSBJ5GzMlwJf\">7 Ablation Studies</a></p>\n<p><a href=\"http://hackernoon.com/preview/PJb0S41IDQAbwYp0e6RZ\">8 Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/5d2sL9hRMUBNmVCZWMBC\">9 Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/HL77hmYOoM9MPB5fKgmq\">10 Conclusion, Acknowledgement and References</a></p>\n<h2 id=\"42kvcachemanager\">4.2 KV Cache Manager</h2>\n<p>The key idea behind vLLM’s memory manager is analogous to the virtual memory [25] in operating systems. OS partitions memory into fixed-sized pages and maps user programs’ logical pages to physical pages. Contiguous logical pages can correspond to non-contiguous physical memory pages, allowing user programs to access memory as though it were contiguous. Moreover, physical memory space needs not to be fully reserved in advance, enabling the OS to dynamically allocate physical pages as needed. vLLM uses the ideas behind virtual memory to manage the KV cache in an LLM service. Enabled by PagedAttention, we organize the KV cache as fixed-size KV blocks, like pages in virtual memory.</p>\n<p>\\\nA request’s KV cache is represented as a series of logical KV blocks, filled from left to right as new tokens and their KV cache are generated. The last KV block’s unfilled positions are reserved for future generations. On GPU workers, a block engine allocates a contiguous chunk of GPU DRAM and</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-95830ir.png\" alt=\"Figure 6. Block table translation in vLLM.\" /></p>\n<p>\\\ndivides it into physical KV blocks (this is also done on CPU RAM for swapping; see §4.5). The KV block manager also maintains block tables—the mapping between logical and physical KV blocks of each request. Each block table entry records the corresponding physical blocks of a logical block and the number of filled positions. Separating logical and physical KV blocks allows vLLM to dynamically grow the KV cache memory without reserving it for all positions in advance, which eliminates most memory waste in existing systems, as in Fig. 2.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2309.06180\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Woosuk Kwon, UC Berkeley with Equal contribution;</p>\n<p>(2) Zhuohan Li, UC Berkeley with Equal contribution;</p>\n<p>(3) Siyuan Zhuang, UC Berkeley;</p>\n<p>(4) Ying Sheng, UC Berkeley and Stanford University;</p>\n<p>(5) Lianmin Zheng, UC Berkeley;</p>\n<p>(6) Cody Hao Yu, Independent Researcher;</p>\n<p>(7) Cody Hao Yu, Independent Researcher;</p>\n<p>(8) Joseph E. Gonzalez, UC Berkeley;</p>\n<p>(9) Hao Zhang, UC San Diego;</p>\n<p>(10) Ion Stoica, UC Berkeley.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"NVIDIA Made Great Strides With Their Open-Source Kernel Code & Wayland Support In 2024","url":"https://www.phoronix.com/news/NVIDIA-2024-Linux-Highlights","date":1735403820,"author":"Michael Larabel","unread":true,"desc":"","content":"This year NVIDIA's official Linux graphics driver enjoyed much more robust Wayland support, their open-source kernel modules have matured greatly and are now being used by default, and their proprietary Vulkan and OpenGL drivers remain in good standing for performant Linux gaming and workstation graphics. NVIDIA's Linux driver stack had a rather great year...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Our Method for Developing PagedAttention","url":"https://hackernoon.com/our-method-for-developing-pagedattention?source=rss","date":1735403410,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/UZfnXsE8xpuPm7xJGzSd\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Joq6DOP9CkD0S4F8MX1r\">2 Background and 2.1 Transformer-Based Large Language Models</a></p>\n<p><a href=\"http://hackernoon.com/preview/25g0m9nrxol991ZY9qci\">2.2 LLM Service & Autoregressive Generation</a></p>\n<p><a href=\"http://hackernoon.com/preview/IQ9Fd8hlh5MpHVMFA27O\">2.3 Batching Techniques for LLMs</a></p>\n<p><a href=\"https://hackernoon.com/preview/ytSMq2pxVtKIRC7iS3kK\">3 Memory Challenges in LLM Serving</a></p>\n<p><a href=\"http://hackernoon.com/preview/6BZmg60VAii9DNx0L1qc\">3.1 Memory Management in Existing Systems</a></p>\n<p><a href=\"http://hackernoon.com/preview/ZzGN1QRkg16N6zN7hCJi\">4 Method and 4.1 PagedAttention</a></p>\n<p><a href=\"http://hackernoon.com/preview/4Da8juHWnURn6g4urrl4\">4.2 KV Cache Manager</a></p>\n<p><a href=\"http://hackernoon.com/preview/GieTLdxmIGxSHDideqV5\">4.3 Decoding with PagedAttention and vLLM</a></p>\n<p><a href=\"http://hackernoon.com/preview/7g3isP8BzgyEsTasNUr2\">4.4 Application to Other Decoding Scenarios</a></p>\n<p><a href=\"http://hackernoon.com/preview/TEjiBAga2TDQ1ZZp5Vbg\">4.5 Scheduling and Preemption</a></p>\n<p><a href=\"http://hackernoon.com/preview/OjYLeVPi5jM2NSWKvipy\">4.6 Distributed Execution</a></p>\n<p><a href=\"http://hackernoon.com/preview/yqADDmfsCggYCcSdA6Nj\">5 Implementation</a></p>\n<p><a href=\"http://hackernoon.com/preview/S6sF02nTOzlXKZE1iQcP\">6 Evaluation and 6.1 Experimental Setup</a></p>\n<p><a href=\"http://hackernoon.com/preview/K3TeA7MNOi0g142ICu5W\">6.2 Basic Sampling</a></p>\n<p><a href=\"http://hackernoon.com/preview/cJxKQHExtKQrburnndpo\">6.3 Parallel Sampling and Beam Search</a></p>\n<p><a href=\"http://hackernoon.com/preview/JEOm7WvIfRfxEjepPqXW\">6.4 Shared prefix</a></p>\n<p><a href=\"http://hackernoon.com/preview/DWk5D3KHLfKhdQC02ayZ\">6.5 Chatbot</a></p>\n<p><a href=\"http://hackernoon.com/preview/l7VvLlkEDSBJ5GzMlwJf\">7 Ablation Studies</a></p>\n<p><a href=\"http://hackernoon.com/preview/PJb0S41IDQAbwYp0e6RZ\">8 Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/5d2sL9hRMUBNmVCZWMBC\">9 Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/HL77hmYOoM9MPB5fKgmq\">10 Conclusion, Acknowledgement and References</a></p>\n<h2 id=\"4method\">4 Method</h2>\n<p>In this work, we develop a new attention algorithm, PagedAttention, and build an LLM serving engine, vLLM, to tackle the challenges outlined in §3. The architecture of vLLM is shown in Fig. 4. vLLM adopts a centralized scheduler to coordinate the execution of distributed GPU workers. The KV cache manager effectively manages the KV cache in a paged fashion, enabled by PagedAttention. Specifically, the KV cache manager manages the physical KV cache memory on the GPU workers through the instructions sent by the centralized scheduler.</p>\n<p>\\\nNext, We describe the PagedAttention algorithm in §4.1. With that, we show the design of the KV cache manager in §4.2 and how it facilitates PagedAttention in §4.3, respectively. Then, we show how this design facilitates effective memory management for various decoding methods (§4.4) and handles the variable length input and output sequences (§4.5). Finally, we show how the system design of vLLM works in a distributed setting (§4.6).</p>\n<h3 id=\"41pagedattention\">4.1 PagedAttention</h3>\n<p>To address the memory challenges in §3, we introduce PagedAttention, an attention algorithm inspired by the classic idea of paging [25] in operating systems. Unlike the traditional attention algorithms, PagedAttention allows storing continuous keys and values in non-contiguous memory space. Specifically, PagedAttention partitions the KV cache of each sequence into KV blocks. Each block contains the key and value vectors for a fixed number of tokens,[1] which we denote as KV</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-2m830v6.png\" alt=\"Figure 5. Illustration of the PagedAttention algorithm, where the attention key and values vectors are stored as non-contiguous blocks in the memory.\" /></p>\n<p>\\\nblock size (𝐵). Denote the key block 𝐾𝑗 = (𝑘(𝑗−1)𝐵+1, . . . , 𝑘𝑗𝐵) and value block 𝑉𝑗 = (𝑣(𝑗−1)𝐵+1, . . . , 𝑣𝑗𝐵). The attention computation in Eq. 4 can be transformed into the following block-wise computation:</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-up930x7.png\" alt=\"\" /></p>\n<p>\\\nwhere 𝐴𝑖𝑗 = (𝑎𝑖,(𝑗−1)𝐵+1, . . . , 𝑎𝑖,𝑗𝐵) is the row vector of attention score on 𝑗-th KV block.</p>\n<p>\\\nDuring the attention computation, the PagedAttention kernel identifies and fetches different KV blocks separately. We show an example of PagedAttention in Fig. 5: The key and value vectors are spread across three blocks, and the three blocks are not contiguous on the physical memory. At each time, the kernel multiplies the query vector 𝑞𝑖 of the query token (“forth”) and the key vectors 𝐾𝑗 in a block (e.g., key vectors of “Four score and seven” for block 0) to compute the attention score𝐴𝑖𝑗, and later multiplies𝐴𝑖𝑗 with the value vectors 𝑉𝑗 in a block to derive the final attention output 𝑜𝑖.</p>\n<p>\\\nIn summary, the PagedAttention algorithm allows the KV blocks to be stored in non-contiguous physical memory, which enables more flexible paged memory management in vLLM.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2309.06180\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[1] In Transformer, each token has a set of key and value vectors across layers and attention heads within a layer. All the key and value vectors can be managed together within a single KV block, or the key and value vectors at different heads and layers can each have a separate block and be managed in separate block tables. The two designs have no performance difference and we choose the second one for easy implementation.</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Woosuk Kwon, UC Berkeley with Equal contribution;</p>\n<p>(2) Zhuohan Li, UC Berkeley with Equal contribution;</p>\n<p>(3) Siyuan Zhuang, UC Berkeley;</p>\n<p>(4) Ying Sheng, UC Berkeley and Stanford University;</p>\n<p>(5) Lianmin Zheng, UC Berkeley;</p>\n<p>(6) Cody Hao Yu, Independent Researcher;</p>\n<p>(7) Cody Hao Yu, Independent Researcher;</p>\n<p>(8) Joseph E. Gonzalez, UC Berkeley;</p>\n<p>(9) Hao Zhang, UC San Diego;</p>\n<p>(10) Ion Stoica, UC Berkeley.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"PagedAttention: Memory Management in Existing Systems","url":"https://hackernoon.com/pagedattention-memory-management-in-existing-systems?source=rss","date":1735402515,"author":"Writings, Papers and Blogs on Text Models","unread":true,"desc":"","content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/UZfnXsE8xpuPm7xJGzSd\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Joq6DOP9CkD0S4F8MX1r\">2 Background and 2.1 Transformer-Based Large Language Models</a></p>\n<p><a href=\"http://hackernoon.com/preview/25g0m9nrxol991ZY9qci\">2.2 LLM Service & Autoregressive Generation</a></p>\n<p><a href=\"http://hackernoon.com/preview/IQ9Fd8hlh5MpHVMFA27O\">2.3 Batching Techniques for LLMs</a></p>\n<p><a href=\"https://hackernoon.com/preview/ytSMq2pxVtKIRC7iS3kK\">3 Memory Challenges in LLM Serving</a></p>\n<p><a href=\"http://hackernoon.com/preview/6BZmg60VAii9DNx0L1qc\">3.1 Memory Management in Existing Systems</a></p>\n<p><a href=\"http://hackernoon.com/preview/ZzGN1QRkg16N6zN7hCJi\">4 Method and 4.1 PagedAttention</a></p>\n<p><a href=\"http://hackernoon.com/preview/4Da8juHWnURn6g4urrl4\">4.2 KV Cache Manager</a></p>\n<p><a href=\"http://hackernoon.com/preview/GieTLdxmIGxSHDideqV5\">4.3 Decoding with PagedAttention and vLLM</a></p>\n<p><a href=\"http://hackernoon.com/preview/7g3isP8BzgyEsTasNUr2\">4.4 Application to Other Decoding Scenarios</a></p>\n<p><a href=\"http://hackernoon.com/preview/TEjiBAga2TDQ1ZZp5Vbg\">4.5 Scheduling and Preemption</a></p>\n<p><a href=\"http://hackernoon.com/preview/OjYLeVPi5jM2NSWKvipy\">4.6 Distributed Execution</a></p>\n<p><a href=\"http://hackernoon.com/preview/yqADDmfsCggYCcSdA6Nj\">5 Implementation</a></p>\n<p><a href=\"http://hackernoon.com/preview/S6sF02nTOzlXKZE1iQcP\">6 Evaluation and 6.1 Experimental Setup</a></p>\n<p><a href=\"http://hackernoon.com/preview/K3TeA7MNOi0g142ICu5W\">6.2 Basic Sampling</a></p>\n<p><a href=\"http://hackernoon.com/preview/cJxKQHExtKQrburnndpo\">6.3 Parallel Sampling and Beam Search</a></p>\n<p><a href=\"http://hackernoon.com/preview/JEOm7WvIfRfxEjepPqXW\">6.4 Shared prefix</a></p>\n<p><a href=\"http://hackernoon.com/preview/DWk5D3KHLfKhdQC02ayZ\">6.5 Chatbot</a></p>\n<p><a href=\"http://hackernoon.com/preview/l7VvLlkEDSBJ5GzMlwJf\">7 Ablation Studies</a></p>\n<p><a href=\"http://hackernoon.com/preview/PJb0S41IDQAbwYp0e6RZ\">8 Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/5d2sL9hRMUBNmVCZWMBC\">9 Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/HL77hmYOoM9MPB5fKgmq\">10 Conclusion, Acknowledgement and References</a></p>\n<h2 id=\"31memorymanagementinexistingsystems\">3.1 Memory Management in Existing Systems</h2>\n<p>Since most operators in current deep learning frameworks [33, 39] require tensors to be stored in contiguous memory, previous LLM serving systems [31, 60] also store the KV cache of one request as a contiguous tensor across the different positions. Due to the unpredictable output lengths from the LLM, they statically allocate a chunk of memory for a request based on the request’s maximum possible sequence length, irrespective of the actual input or eventual output length of the request.</p>\n<p>\\\nFig. 3 illustrates two requests: request A with 2048 maximum possible sequence length and request B with a maximum of 512. The chunk pre-allocation scheme in existing systems has three primary sources of memory wastes: reserved slots for future tokens, internal fragmentation due to over-provisioning for potential maximum sequence lengths, and external fragmentation from the memory allocator like the buddy allocator. </p>\n<p>\\\nThe external fragmentation will never be used for generated tokens, which is known before serving a request. Internal fragmentation also remains unused, but this is only realized after a request has finished sampling. They are both pure memory waste. </p>\n<p>\\\nAlthough the reserved memory is eventually used, reserving this space for the entire request’s duration, especially when the reserved space is large, occupies the space that could otherwise be used to process other requests. We visualize the average percentage of memory wastes in our experiments in Fig. 2, revealing that the actual effective memory in previous systems can be as low as 20.4%.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-2z830g5.png\" alt=\"Figure 4. vLLM system overview.\" /></p>\n<p>\\\nAlthough compaction [54] has been proposed as a potential solution to fragmentation, performing compaction in a performance-sensitive LLM serving system is impractical due to the massive KV cache. Even with compaction, the pre-allocated chunk space for each request prevents memory sharing specific to decoding algorithms in existing memory management systems.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2309.06180\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Woosuk Kwon, UC Berkeley with Equal contribution;</p>\n<p>(2) Zhuohan Li, UC Berkeley with Equal contribution;</p>\n<p>(3) Siyuan Zhuang, UC Berkeley;</p>\n<p>(4) Ying Sheng, UC Berkeley and Stanford University;</p>\n<p>(5) Lianmin Zheng, UC Berkeley;</p>\n<p>(6) Cody Hao Yu, Independent Researcher;</p>\n<p>(7) Cody Hao Yu, Independent Researcher;</p>\n<p>(8) Joseph E. Gonzalez, UC Berkeley;</p>\n<p>(9) Hao Zhang, UC San Diego;</p>\n<p>(10) Ion Stoica, UC Berkeley.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The HackerNoon Newsletter: Will AI Widen Global Inequality? (12/28/2024)","url":"https://hackernoon.com/12-28-2024-newsletter?source=rss","date":1735401854,"author":"Noonification","unread":true,"desc":"","content":"\n              \n        <p><strong>How are you, hacker?</strong></p>\n        <br />\n        <p>🪐 What’s happening in tech today, December 28, 2024?</p>\n        <br />\n        <p>\n          The\n          <a href=\"https://hackernoon.com/noonification\" target=\"_blank\" rel=\"noopener\"> HackerNoon Newsletter</a>\n          brings the HackerNoon \n          <a href=\"https://hackernoon.com\" target=\"_blank\" rel=\"noopener\">homepage</a>\n          straight to your inbox.\n          <a href=\"https://hackernoon.com/on-this-day\" target=\"_blank\" rel=\"noopener\">On this day,</a>\n          \n            <strong>The US Signed the Endangered Species Act (ESA)</strong> in 1973,  <strong>The First Commercial Movie Was Screened in Paris</strong> in 1895,  <strong>The First American In-Vitro Fertilization Baby is Born</strong> in 1981, \n          \n          and  we present you with these top quality stories. \n          \n            From \n        <a href=\"https://hackernoon.com/week-2-it-was-within-me-all-along\" class=\"eventTitle\"><strong>Week 2: It Was Within Me All Along</strong></a>\n       to \n        <a href=\"https://hackernoon.com/stand-out-from-other-developers-by-contributing-to-open-source\" class=\"eventTitle\"><strong>Stand Out From Other Developers By Contributing to Open Source</strong></a>,\n       let’s dive right in.\n          \n        </p>\n      \n              \n          <h2><a href=\"https://hackernoon.com/stand-out-from-other-developers-by-contributing-to-open-source\">Stand Out From Other Developers By Contributing to Open Source</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/NZ9j3rbETVbGsErqXpofkOsigAh1-wj02ojv.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/empiree361\">@empiree361</a> [ 7 Min read ] If you truly love programming and want to grow as a developer, strive to create something of your own — whether it’s a small library or a service. <a href=\"https://hackernoon.com/stand-out-from-other-developers-by-contributing-to-open-source\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/how-to-increase-your-intelligence-even-if-youre-not-genetically-gifted\">How to Increase Your Intelligence (even if Youre Not Genetically Gifted)</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/x21VprIQHYaYrJEbiyMkN7uuOTH2-gq0360c.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/praisejames\">@praisejames</a> [ 4 Min read ] The true test of intelligence is getting what you want out of life. <a href=\"https://hackernoon.com/how-to-increase-your-intelligence-even-if-youre-not-genetically-gifted\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/week-2-it-was-within-me-all-along\">Week 2: It Was Within Me All Along</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/myasVcWKrUhFH4jfr0A8YHyvVVq2-9v0310f.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/ivacvetkovska2000_vp8qaqs8\">@ivacvetkovska2000_vp8qaqs8</a> [ 4 Min read ] Embracing honesty while adapting to life abroad: lessons in change, connection, and personal growth. <a href=\"https://hackernoon.com/week-2-it-was-within-me-all-along\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/will-ai-widen-global-inequality\">Will AI Widen Global Inequality?</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/bf5MdUZkm2XA5ajgVIztMBkqLBz2-1a033bn.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/zacamos\">@zacamos</a> [ 5 Min read ] AI gives ordinary people lots of computing power — but only if they have the ability and infrastructure to access it. Heres how AI may exacerbate inequalities. <a href=\"https://hackernoon.com/will-ai-widen-global-inequality\">Read More.</a></p>\n        \n              \n        <br />\n        <p>🧑‍💻 What happened in your world this week?</p>\n        <p>\n          It's been said that\n          <a href=\"https://hackernoon.com/developers-the-why-and-how-to-writing-technical-articles-54e824789ef6\">writing can help consolidate technical knowledge</a>,\n          <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\">establish credibility</a>,\n          <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\"> and contribute to emerging community standards</a>.\n          Feeling stuck? We got you covered ⬇️⬇️⬇️\n        </p>\n        <br />\n        <p>\n          <a href=\"https://app.hackernoon.com/mobile/lZx3fmlPdlPJpVBIdble\">ANSWER THESE GREATEST INTERVIEW QUESTIONS OF ALL TIME</a>\n        </p>\n        <br />\n        <p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>\n        <br />\n        <p><img src=\"https://cdn.hackernoon.com/images/the-hackernoon-newsletter-footer.png\" alt /></p>\n      \n            ","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Best iPad apps for unleashing and exploring your creativity","url":"https://techcrunch.com/2024/12/28/best-ipad-apps-for-unleashing-and-exploring-your-creativity/","date":1735401600,"author":"Aisha Malik","unread":true,"desc":"","content":"<p>There are a number of iPad apps that can help you explore and express your creativity. Although the iPad started off as a simple device that could be used to stream content or browse the web on the go, Apple has essentially turned its iPads into powerful machines that can be used to do things [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"How to watch CES 2025’s press conferences","url":"https://techcrunch.com/2024/12/28/how-to-watch-ces-2025s-press-conferences/","date":1735401600,"author":"Brian Heater","unread":true,"desc":"","content":"<p>CES kicks off January 7. The annual Las Vegas event sets the tone of the year’s consumer electronics and automotive industries. As always, TechCrunch will be there, sniffing stories from the most exciting startups and tech giants. If you really want a piece of the action without paying for the hotel and flight, many of [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"This Cryptographer Helps Quantum-Proof the Internet","url":"https://spectrum.ieee.org/post-quantum-cryptography-2670649921","date":1735398004,"author":"Edd Gent","unread":true,"desc":"","content":"<p>Joppe Bos designs encryption that even quantum computers can’t crack</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4ODQwMC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4MzM4OTUzMn0.i53hTuMqld3Yi-mX92DQqiF84aNCMT7T575-JYPYJBo/image.jpg?width=600","enclosureMime":""},{"title":"Revisiting the biggest moments in the space industry in 2024","url":"https://techcrunch.com/2024/12/28/revisiting-the-biggest-moments-in-the-space-industry-in-2024/","date":1735398000,"author":"Aria Alamalhodaei","unread":true,"desc":"","content":"<p>We are at the dawn of a new space age. If you doubt, simply look back at the last year: From SpaceX’s historic catch of the Super Heavy booster to the record-breaking number of lunar landing attempts, this year was full of historic and ambitious missions and demonstrations.  We’re taking a look back at the [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The Top 10 Telecommunications Stories of 2024","url":"https://spectrum.ieee.org/telecom-news-2024","date":1735394404,"author":"Margo Anderson","unread":true,"desc":"","content":"<p>Tales of a broadband space race, a 6G signal trace, and a Wi-Fi data chase</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM2ODUxNy9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1NDg3NTAxOX0.Q-RK-SFsf_CESV6o7W1dMKlLyyXc0y0tdMFhaOKANxQ/image.jpg?width=600","enclosureMime":""},{"title":"A \"Safe C++\" Being Explored Using The New ClangIR","url":"https://www.phoronix.com/news/RFC-Safe-CXX-Using-ClangIR","date":1735394400,"author":"Michael Larabel","unread":true,"desc":"","content":"An interesting \"request for comments\" proposal I have been meaning to write about since last month is in-development work developing \"Safe C++\" as an extension to the LLVM Clang compiler and making use of the new, in-development ClangIR...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"FFmpeg Lands Improved Support For Flash Video \"FLV\" With Multi-Track Audio/Video","url":"https://www.phoronix.com/news/FFmpeg-FLV-Improvements","date":1735393523,"author":"Michael Larabel","unread":true,"desc":"","content":"It wasn't on my bingo card for end of year 2024 but the widely-used FFmpeg multimedia library has seen a new round of improvements to the Flash Video (FLV) support...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Fedora's Captivating 2024 With Many New Features & Leading Innovations","url":"https://www.phoronix.com/news/Fedora-Linux-2024-Recap","date":1735386087,"author":"Michael Larabel","unread":true,"desc":"","content":"The Fedora Linux distribution had another great year with the successful releases of Fedora 40 and Fedora 41 that were both rather polished and largely on-time -- something that couldn't be said frequently of Fedora releases long ago. Fedora Linux has continued pushing leading edge innovations into their distribution thanks to the sponsorship and upstream contributions of Red Hat engineers. 2024 was a rather successful year for this high grade Linux distribution...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"FSF Encouraging Pressure Campaign On Microsoft For 2025","url":"https://www.phoronix.com/news/FSF-Pressuring-Microsoft-2025","date":1735385630,"author":"Michael Larabel","unread":true,"desc":"","content":"Looking for a 2025 New Year's resolution? The Free Software Foundation (FSF) is encouraging a pressure campaign on Microsoft to continue...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"GIMP 3.0 RC2 Released With Bug Fixes Plus A Few Last Minute Features","url":"https://www.phoronix.com/news/GIMP-3.0-RC2-Released","date":1735344985,"author":"Michael Larabel","unread":true,"desc":"","content":"The long-awaited GIMP 3.0 image editing program that is a free software alternative to Adobe Photoshop will not see its stable release in 2024... But just before the New Year, the GIMP 3.0 Release Candidate 2 is now available for testing...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Bench shuts down, leaving thousands of businesses without access to accounting and tax docs","url":"https://techcrunch.com/2024/12/27/bench-shuts-down-leaving-thousands-of-businesses-without-access-to-accounting-and-tax-docs/","date":1735336320,"author":"Charles Rollet","unread":true,"desc":"","content":"<p>Bench, a Canada-based accounting startup that offered software-as-a-service for small and medium-sized businesses, has abruptly shut down, according to a notice posted on its website.&#160;&#160; “We regret to inform you that as of December 27, 2024, the Bench platform will no longer be accessible,” the notice reads. “We know this news is abrupt and may [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Nonprofit group joins Elon Musk’s effort to block OpenAI’s for-profit transition","url":"https://techcrunch.com/2024/12/27/nonprofit-group-joins-elon-musks-effort-to-block-openais-for-profit-transition/","date":1735335586,"author":"Kyle Wiggers","unread":true,"desc":"","content":"<p>Encode, the nonprofit organization that co-sponsored California&#8217;s ill-fated SB 1047 AI safety legislation, has requested permission to file an amicus brief in support of Elon Musk&#8217;s injunction to halt OpenAI&#8217;s transition to a for-profit company. In a proposed brief submitted to the U.S. District Court for the Northern District of California Friday afternoon, counsel for [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"GNOME Image Viewer Adds Image Editing Support","url":"https://www.phoronix.com/news/GNOME-Image-Viewer-Editing","date":1735332862,"author":"Michael Larabel","unread":true,"desc":"","content":"The GNOME Image Viewer has merged initial support for basic image editing capabilities into the application...","flags":null,"enclosureUrl":"","enclosureMime":""}]}