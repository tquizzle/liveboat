{"id":"USgKoryE83j5SszZjyr68sh7DjLn4j6MWUckh4V9BHd8VFvyhTMUW31KA1puTBCFGhS4TzMjShH3jpXxMAYW7X","title":"top scoring links : unRAID","displayTitle":"Reddit - UnRAID","url":"https://www.reddit.com/r/unRAID/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/unRAID/top/?sort=top&t=day&limit=6","items":[{"title":"Pulling my hair out with Immich Docker Compose setting in Unraid","url":"https://www.reddit.com/r/unRAID/comments/1hn1r5j/pulling_my_hair_out_with_immich_docker_compose/","date":1735257577,"author":"/u/Jj_cale","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p>Hey folks,</p> <p>I have spent at least 10 hours on this since yday and I just could not figure this one out. Could one of you god-tier folks give me a hand up here:</p> <p>What I am trying to do, is to put all the actual photo upload here at share Immich_photo, which sits on my array and I would like to the rest elements, thumbs, profiles, ML models and temp uploads to go to a share called immich_cache, which is a share that sits only on my cache.</p> <p>I have tried to follow the guide here: <a href=\"https://github.com/immich-app/immich/discussions/2328#discussioncomment-5712579\">https://github.com/immich-app/immich/discussions/2328#discussioncomment-5712579</a> and i could not just get it to work </p> <p>Here is my .env file: </p> <pre><code># The location where your uploaded files are stored UPLOAD_LOCATION=/mnt/user/immich_cache/upload/ # The location where your database files are stored DB_DATA_LOCATION=/mnt/user/appdata/postgres/ LIBRARY_LOCATION=/mnt/user/immich_photos/ THUMBS_LOCATION=/mnt/user/immich_cache/thumbs/ PROFILE_LOCATION=/mnt/user/immich_cache/profile/ VIDEO_LOCATION=/mnt/user/immich_cache/encoded-video/ ML_LOCATION=/mnt/user/immich_cache/ml/ # To set a timezone, uncomment the next line and change Etc/UTC to a TZ identifier from this list: # TZ=Etc/UTC # The Immich version to use. You can pin this to a specific version like &quot;v1.71.0&quot; IMMICH_VERSION=release # Connection secret for postgres. You should change it to a random password # Please use only the characters `A-Za-z0-9`, without special characters or spaces DB_PASSWORD=postgres # The values below this line do not need to be changed ################################################################################### DB_USERNAME=postgres DB_DATABASE_NAME=immichhttps://en.wikipedia.org/wiki/List_of_tz_database_time_zones#List </code></pre> <p>and here is my docker-compose:</p> <pre><code>name: immich services: immich-server: container_name: immich_server image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release} # extends: # file: hwaccel.transcoding.yml # service: cpu # set to one of [nvenc, quicksync, rkmpp, vaapi, vaapi-wsl] for accelerated transcoding volumes: # Do not edit the next line. If you want to change the media storage location on your system, edit the value of UPLOAD_LOCATION in the .env file - ${UPLOAD_LOCATION}:/usr/src/app/upload - ${LIBRARY_LOCATION}:/usr/src/app/upload/library - ${THUMBS_LOCATION}:/usr/src/app/upload/thumbs - ${PROFILE_LOCATION}:/usr/src/app/upload/profile - ${VIDEO_LOCATION}:/usr/src/app/upload/encoded-video - ${ML_LOCATION}:/usr/src/app/upload/ml - /etc/localtime:/etc/localtime:ro env_file: - .env ports: - &#39;2283:2283&#39; depends_on: - redis - database restart: always healthcheck: disable: false immich-machine-learning: container_name: immich_machine_learning # For hardware acceleration, add one of -[armnn, cuda, openvino] to the image tag. # Example tag: ${IMMICH_VERSION:-release}-cuda image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION:-release} # extends: # uncomment this section for hardware acceleration - see https://immich.app/docs/features/ml-hardware-acceleration # file: hwaccel.ml.yml # service: cpu # set to one of [armnn, cuda, openvino, openvino-wsl] for accelerated inference - use the `-wsl` version for WSL2 where applicable volumes: - ${UPLOAD_LOCATION}:/usr/src/app/upload - ${LIBRARY_LOCATION}:/usr/src/app/upload/library - ${THUMBS_LOCATION}:/usr/src/app/upload/thumbs - ${PROFILE_LOCATION}:/usr/src/app/upload/profile - ${VIDEO_LOCATION}:/usr/src/app/upload/encoded-video - ${ML_LOCATION}:/usr/src/app/upload/ml - /etc/localtime:/etc/localtime:ro env_file: - .env restart: always healthcheck: disable: false redis: container_name: immich_redis image: docker.io/redis:6.2-alpine@sha256:eaba718fecd1196d88533de7ba49bf903ad33664a92debb24660a922ecd9cac8 healthcheck: test: redis-cli ping || exit 1 restart: always database: container_name: immich_postgres image: docker.io/tensorchord/pgvecto-rs:pg14-v0.2.0@sha256:90724186f0a3517cf6914295b5ab410db9ce23190a2d9d0b9dd6463e3fa298f0 environment: POSTGRES_PASSWORD: ${DB_PASSWORD} POSTGRES_USER: ${DB_USERNAME} POSTGRES_DB: ${DB_DATABASE_NAME} POSTGRES_INITDB_ARGS: &#39;--data-checksums&#39; volumes: # Do not edit the next line. If you want to change the database storage location on your system, edit the value of DB_DATA_LOCATION in the .env file - ${DB_DATA_LOCATION}:/var/lib/postgresql/data healthcheck: test: &gt;- pg_isready --dbname=&quot;$${POSTGRES_DB}&quot; --username=&quot;$${POSTGRES_USER}&quot; || exit 1; Chksum=&quot;$$(psql --dbname=&quot;$${POSTGRES_DB}&quot; --username=&quot;$${POSTGRES_USER}&quot; --tuples-only --no-align --command=&#39;SELECT COALESCE(SUM(checksum_failures), 0) FROM pg_stat_database&#39;)&quot;; echo &quot;checksum failure count is $$Chksum&quot;; [ &quot;$$Chksum&quot; = &#39;0&#39; ] || exit 1 interval: 5m # start_interval: 15s # start_period: 5m command: &gt;- postgres -c shared_preload_libraries=vectors.so -c &#39;search_path=&quot;$$user&quot;, public, vectors&#39; -c logging_collector=on -c max_wal_size=2GB -c shared_buffers=512MB -c wal_compression=on restart: always volumes: {} </code></pre> <p>Once I made the modification, I couldn&#39;t even get the immich-server docker to start (it just keeps on retarting). I am pretty sure something is wrong with the way I mapped the vol somehow but I am just not skilled enough with docker to figure it out. been going at it for hours and hours... any ideas? </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Jj_cale\"> /u/Jj_cale </a> <br/> <span><a href=\"https://www.reddit.com/r/unRAID/comments/1hn1r5j/pulling_my_hair_out_with_immich_docker_compose/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/unRAID/comments/1hn1r5j/pulling_my_hair_out_with_immich_docker_compose/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Steam library","url":"https://www.reddit.com/r/unRAID/comments/1hmxeo3/steam_library/","date":1735245470,"author":"/u/smitchen0","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p>I am new to unraid and Iâ€™m sure this questions has been asked. I have a Linux desktop, Linux laptop, and a steam deck. I was also thinking of setting up another system as a game console running in my basement. </p> <p>What I want to do is have the two desktops be able to share the same library through unraid and, in individual sessions, play using the storage on the LAN. </p> <p>Is it as simple as setting up a steam library on the unraid and have any of my accounts (laptop, desktop etc.) look at that directory? This will be on one steam account and only one person playing at a time. </p> <p>It would be nice to always have all of my games downloaded and playable on any device rather than just transferring all the time. Is this easy to implement or is there a better method?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/smitchen0\"> /u/smitchen0 </a> <br/> <span><a href=\"https://www.reddit.com/r/unRAID/comments/1hmxeo3/steam_library/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/unRAID/comments/1hmxeo3/steam_library/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Is now a good time to buy 20TB re-cert drives?","url":"https://www.reddit.com/r/unRAID/comments/1hmwjtt/is_now_a_good_time_to_buy_20tb_recert_drives/","date":1735243143,"author":"/u/vovik0","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p>Are 20TB fairly priced on eBay right now? I feel like they were closer to $200 a couple months ago and I want to wait, but with the potential of tarrifs increasing prices and depleting used market stock I&#39;m itching to pull the trigger now. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/vovik0\"> /u/vovik0 </a> <br/> <span><a href=\"https://www.reddit.com/r/unRAID/comments/1hmwjtt/is_now_a_good_time_to_buy_20tb_recert_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/unRAID/comments/1hmwjtt/is_now_a_good_time_to_buy_20tb_recert_drives/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"I have EVERYTHING for my home server EXCEPT a cache SSD.","url":"https://www.reddit.com/r/unRAID/comments/1hmtvyo/i_have_everything_for_my_home_server_except_a/","date":1735235965,"author":"/u/-ThatGingerKid-","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p>So, here&#39;s my question, can I use one of my 1TB HHDs as my Cache for now, then when I get an SSD in a few weeks, shift things up and use the NVME drive as the cache and change out the cache HDD to be part of the NAS pool?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/-ThatGingerKid-\"> /u/-ThatGingerKid- </a> <br/> <span><a href=\"https://www.reddit.com/r/unRAID/comments/1hmtvyo/i_have_everything_for_my_home_server_except_a/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/unRAID/comments/1hmtvyo/i_have_everything_for_my_home_server_except_a/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"How to configure drives","url":"https://www.reddit.com/r/unRAID/comments/1hmszsq/how_to_configure_drives/","date":1735233506,"author":"/u/DonDachi","unread":true,"desc":"","content":"<!-- SC_OFF --><div class=\"md\"><p>Hello, I have a dumb question. How to configure raid 6?<br/> Recently I bought PC parts for my custom NAS server and I was going to use trueNAS on it, I watched a bunch of videos on it but sadly after a few hours of trying I gave up, it seems like in this new hardware trueNAS is not an option so I just installed unRAID on it. HDD setup is easy I created an array of six devices: 2 parity and 4 disk (is this now raid 6?) and it shows me ~16TB storage (all of 6 are 4TB disks) but if it were raid 6 I would have something about 12TB storage, right? I don&#39;t fully understand how it works.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/DonDachi\"> /u/DonDachi </a> <br/> <span><a href=\"https://www.reddit.com/r/unRAID/comments/1hmszsq/how_to_configure_drives/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/unRAID/comments/1hmszsq/how_to_configure_drives/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Are these SAS ports? I'm considering upgrading mobo/cpu/memory. Just wanted to make sure replacement mobo is compatible.","url":"https://www.reddit.com/r/unRAID/comments/1hmqw33/are_these_sas_ports_im_considering_upgrading/","date":1735227759,"author":"/u/tackle","unread":true,"desc":"","content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/tackle\"> /u/tackle </a> <br/> <span><a href=\"https://i.imgur.com/EkqcCYV.jpeg\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/unRAID/comments/1hmqw33/are_these_sas_ports_im_considering_upgrading/\">[comments]</a></span>","flags":null,"enclosureUrl":"","enclosureMime":""}]}