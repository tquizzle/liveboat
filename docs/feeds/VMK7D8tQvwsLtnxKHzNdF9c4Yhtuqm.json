{"id":"VMK7D8tQvwsLtnxKHzNdF9c4Yhtuqm","title":"Hacker News: Best","displayTitle":"HN","url":"https://hnrss.org/best","feedLink":"https://news.ycombinator.com/best","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":30,"items":[{"title":"The Burnout Machine","url":"https://unionize.fyi/","date":1742495057,"author":"flxfxp","guid":190,"unread":true,"content":"<p><i>Originally written by Biozombie, published in 2600 Hacker Quarterly, Autumn 2024</i></p><p>\n            Let’s get real for a minute: the tech industry loves to sell us on the myth of the \"dream\n            job.\"\n            You\n            know\n            the\n            pitch - beanbags in the office, free kombucha on tap, and \"Agile\" processes that are supposed to\n            make\n            everything\n            more flexible, more efficient. But the reality? It’s a meat grinder that chews up developers,\n            sysadmins,\n            and\n            infosec pros and spits them out the other side - burnt out, disillusioned, and disposable.\n        </p><p>\n            We’re living in a world where billion dollar tech companies expect us to live and breathe code,\n            demanding\n            80\n            hour weeks under the guise of \"passion.\" And what do we get in return? Burnout, anxiety, and the\n            constant\n            threat\n            of layoffs. It’s time to face facts: this industry is not your friend. It’s a machine, and\n            unless we\n            start\n            organizing, it’s going to keep grinding us down. It’s time to talk about unionizing tech jobs.\n        </p><p>\n            Remember when Agile was supposed to save us all? Flexible sprints, self-organizing teams - yeah, right. In\n            practice, Agile has been twisted into a tool for management to push us harder and faster. They say\n            it’s\n            about\n            \"responding to change over following a plan,\" but let’s be honest - it’s about dangling\n            more\n            carrots\n            and keeping\n            us on a treadmill that never stops. The sprint becomes a marathon, and we’re the ones paying the\n            price.\n\n        </p><p>\n            And then there’s burnout. We’re in an industry where burnout isn’t just common -\n            it’s\n            expected. If you’re not\n            pulling all-nighters, you’re \"not committed.\" If you’re not answering Slack messages at\n            midnight,\n            you’re \"not a\n            team player.\" This culture is toxic, and it’s only getting worse. The relentless churn of\n            projects,\n            the\n            constant\n            pressure to innovate, and the ever-present threat of obsolescence create a perfect storm of stress. And\n            what’s\n            the industry’s solution? A mindfulness app and a lecture on work-life balance. Give me a break.\n        </p><p>\n            Let’s talk about job security - because there isn’t any. The tech industry loves to hype itself\n            as a\n            meritocracy, where the best and the brightest rise to the top. But in reality, it’s a meat market. As\n            soon\n            as\n            you’re not \"on the cutting edge,\" you’re out. Outsourcing, contract work, gig economy\n            bullshit -\n            it’s all\n            designed to keep us insecure, to keep us grinding away at the next big thing with no guarantee that\n            we’ll\n            have a\n            job next week, next month, or next year.\n        </p><p>\n            Companies love to brag about their innovation, but the real innovation is finding new ways to make us\n            disposable. Permanent employment? That’s for suckers. Why pay benefits and offer job security when\n            they\n            can\n            churn through contractors and freelancers like cheap code? And don’t get me started on those\n            non-compete\n            clauses\n            - designed to keep you locked down and terrified to make a move that might actually be good for your career.\n        </p><p>\n            And let’s not forget the ethical side of this equation. We’re being asked to build the future,\n            to\n            develop AI,\n            blockchain, and all the other buzzword technologies that are supposed to change the world. But at what cost?\n            How\n            many of us have been forced to work on projects that make us sick to our stomachs - surveillance tech, data\n            mining tools, algorithms that reinforce social biases - because we don’t have the power to say no?\n        </p><p>\n            That’s the kicker. We’re the ones building the damn future, but we have no say in how it’s\n            built. We don’t get\n            to decide whether our code is used for good or for evil. And as long as we’re isolated, as long as\n            we’re afraid\n            to speak up because we might lose our jobs, nothing will change.\n        </p><p>\n            This industry isn’t going to fix itself. The billionaires at the top aren’t going to suddenly\n            grow a\n            conscience,\n            and they aren’t going to give us the power to push back. That power has to come from us - from\n            organizing,\n            from\n            resisting, from breaking - unless we organize, unless we unionize.\n        </p><p>\n            Unionizing isn’t just about getting better pay or benefits (though we desperately need both).\n            It’s\n            about taking\n            back some control. It’s about having a say in how we work, what we work on, and how we’re\n            treated.\n            It’s about\n            saying no to the endless churn, the burnout culture, the gig economy bullshit.\n        </p><p>\n            And don’t let anyone tell you it’s impossible. The Alphabet Workers Union at Google?\n            They’re\n            showing us it can\n            be done. They’re standing up to one of the biggest companies in the world and saying,\n            \"Enough.\" We\n            need\n            more of\n            that. We need to take that energy and spread it across the industry - across all the companies that are\n            profiting off our sweat and tears.\n        </p><p>\n            Hackers, we’ve always been about more than just code. We’ve been about freedom - freedom of\n            information, freedom\n            from control. Unionizing is the next logical step. It’s about taking the hacker ethos into the\n            workplace,\n            about\n            organizing to protect ourselves and each other.\n        </p><ul><li> Talk to your coworkers. Break the silence. The first step to\n                organizing is\n                realizing you’re not alone.\n            </li><li><strong>Support Existing Efforts:</strong> If you’re in a company where union efforts are already\n                underway, get\n                involved.\n                If not, start thinking about how you can start one.\n            </li><li> We’re hackers - we know how to communicate securely, how to\n                organize without\n                being\n                detected. Let’s use those skills to build something real, something that can stand up to the\n                powers that\n                be.</li><li> Let’s make sure that any union platform we build isn’t\n                just about wages and\n                hours, but\n                about ethics too. We need to have a say in what we’re building and how it’s used.</li></ul><p>\n            The tech industry is a runaway train, and if we don’t do something soon, we’re going to get run\n            over. The\n            burnout, the job insecurity, the nightmares - it’s all going to keep getting worse unless we take a\n            stand.\n            Unionizing isn’t just a nice idea - it’s a necessity.\n        </p><p>\n            So let’s do what hackers do best: let’s disrupt. Let’s take the tools they’ve given\n            us, the skills we’ve\n            honed, and use them to build something better. Let’s unionize. Let’s take back our industry,\n            take back our\n            jobs, and take back our futures.\n        </p><p>\n            The future of tech is being written right now, and it’s up to us to decide what kind of story it will\n            be.\n            Let’s make it a story we can be proud of.\n        </p>","contentLength":7983,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43427002"},{"title":"Claude can now search the web","url":"https://www.anthropic.com/news/web-search","date":1742489472,"author":"meetpateltech","guid":189,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43425655"},{"title":"FOSS infrastructure is under attack by AI companies","url":"https://thelibre.news/foss-infrastructure-is-under-attack-by-ai-companies/","date":1742475049,"author":"todsacerdoti","guid":188,"unread":true,"content":"<p>Three days ago, Drew DeVault - founder and CEO of SourceHut - published a blogpost called, \"Please stop externalizing your costs directly into my face\", where he complained that LLM companies were crawling data without respecting robosts.txt and causing severe outages to SourceHut.</p><p>I went, \"Interesting!\", and moved on.</p><p>Then, yesterday morning, KDE GitLab infrastructure was overwhelmed by another AI crawler, with IPs from an Alibaba range; this caused GitLab to be temporarily inaccessible by KDE developers.</p><p>I then discovered that, one week ago, an Anime girl started appearing on the GNOME GitLab instance, as the page was loaded. It turns out that it's the default loading page for Anubis, a proof-of-work challenger that blocks AI scrapers that are causing outages.</p><p>By now, it should be pretty clear that this is no coincidence. AI scrapers are getting more and more aggressive, and - since FOSS software relies on public collaboration, whereas private companies don't have that requirement - this is putting some extra burden on Open Source communities.</p><p>So let's try to get more details – going back to Drew's blogpost. According to Drew, LLM crawlers don't respect robots.txt requirements and include expensive endpoints like git blame, every page of every git log, and every commit in your repository. They do so using random User-Agents from tens of thousands of IP addresses, each one making no more than one HTTP request, trying to blend in with user traffic.</p><p>Due to this, it's hard to come off with a good set of mitigations. Drew says that several high-priority tasks have been delayed for weeks or months due to these interruptions, users have been occasionally affected (because it's hard to distinguish bots and humans), and - of course - this causes occasional outages of SourceHut.</p><p>Drew here does not distinguish between which AI companies are more or less respectful of robots.txt files, or more accurate in their user agent reporting; we'll be able to look more into that later.</p><p>Finally, Drew points out that this is not some isolated issue. He says, </p><blockquote>All of my sysadmin friends are dealing with the same problems, [and] every time I sit down for beers or dinner to socialize with sysadmin friends it's not long before we're complaining about the bots. [...] The desperation in these conversations is palpable.</blockquote><p>Which brings me back to yesterday's KDE GitLab issues. According to Ben, part of the KDE sysadmin team, all of the IPs that were performing this DDoS were claiming to be MS Edge, and were due to Chinese AI companies; he mentions that Western LLM operators, such as OpenAI and Anthropic, were at least setting a proper UA - again, more on this later.</p><p>The solution - for now - was to ban the version of Edge that the bots were claiming to be, though it's hard to believe that this will be a definitive solution; these bots do seem keen on changing user agents to try to blend in as much as possible.</p><p>Indeed, GNOME has been experiencing issues since a last November; as a temporary solution they had rate-limited non-logged in users from seeing merge requests and commits, which obviously also caused issues for real human guests.</p><p>The solution the eventually settled to was switching to Anubis. This is a page that presents a challenge to the browser, which then has to spend time doing some math and presenting the solution back to the server. If it's right, you get access to the website.</p><p>According to the developer, this project is \"a bit of a nuclear response, but AI scraper bots scraping so aggressively have forced my hand. I hate that I have to do this, but this is what we get for the modern Internet because bots don't conform to standards like robots.txt, even when they claim to\".</p><p>However, this is also causing user issues. When a lot of people open the link from the same place, it might happen that they get served some higher-difficulty exercise that will take some time to complete; there's one user reporting one minute delay, and another - from his phone - having to wait around two minutes.</p><p>Why? Well, a GitLab link was pasted in a chatroom! Similarly, the same happened when the Triple Buffering GNOME merge request was posted to Hacker News, and thus received a lot of attention over there. As the developer said, it's a nuclear option for crawlers, but it also has human consequences.</p><p>Over Mastodon, one GNOME sysadmin, Bart Piotrowski, kindly shared some numbers to let people fully understand the scope of the problem. According to him, in around two hours and a half they received 81k total requests, and out of those only 3% passed Anubi's proof of work, hinting at 97% of the traffic being bots – an insane number!</p><p>That said, at least  worked. Other organizations are having a harder time dealing with these scrapers.</p><p>As an example, here's Jonathan Corbet, who runs the FOSS news source LWN, warns users that the website might be \"occasionally sluggish\"… due to DDoS from AI scraper bots. He claims that \"only a small fraction of our traffic is serving actual human readers\", and at some point, the bots \"decides to hit us from hundreds of IP addresses at once. [..] They don't identify themselves as bots, and robots.txt is the only thing they don't read off the site\".</p><p>Many expressed solidarity, including Kevin Fenzi, sysadmin for the Fedora project. They've also been having issues with AI scrapers: firstly, one month ago they had to fight to get pagure.io to stay alive:</p><p>However, things got  over time, so they had to block a bunch of subnets, which has also impacted many real users. Out of desperation, at one point Kevin decided to ban the entire country of Brazil to get things to work again; to my understanding, this ban is still in effect, and it's not so clear where a longer-term solution might be found.</p><p>And, as Neal Gompa points out, even this blocking an entire country only gets you so far, and apparently the Fedora infrastructure has been \"regularly down for weeks\" because of AI scrapers.</p><p>Another project that's been hit by this issue  is Inkscape. According to Martin Owens, it's not \"the usual Chinese DDoS from last year, but from a pile of companies that started ignoring our spider conf and started spoofing their browser info. I now have a Prodigius block list. If you happen to work for a big company doing AI, you may not get our website anymore\".</p><p>And, well, Martin is not the only developer who has built a \"prodigious block list\". Even BigGrizzly from Frama software was flooded by a bad LLM crawler, and built a list of 460K IPs with spoofed user agents to ban; he's offering to share the list around.</p><p>One more comprehensive attempt at this is the \"ai.robots.txt\" project, an open list of web crawlers associated with AI companies. They offer a robots.txt that implements the Robots Exclusion Protocol and a .htaccess file that will return an error page when getting a request from any AI crawler in their list.</p><p>We can get some more numbers about the crawlers if we go a few months back. Here's a post by Dennis Schubert about the Diaspora (an Open Source decentralized social network) infrastructure, where he says that \"looking at the traffic logs made him impressively angry\".</p><p>In the blogpost, he claims that one fourth of his entire web traffic is due to bots with an OpenAI user agent, 15% is due to Amazon, 4.3% is due to Anthropic, and so on. Overall, we're talking about 70% of the entire requests being from AI companies.</p><blockquote>they don’t just crawl a page once and then move on. Oh, no, they come back every 6 hours because lol why not. They also don’t give a single flying fuck about , because why should they. [...] If you try to rate-limit them, they’ll just switch to other IPs all the time. If you try to block them by User Agent string, they’ll just switch to a non-bot UA string (no, really). This is literally a DDoS on the entire internet.</blockquote><p>A similar number is given by the Read the Docs project. In a blogpost called, \"AI crawlers need to be more respectful\", they claim that blocking  AI crawlers immediately decreased their traffic by 75%, going from 800GB/day to 200GB/day. This made the project save up around $1500 a month.</p><p>The rest of the article is pretty impressive too; they talk about crawlers downloading tens of terabytes of data within a few days, or more. It's hard to block them entirely, since they use various different IPs.</p><p>I do wonder how much of this is scraping for training data, and how much instead is the \"search\" function that most LLMs provide; nonetheless, according to Schubert, \"normal\" crawlers such as Google's and Bing's only add up to a fraction of a single percentage point, which hints at the fact that other companies are indeed abusing their web powers.</p><p>But it's not just scrapers, or I would've titled this \"AI scrapers\", not \"AI companies\". Another issue that Open Source community have been fighting with is AI-generated bug reports, as an example.</p><p>This was first reported by Daniel Stenberg of the Curl project, in a blogpost titled \"The I in LLM stands for Intelligence\". Curl offers a bug bounty project, but lately, they've noticed that many bug reports are generated by AI. These look credible and take up a lot of developer time to check, but they also contain the typical hallucinations you'd expect from AIs. </p><p>It's pretty crazy to have to go through your own code because a bug report confidently tells you there's some critical security issue to fix, and … not finding it, because the whole issue is just AI hallucination.</p><p>A similar issue was reported by Seth Larson, who's on the security report triage team for CPython, pip, urllib3, Requests, and more. He says,</p><blockquote>Recently I've noticed an uptick in extremely low-quality, spammy, and LLM-hallucinated security reports to open source projects. The issue is in the age of LLMs, these reports appear at first-glance to be potentially legitimate and thus require time to refute.</blockquote><p>This is a pretty big issue. As he points out, responding to security reports is expensive, and responding to invented but credible bug reports causes some significant additional burden on maintainers, which might drive them out of the Open Source world.</p><p>The article ends with a request: please, do not use AI or LLM systems for detecting vulnerabilities. He says, \"These systems today cannot understand code, finding security vulnerabilities requires understanding code AND understanding human-level concepts like intent, common usage, and context.\".</p><p>Again, I want to point out that these issues impact disproportionately on the FOSS world; not only do Open Source projects often have less resources compared to commercial products, but - being community-driven projects - much more of their infrastructure is public and thus susceptible to both crawlers and AI-generated bug reports or issues. </p>","contentLength":10751,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43422413"},{"title":"The Frontend Treadmill","url":"https://polotek.net/posts/the-frontend-treadmill/","date":1742473531,"author":"Kerrick","guid":187,"unread":true,"content":"<p>A lot of frontend teams are very convinced that rewriting their frontend will lead to the promised land. And I am the bearer of bad tidings.</p><p>If you are building a product that you hope has longevity, your frontend framework is the least interesting technical decision for you to make. And all of the time you spend arguing about it is wasted energy.</p><p>If your product is still around in 5 years, you’re doing great and you should feel successful. But guess what? Whatever framework you choose will be obsolete in 5 years. That’s just how the frontend community has been operating, and I don’t expect it to change soon. Even the popular frameworks that are still around are completely different. Because change is the name of the game. So they’re gonna rewrite their shit too and just give it a new version number.</p><p>Product teams that are smart are getting off the treadmill. Whatever framework you currently have, start investing in getting to know it deeply. Learn the tools until they are not an impediment to your progress. That’s the only option. Replacing it with a shiny new tool is a trap.</p><p>I also wanna give a piece of candid advice to engineers who are searching for jobs. If you feel strongly about what framework you want to use, please make that a criteria for your job search. Please stop walking into teams and derailing everything by trying to convince them to switch from framework X to your framework of choice. It’s really annoying and tremendously costly.</p><p>I always have to start with the cynical take. It’s just how I am. But I do want to talk about what I think should be happening instead.</p><p>Companies that want to reduce the cost of their frontend tech becoming obsoleted so often should be looking to get back to fundamentals. Your teams should be working closer to the web platform with a lot less complex abstractions. We need to relearn what the web is capable of and go back to that.</p><p>Let’s be clear, I’m not suggesting this is strictly better and the answer to all of your problems. I’m suggesting this as an intentional business tradeoff that I think provides more value and is less costly in the long run. I believe if you stick closer to core web technologies, you’ll be better able to hire capable engineers in the future without them convincing you they can’t do work without rewriting millions of lines of code.</p><p>And if you’re an engineer, you will be able to retain much higher market value over time if you dig into and understand core web technologies. I was here before react, and I’ll be here after it dies. You may trade some job marketability today. But it does a lot more for career longevity than trying to learn every new thing that gets popular. And you see how quickly they discarded us when the market turned anyway. Knowing certain tech won’t save you from those realities.</p><p>I couldn’t speak this candidly about this stuff when I held a management role. People can’t help but question my motivations and whatever agenda I may be pushing. Either that or I get into a lot of trouble with my internal team because they think I’m talking about them. But this is just what I’ve seen play out after doing this for 20+ years. And I feel like we need to be able to speak plainly.</p><p>This has been brewing in my head for a long time. The frontend ecosystem is kind of broken right now. And it’s frustrating to me for a few different reasons. New developers are having an extremely hard time learning enough skills to be gainfully employed. They are drowning in this complex garbage and feeling really disheartened. As a result, companies are finding it more difficult to do basic hiring. The bar is so high just to get a regular dev job. And everybody loses.</p><p>What’s even worse is that I believe a lot of this energy is wasted. People that are learning the current tech ecosystem are absolutely not learning web fundamentals. They are too abstracted away. And when the stack changes again, these folks are going to be at a serious disadvantage when they have to adapt away from what they learned. It’s a deep disservice to people’s professional careers, and it’s going to cause a lot of heartache later.</p><p>On a more personal note, this is frustrating to me because I think it’s a big part of why we’re seeing the web stagnate so much. I still run into lots of devs who are creative and enthusiastic about building cool things. They just can’t. They are trying and failing because the tools being recommended to them are just not approachable enough. And at the same time, they’re being convinced that learning fundamentals is a waste of time because it’s so different from what everybody is talking about.</p><p>I guess I want to close by stating my biases. I’m a web guy. I’ve been bullish on the web for 20+ years, and I will continue to be. I think it is an extremely capable and unique platform for delivering software. And it has only gotten better over time while retaining an incredible level of backwards compatibility. The underlying tools we have are dope now. But our current framework layer is working against the grain instead of embracing the platform.</p>","contentLength":5133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43422162"},{"title":"fd: A simple, fast and user-friendly alternative to 'find'","url":"https://github.com/sharkdp/fd","date":1742384657,"author":"tosh","guid":186,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43410692"},{"title":"I'm the Canadian who was detained by ICE for two weeks","url":"https://www.theguardian.com/us-news/2025/mar/19/canadian-detained-us-immigration-jasmine-mooney","date":1742383272,"author":"n1b0m","guid":185,"unread":true,"content":"<p>here was no explanation, no warning. One minute, I was in an immigration office talking to an officer about my work visa, which had been approved months before and allowed me, a Canadian, to work in the US. The next, I was told to put my hands against the wall, and patted down like a criminal before being <a href=\"https://www.theguardian.com/us-news/2025/mar/18/canadian-actor-jasmine-mooney-detained-mexico-border\" data-link-name=\"in body link\">sent to an Ice detention center</a> without the chance to talk to a lawyer.</p><p>I grew up in Whitehorse, Yukon, a small town in the northernmost part of <a href=\"https://www.theguardian.com/world/canada\" data-link-name=\"in body link\" data-component=\"auto-linked-tag\">Canada</a>. I always knew I wanted to do something bigger with my life. I left home early and moved to Vancouver, British Columbia, where I built a career spanning multiple industries – acting in film and television, owning bars and restaurants, flipping condos and managing Airbnbs.</p><p>In my 30s, I found my true passion working in the health and wellness industry. I was given the opportunity to help launch an American brand of health tonics called Holy! Water – a job that would involve moving to the US.</p><p>I was granted my trade Nafta work visa, which allows Canadian and Mexican citizens to work in the US in specific professional occupations, on my second attempt. It goes without saying, then, that I have no criminal record. I also love the US and consider myself to be a kind, hard-working person.</p><p>I started working in California and travelled back and forth between Canada and the US multiple times without any complications – until one day, upon returning to the US, a border officer questioned me about my initial visa denial and subsequent visa approval. He asked why I had gone to the San Diego border the second time to apply. I explained that that was where my lawyer’s offices were, and that he had wanted to accompany me to ensure there were no issues.</p><p>After a long interrogation, the officer told me it seemed “shady” and that my visa hadn’t been properly processed. He claimed I also couldn’t work for a company in the US that made use of hemp – one of the beverage ingredients. He revoked my visa, and told me I could still work for the company from Canada, but if I wanted to return to the US, I would need to reapply.</p><p>I was devastated; I had just started building a life in California. I stayed in Canada for the next few months, and was eventually offered a similar position with a different health and wellness brand.</p><p>I restarted the visa process and returned to the same immigration office at the San Diego border, since they had processed my visa before and I was familiar with it. Hours passed, with many confused opinions about my case. The officer I spoke to was kind but told me that, due to my previous issues, I needed to apply for my visa through the consulate. I told her I hadn’t been aware I needed to apply that way, but had no problem doing it.</p><p>Then she said something strange: “You didn’t do anything wrong. You are not in trouble, you are not a criminal.”</p><p>I remember thinking: <em>Why would she say that? Of course I’m not a criminal!</em></p><p>She then told me they had to send me back to Canada. That didn’t concern me; I assumed I would simply book a flight home. But as I sat searching for flights, a man approached me.</p><p>There was no explanation, no warning. He led me to a room, took my belongings from my hands and ordered me to put my hands against the wall. A woman immediately began patting me down. The commands came rapid-fire, one after another, too fast to process.</p><p>They took my shoes and pulled out my shoelaces.</p><p>“What are you doing? What is happening?” I asked.</p><p>“You are being detained.”</p><p>“I don’t understand. What does that mean? For how long?”</p><p>That would be the response to nearly every question I would ask over the next two weeks: “I don’t know.”</p><p>They brought me downstairs for a series of interviews and medical questions, searched my bags and told me I had to get rid of half my belongings because I couldn’t take everything with me.</p><p>“Take everything with me where?” I asked.</p><p>A woman asked me for the name of someone they could contact on my behalf. In moments like this, you realize you don’t actually know anyone’s phone number anymore. By some miracle, I had recently memorized my best friend Britt’s number because I had been putting my grocery points on her account.</p><p>I gave them her phone number.</p><p>They handed me a mat and a folded-up sheet of aluminum foil.</p><p>I was taken to a tiny, freezing cement cell with bright fluorescent lights and a toilet. There were five other women lying on their mats with the aluminum sheets wrapped over them, looking like dead bodies. The guard locked the door behind me.</p><p>For two days, we remained in that cell, only leaving briefly for food. The lights never turned off, we never knew what time it was and no one answered our questions. No one in the cell spoke English, so I either tried to sleep or meditate to keep from having a breakdown. I didn’t trust the food, so I fasted, assuming I wouldn’t be there long.</p><p>On the third day, I was finally allowed to make a phone call. I called Britt and told her that I didn’t understand what was happening, that no one would tell me when I was going home, and that she was my only contact.</p><p>They gave me a stack of paperwork to sign and told me I was being given a five-year ban unless I applied for re-entry through the consulate. The officer also said it didn’t matter whether I signed the papers or not; it was happening regardless.</p><p>I was so delirious that I just signed. I told them I would pay for my flight home and asked when I could leave.</p><p>Then they moved me to another cell – this time with no mat or blanket. I sat on the freezing cement floor for hours. That’s when I realized they were processing me into real jail: the Otay Mesa Detention Center.</p><p>I was told to shower, given a jail uniform, fingerprinted and interviewed. I begged for information.</p><p>“How long will I be here?”</p><p>“I don’t know your case,” the man said. “Could be days. Could be weeks. But I’m telling you right now – you need to mentally prepare yourself for months.”</p><p>I felt like I was going to throw up.</p><p>I was taken to the nurse’s office for a medical check. She asked what had happened to me. She had never seen a Canadian there before. When I told her my story, she grabbed my hand and said: “Do you believe in God?”</p><p>I told her I had only recently found God, but that I now believed in God more than anything.</p><p>“I believe God brought you here for a reason,” she said. “I know it feels like your life is in a million pieces, but you will be OK. Through this, I think you are going to find a way to help others.”</p><p>At the time, I didn’t know what that meant. She asked if she could pray for me. I held her hands and wept.</p><p>I felt like I had been sent an angel.</p><p>I was then placed in a real jail unit: two levels of cells surrounding a common area, just like in the movies. I was put in a tiny cell alone with a bunk bed and a toilet.</p><p>The best part: there were blankets. After three days without one, I wrapped myself in mine and finally felt some comfort.</p><p>For the first day, I didn’t leave my cell. I continued fasting, terrified that the food might make me sick. The only available water came from the tap attached to the toilet in our cells or a sink in the common area, neither of which felt safe to drink.</p><p>Eventually, I forced myself to step out, meet the guards and learn the rules. One of them told me: “No fighting.”</p><p>“I’m a lover, not a fighter,” I joked. He laughed.</p><p>I asked if there had ever been a fight here.</p><p>“In this unit? No,” he said. “No one in this unit has a criminal record.”</p><p>That’s when I started meeting the other women.</p><p>That’s when I started hearing their stories.</p><p>And that’s when I made a decision: I would never allow myself to feel sorry for my situation again. No matter how hard this was, I had to be grateful. Because every woman I met was in an even more difficult position than mine.</p><p>There were around 140 of us in our unit. Many women had lived and worked in the US legally for years but had overstayed their visas – often after reapplying and being denied. They had all been detained without warning.</p><p>If someone is a criminal, I agree they should be taken off the streets. But not one of these women had a criminal record. These women acknowledged that they shouldn’t have overstayed and took responsibility for their actions. But their frustration wasn’t about being held accountable; it was about the endless, bureaucratic limbo they had been trapped in.</p><p>The real issue was how long it took to get out of the system, with no clear answers, no timeline and no way to move forward. Once deported, many have no choice but to abandon everything they own because the cost of shipping their belongings back is too high.</p><p>I met a woman who had been on a road trip with her husband. She said they had 10-year work visas. While driving near the San Diego border, they mistakenly got into a lane leading to Mexico. They stopped and told the agent they didn’t have their passports on them, expecting to be redirected. Instead, they were detained. They are both pastors.</p><p>I met a family of three who had been living in the US for 11 years with work authorizations. They paid taxes and were waiting for their green cards. Every year, the mother had to undergo a background check, but this time, she was told to bring her whole family. When they arrived, they were taken into custody and told their status would now be processed from within the detention center.</p><p>Another woman from Canada had been living in the US with her husband who was detained after a traffic stop. She admitted she had overstayed her visa and accepted that she would be deported. But she had been stuck in the system for almost six weeks because she hadn’t had her passport. Who runs casual errands with their passport?</p><p>One woman had a 10-year visa. When it expired, she moved back to her home country, Venezuela. She admitted she had overstayed by one month before leaving. Later, she returned for a vacation and entered the US without issue. But when she took a domestic flight from Miami to Los Angeles, she was picked up by Ice and detained. She couldn’t be deported because Venezuela wasn’t accepting deportees. She didn’t know when she was getting out.</p><p>There was a girl from India who had overstayed her student visa for three days before heading back home. She then came back to the US on a new, valid visa to finish her master’s degree and was handed over to Ice due to the three days she had overstayed on her previous visa.</p><p>There were women who had been picked up off the street, from outside their workplaces, from their homes. All of these women told me that they had been detained for time spans ranging from a few weeks to 10 months. One woman’s daughter was outside the detention center protesting for her release.</p><p>That night, the pastor invited me to a service she was holding. A girl who spoke English translated for me as the women took turns sharing their prayers – prayers for their sick parents, for the children they hadn’t seen in weeks, for the loved ones they had been torn away from.</p><p>Then, unexpectedly, they asked if they could pray for me. I was new here, and they wanted to welcome me. They formed a circle around me, took my hands and prayed. I had never felt so much love, energy and compassion from a group of strangers in my life. Everyone was crying.</p><p>At 3am the next day, I was woken up in my cell.</p><p>“Pack your bag. You’re leaving.”</p><p>I jolted upright. “I get to go home?”</p><p>The officer shrugged. “I don’t know where you’re going.”</p><p>Of course. No one ever knew anything.</p><p>I grabbed my things and went downstairs, where 10 other women stood in silence, tears streaming down their faces. But these weren’t happy tears. That was the moment I learned the term “transferred”.</p><p>For many of these women, detention centers had become a twisted version of home. They had formed bonds, established routines and found slivers of comfort in the friendships they had built. Now, without warning, they were being torn apart and sent somewhere new. Watching them say goodbye, clinging to each other, was gut-wrenching.</p><p>I had no idea what was waiting for me next. In hindsight, that was probably for the best.</p><p>Our next stop was Arizona, the San Luis Regional Detention Center. The transfer process lasted 24 hours, a sleepless, grueling ordeal. This time, men were transported with us. Roughly 50 of us were crammed into a prison bus for the next five hours, packed together – women in the front, men in the back. We were bound in chains that wrapped tightly around our waists, with our cuffed hands secured to our bodies and shackles restraining our feet, forcing every movement into a slow, clinking struggle.</p><p>When we arrived at our next destination, we were forced to go through the entire intake process all over again, with medical exams, fingerprinting – and pregnancy tests; they lined us up in a filthy cell, squatting over a communal toilet, holding Dixie cups of urine while the nurse dropped pregnancy tests in each of our cups. It was disgusting.</p><p>We sat in freezing-cold jail cells for hours, waiting for everyone to be processed. Across the room, one of the women suddenly spotted her husband. They had both been detained and were now seeing each other for the first time in weeks.</p><p>The look on her face – pure love, relief and longing – was something I’ll never forget.</p><p>We were beyond exhausted. I felt like I was hallucinating.</p><p>The guard tossed us each a blanket: “Find a bed.”</p><p>There were no pillows. The room was ice cold, and one blanket wasn’t enough. Around me, women lay curled into themselves, heads covered, looking like a room full of corpses. This place made the last jail feel like the Four Seasons.</p><p>I kept telling myself: <em>Do not let this break you.</em></p><p>Thirty of us shared one room. We were given one Styrofoam cup for water and one plastic spoon that we had to reuse for every meal. I eventually had to start trying to eat and, sure enough, I got sick. None of the uniforms fit, and everyone had men’s shoes on. The towels they gave us to shower were hand towels. They wouldn’t give us more blankets. The fluorescent lights shined on us 24/7.</p><p>Everything felt like it was meant to break you. Nothing was explained to us. I wasn’t given a phone call. We were locked in a room, no daylight, with no idea when we would get out.</p><p>I tried to stay calm as every fiber of my being raged towards panic mode. I didn’t know how I would tell Britt where I was. Then, as if sent from God, one of the women showed me a tablet attached to the wall where I could send emails. I only remembered my CEO’s email from memory. I typed out a message, praying he would see it.</p><p>Through him, I was able to connect with Britt. She told me that they were working around the clock trying to get me out. But no one had any answers; the system made it next to impossible. I told her about the conditions in this new place, and that was when we decided to go to the media.</p><p>She started working with a reporter and asked whether I would be able to call her so she could loop him in. The international phone account that Britt had previously tried to set up for me wasn’t working, so one of the other women offered to let me use her phone account to make the call.</p><p>We were all in this together.</p><p>With nothing to do in my cell but talk, I made new friends – women who had risked everything for the chance at a better life for themselves and their families.</p><p>Through them, I learned the harsh reality of seeking asylum. Showing me their physical scars, they explained how they had paid smugglers anywhere from $20,000 to $60,000 to reach the US border, enduring brutal jungles and horrendous conditions.</p><p>One woman had been offered asylum in Mexico within two weeks but had been encouraged to keep going to the US. Now, she was stuck, living in a nightmare, separated from her young children for months. She sobbed, telling me how she felt like the worst mother in the world.</p><p>Many of these women were highly educated and spoke multiple languages. Yet, they had been advised to pretend they didn’t speak English because it would supposedly increase their chances of asylum.</p><p>Some believed they were being used as examples, as warnings to others not to try to come.</p><p>Women were starting to panic in this new facility, and knowing I was most likely the first person to get out, they wrote letters and messages for me to send to their families.</p><p>It felt like we had all been kidnapped, thrown into some sort of sick psychological experiment meant to strip us of every ounce of strength and dignity.</p><p>We were from different countries, spoke different languages and practiced different religions. Yet, in this place, none of that mattered. Everyone took care of each other. Everyone shared food. Everyone held each other when someone broke down. Everyone fought to keep each other’s hope alive.</p><p>I got a message from Britt. My story had started to blow up in the media.</p><p>Almost immediately after, I was told I was being released.</p><p>My Ice agent, who had never spoken to me, told my lawyer I could have left sooner if I had signed a withdrawal form, and that they hadn’t known I would pay for my own flight home.</p><p>From the moment I arrived, I begged every officer I saw to let me pay for my own ticket home. Not a single one of them ever spoke to me about my case.</p><p>To put things into perspective: I had a Canadian passport, lawyers, resources, media attention, friends, family and even politicians advocating for me. Yet, I was still detained for nearly two weeks.</p><p>Imagine what this system is like for every other person in there.</p><p>A small group of us were transferred back to San Diego at 2am – one last road trip, once again shackled in chains. I was then taken to the airport, where two officers were waiting for me. The media was there, so the officers snuck me in through a side door, trying to avoid anyone seeing me in restraints. I was beyond grateful that, at the very least, I didn’t have to walk through the airport in chains.</p><p>To my surprise, the officers escorting me were incredibly kind, and even funny. It was the first time I had laughed in weeks.</p><p>I asked if I could put my shoelaces back on.</p><p>“Yes,” one of them said with a grin. “But you better not run.”</p><p>“Yeah,” the other added. “Or we’ll have to tackle you in the airport. That’ll really make the headlines.”</p><p>I laughed, then told them I had spent a lot of time observing the guards during my detention and I couldn’t believe how often I saw humans treating other humans with such disregard. “But don’t worry,” I joked. “You two get five stars.”</p><p>When I finally landed in Canada, my mom and two best friends were waiting for me. So was the media. I spoke to them briefly, numb and delusional from exhaustion.</p><p>It was surreal listening to my friends recount everything they had done to get me out: working with lawyers, reaching out to the media, making endless calls to detention centers, desperately trying to get through to Ice or anyone who could help. They said the entire system felt rigged, designed to make it nearly impossible for anyone to get out.</p><p>The reality became clear: Ice detention isn’t just a bureaucratic nightmare. It’s a business. These facilities are privately owned and run for profit.</p><p>Companies like CoreCivic and GEO Group receive <a href=\"https://www.opensecrets.org/news/2022/06/private-prison-industry-shifts-focus-to-immigrant-detention-centers-funding-immigration-hawks/\" data-link-name=\"in body link\">government funding</a> based on the number of people they detain, which is why they <a href=\"https://theappeal.org/geo-group-earnings-mass-deportations/\" data-link-name=\"in body link\">lobby</a> for stricter immigration policies. It’s a <a href=\"https://investors.geogroup.com/news-releases/news-release-details/geo-group-awarded-15-year-contract-us-immigration-and-customs\" data-link-name=\"in body link\">lucrative business</a>: CoreCivic made over <a href=\"https://ir.corecivic.com/static-files/d3f1752e-87b8-4256-99ed-f3803c5817f8\" data-link-name=\"in body link\">$560m</a> from Ice contracts in a single year. In 2024, GEO Group made more than <a href=\"https://www.usaspending.gov/recipient/9b308edb-a62c-659b-704b-ef4e5cf3f795-P/2024\" data-link-name=\"in body link\">$763m</a> from Ice contracts.</p><p>The more detainees, the more money they make. It stands to reason that these companies have no incentive to release people quickly. What I had experienced was finally starting to make sense.</p><p>This is not just my story. It is the story of thousands and thousands of people still trapped in a system that profits from their suffering. I am writing in the hope that someone out there – someone with the power to change any of this – can help do something.</p><p>The strength I witnessed in those women, the love they gave despite their suffering, is what gives me faith. Faith that no matter how flawed the system, how cruel the circumstances, humanity will always shine through.</p><p>Even in the darkest places, within the most broken systems, humanity persists. Sometimes, it reveals itself in the smallest, most unexpected acts of kindness: a shared meal, a whispered prayer, a hand reaching out in the dark. We are defined by the love we extend, the courage we summon and the truths we are willing to tell.</p>","contentLength":20550,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43410548"},{"title":"US appeals court rules AI generated art cannot be copyrighted","url":"https://www.reuters.com/world/us/us-appeals-court-rejects-copyrights-ai-generated-art-lacking-human-creator-2025-03-18/","date":1742321853,"author":"rvz","guid":184,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43402790"},{"title":"Apple restricts Pebble from being awesome with iPhones","url":"https://ericmigi.com/blog/apple-restricts-pebble-from-being-awesome-with-iphones/","date":1742315001,"author":"griffinli","guid":183,"unread":true,"content":"<p>During Pebble v1, I learned how much harder it is to build a great smartwatch experience on iPhone than it is on Android. It sounds like things have actually gotten worse over the last 8 years. </p><p>I want to set expectations accordingly. We will build a good app for iOS, but be prepared - there is no way for us to support all the functionality that Apple Watch has access to. It’s impossible for a 3rd party smartwatch to send text messages, or perform actions on notifications (like dismissing, muting, replying) and many, many other things.</p><p>Here are the things that are harder or impossible for 3rd party smartwatches (ie non Apple Watches) to do on iPhone:</p><ul><li>There’s no way for a smartwatch to send text messages or iMessages.</li><li>You can’t reply to notifications or take ‘actions’ like marking something as done.</li><li>It’s very difficult to enable other iOS apps to work with Pebble. Basically iOS does not have the concept of ‘interprocess communication’(IPC) like on Android. What we did before was publish an SDK that other apps (like Strava) could integrate to make their own BLE connection to Pebble. It was a clunky quasi-solution that other apps didn’t like, because it was hard to test (among other things)</li><li>If you (accidentally) close our iOS app, then your watch can’t talk to app or internet</li><li>Impossible for watch to detect if you are using your phone, so your watch will buzz and display a notification even if you are staring at your iPhone</li><li>You can’t easily side load apps onto an iPhone. That means we have to publish the app on the iPhone appstore. This is a gigantic pain because Apple. Every update comes with the risk that a random app reviewer could make up some BS excuse and block the update.</li><li>Because of iOS Appstore rules, it would be hard for us to enable 3rd party watchface/app developers to charge for their work (ie we can’t easily make an appstore within our app)</li><li>Getting a <a href=\"https://pebble.github.io/rockyjs/\">Javascript engine to run in PebbleOS</a> forced us to go through many hoops due to iOS — creating a compiler inside the Pebble iPhone app that in itself needed to be written in (cross-compiled to) JS to work with Apple's restriction on downloadable code can only be JS</li><li>As a Pebble watch/app developer, using the iOS app as relay to the watch sucks since the \"developer mode\" terminates every few minutes</li></ul><p>As an aside, back at Pebble, we went to <a href=\"https://www.theverge.com/2015/11/23/9788146/pebble-time-quick-message-reply\">crazy lengths</a> to find a way to let Pebble users to send text messages from Pebble. Our bizdev team did an impressive custom SMS-over-IP deal with AT&amp;T to enable this, but the end result was a pretty rough user experience (messages sent from Pebble didn’t appear in the Messages app on iPhone).</p><p> - it sounds like the situation has actually . This 2024 <a href=\"https://s3.amazonaws.com/jnswire/jns-media/7d/b2/15969695/showtemp_83.pdf\">class action lawsuit</a> against Apple states that: </p><ul><li>You must set notifications to display full content previews on your lockscreen for them to also be sent to a 3rd party watch (new restriction added in iOS 13).</li><li>Apple closed off the ability of smartwatches after Pebble to negotiate with carriers to\nprovide messaging services, and now requires users to turn off iMessage (disabling iOS’s core messaging platform) if they want to take advantage of such contracts between a third-party smartwatch maker and cellular carriers.</li></ul><h3><strong>Why are things so much harder on iOS?</strong></h3><p>Well, mostly because Apple systematically makes it nearly impossible for 3rd party wearable developers to build a smartwatch experience comparable to Apple Watch experience.</p><p>Apple claims their restrictions on competitors are only about security, privacy, crafting a better experience etc etc. At least that’s what they tell you as they tuck you into bed. I personally don’t agree - they’re clearly using their market power to lock consumers into their walled ecosystem. This causes there to be less competition, which increases prices and reduces innovation. DOJ seems to agree. For now at least…Tim Apple <a href=\"https://www.axios.com/2025/01/03/tim-cook-apple-donate-1-million-trump-inauguration\">paid $1m</a> to <a href=\"https://www.tuaw.com/2025/01/22/tech-leaders-attend-trumps-inauguration-with-tim-cook/\">sit near Trump at the inauguration</a>, so who knows how long until Trump tells DOJ to drop the case. There’s also an Apple Watch class-action lawsuit working its way through the system.</p><h3><strong>But we’re going to try anyways</strong></h3><p>The problem is that 40% of everyone who signed up on <a href=\"http://repebble.com/\">rePebble.com</a> still uses an iPhone. So we’re going to make a damn iOS app. I guess we’re gluttons for punishment. Just understand a few things:</p><ul><li>Our watch will&nbsp;&nbsp;appear to have less developed functionality on iOS than Android. This is Apple’s fault, not ours.</li><li>Some features will appear first on our Android app, and then eventually we’ll add them to the iOS app. This is because the majority of our development team uses Android phones, and generally we’re building things for ourselves, so naturally Android comes first.</li><li>I don’t want to see any tweets or blog posts or complaints or whatever later on about this. I’m publishing this now so you can make an informed decision about whether to buy a new watch or not. If you’re worried about this, the easiest solution is to .</li></ul><p>Apple will never change their ways unless you, the Pebble-curious iPhone user, complain loudly or switch to Android. Which is also hard because Apple tries it’s best to lock you into their platform.</p><p><strong>Are you an iPhone user who wants to use our watches?</strong> Start by posting a comment below this post. Hopefully there will be a lot - let’s show them that people actually want this to improve.</p><p>If you live in the US, tell your elected representatives to support legislation like ACCESS Act and AICO.</p><p>If you live in Europe, thank you for voting for representatives who passed the DMA. We will be <a href=\"https://developer.apple.com/support/ios-interoperability/\">petitioning Apple</a> under DMA Article 6 to request interoperability with Apple Watch APIs. </p>","contentLength":5614,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43401245"},{"title":"Two new PebbleOS watches","url":"https://ericmigi.com/blog/introducing-two-new-pebbleos-watches/","date":1742313567,"author":"griffinli","guid":182,"unread":true,"content":"<p>We’re excited to announce two new smartwatches that run open source PebbleOS and are compatible with thousands of your beloved Pebble apps. </p><ul><li> has an ultra crisp black and white display, polycarbonate frame, costs $149 and starts shipping in July.</li><li> has a larger 64-colour display, metal frame, costs $225 and starts shipping in December.</li></ul><p>Both are available in limited quantities, with worldwide shipping. Prices are in USD. Pre-ordering is the only way to get one - they will not be sold in stores. Pre-order today at <a href=\"http://store.repebble.com/\">store.rePebble.com</a>!</p><h2>Why are we making new Pebble-like smartwatches?</h2><p>Pretty simple - because we want one! No company has made a perfect smartwatch for people like <a href=\"https://www.youtube.com/watch?v=lKie-vgUGdI\">us</a>, so we’re going to make the exact smartwatch we want. Read the <a href=\"https://ericmigi.com/blog/why-were-bringing-pebble-back\">full story on my blog</a>, but it comes down to 5 key features:</p><ul><li>Simple and beautiful design</li></ul><p>No smartwatch on the market since Pebble offers this combination of features…until today!</p><p>I think you might recognize this one 😉 It’s almost exactly a Pebble 2, upgraded with modern chips and new tricks. Duo is short for ‘Do-over’.</p><p><em><strong>Similar to Pebble 2, it features</strong></em></p><ul><li>Ultra crisp 1.26” black and white e-paper display</li><li>Runs 10,000+ Pebble apps and watchfaces</li><li>Lightweight polycarbonate frame in two colour options - White or Black</li><li>Water resistant (targeting IPX8)</li></ul><ul><li>30 day battery life (up from 7)</li><li>Linear resonance actuator (quieter and stronger than vibrating motor)</li><li>More reliable buttons (up to 30% longer lifetime in testing)</li><li>Barometer and compass sensors</li></ul><p>Since this watch will look and feel just like a Pebble 2, you can refamiliarize yourself with it via<a href=\"https://www.youtube.com/watch?v=KQh1b_srGM4\"> videos</a>, or<a href=\"https://www.starkinsider.com/2016/10/pebble-2-review-smartwatch-perfected.html?utm_source=chatgpt.com\"> reviews</a>. For people interested in hacking on PebbleOS firmware, we’re offering an optional JTAG connector. I recommend buying 2 units if you want to hack, just in case!</p><p>This is my dream watch. It’s everything Pebble Time 2 was going to be and more! </p><ul><li>64-colour 1.5” e-paper display. Same display as Pebble Time 2 - much more room for text and details (53% bigger and 88% more pixels)</li><li>Runs 10,000+ Pebble apps and watchfaces</li><li>Metal frame and buttons (Black/White and likely a 3rd colour option as well)</li><li>30 day battery life (estimate)</li><li>Flat glass lens (less glare and reflections than Pebble Time family curved lens)</li><li>Water resistant (targeting IPX8)</li><li>Linear resonance actuator (vibrator)</li><li>Standard 22mm watch strap</li></ul><p>The industrial design is closely based on Pebble 2, which I really love. It’s slightly bigger to accommodate the larger display. Both the frame and buttons are made of metal (most likely CNC milled aluminum). More details, including final colour options, will be shared later this year. </p><p>Left: Core 2 Duo - Right: Core Time 2</p><table><tbody><tr></tr><tr></tr><tr></tr><tr><td>6-axis IMU, compass, barometer</td></tr><tr></tr><tr><td><em><strong>Linear resonance actuator (vibrator)</strong></em></td></tr><tr></tr><tr></tr><tr></tr><tr><td>Heart rate, step and sleep tracking</td></tr><tr></tr></tbody></table><p>Each watch runs open source&nbsp;<a href=\"https://github.com/pebble-dev/pebble-firmware\">PebbleOS</a>. This enables all the baseline Pebble features like receiving notifications, timeline, watchfaces, alarms, timers, calendar, music control, basic fitness tracking, etc. </p><p>The really fun part is that most of the existing 10,000+ PebbleOS watchfaces and apps will immediately work on these new watches, though some may try to access web services that no longer exist. Browse the full appstore on <a href=\"https://apps.rebble.io/en_US/watchfaces\">apps.rebble.io</a>.</p><p>Existing apps/faces will show up with a border on Core Time 2 until developers update them, since it has a larger display (200x228 vs 144x168 pixels). Read more about on the <a href=\"https://developer.rebble.io/developer.pebble.com/blog/2016/10/11/Emery-SDK-Beta/index.html\">old Pebble dev blog</a>.</p><p>We will publish a companion mobile app for Android and iOS. My friend and past Pebble colleague, Steve, recently joined us to lead this effort. He’s joining crc32, long-time <a href=\"https://github.com/pebble-dev/mobile-app/actions/runs/13609363513\">Cobble</a> developer, who has been working with me since last summer. We’ll also be working on an updated SDK for creating new PebbleOS watchfaces or apps.</p><p>These watches will be sold exclusively through <a href=\"http://store.repebble.com/\">store.rePebble.com</a>. Due to limited supply of display inventory, both watches will be manufactured in limited quantities. I highly recommend placing a pre-order - we will be manufacturing fewer watches than the number of people who have signed up on rePebble already! A pre-order secures your watch but gives you flexibility if you change your mind - you can get a refund at any time up until your watch ships. </p><p>Left: Core 2 Duo running PebbleOS - Right: Engineering samples</p><p>Schedule-wise,  is quite far along. We’ve already produced dozens of Core 2 Duo watches for testing and development. We’ve tested and confirmed our <a href=\"https://i.imgur.com/HwdTncx.mp4\">button improvements</a>. PebbleOS has been compiled for the new architecture and runs on the watch. All firmware development is open source - you can follow the fun on <a href=\"https://github.com/pebble-dev/pebble-firmware/commits/main/\">Github</a> and <a href=\"https://discord.gg/aRUAYFN\">Discord</a>. Our current schedule calls for shipments to begin in July. </p><p>How are we so far along, given that PebbleOS was only open-sourced in January? Two things helped: a) I took a monetary risk and began product development a bit earlier 😉,&nbsp;and b) we found a supplier who still had inventory of some Pebble 2 components. </p><p>Core Time 2 display lighting up! </p><p>For , we’ve finished component selection, initial industrial and mechanical design, and found sources for long lead time components. Now, we’re in the middle of creating the first prototypes. </p><p>The grand irony of hardware development is that software development is usually the slowest part of the project. Not this time! We’re extraordinarily thankful to Google for open sourcing PebbleOS, which gave us a massive boost.</p><p>We’ll share more details (like Core Time 2 frame colour options) later this year, as we get closer to mass production. Our current schedule shows shipments beginning in December. Follow along on this <a href=\"https://ericmigi.com/\">blog</a>, via <a href=\"https://repebble.com/signup\">email</a>, on <a href=\"https://bsky.app/profile/ericmigi.com\">Bluesky</a> or <a href=\"https://twitter.com/ericmigi\">Twitter</a>.</p><h3>Why were these specific specifications selected?</h3><p>Building a smartwatch is an exhausting (😂) exercise of constraint maximization. Think of it as linear algebra - we’re solving multiple equations with multiple unknown variables. The primary constraint is display selection. This choice governs the size and shape of the physical design, as well as being the component with the most power drain and biggest cost. Other variables are Bluetooth chip (governs: cost, software compatibility, engineering time), factory selection (cost, quality, speed, risk), sensors (power, cost, software), battery (size, battery life, cost) and more!</p><p>Despite what Pebble’s second Kickstarter branding proclaimed, solving this inherently requires compromise. It’s a fine line to walk and generally the data is not 100% known upfront, so it inevitably requires some degree of trusting your gut. </p><p>: we’re adding a touchscreen to Core Time 2. Why? Very specifically, I want to add the concept of ‘complications’ to watchfaces and widgets. Like on Apple Watch, these complications/widgets will show glanceable information like weather, next calendar event, step count, etc. The touch screen adds the ability to tap on the complication and directly open the associated app. This is much faster to use than opening an app via the button menu, and saves your quick launch (long-press on the buttons) for other apps. The touchscreen may be used for other interactions, like swiping to rapidly scroll down a list, but that will be lower priority. </p><p>Core Time 2 will be the first PebbleOS watch to ship with a touchscreen, but not the first that we designed! In 2015, Pebble designed it into a watch called Cutts (or C2) and did some (very) preliminary software development work to integrate a touchscreen into PebbleOS.</p><p>: I’m really excited that battery life will be increased from 7 days to 1 month! This is due to massive improvements in Bluetooth chip power efficiency over the last 10 years. </p><p>: we’re adding this primarily for potential use in apps that benefit from audio output, like a ChatGPT or other AI agent app. It can’t easily be used for making voice calls, since the watches will not support Bluetooth Classic (required for headset profiles), but theoretically someone could write a custom voice calling client (eg SIP or something).</p><p>: neither watch will support smartstraps. Sorry. Most people don’t even remember this feature even existed, which is kinda the answer to why it will not be supported. RIP.</p><h3>You shouldn’t get one if…</h3><p><strong>You need a perfectly polished smartwatch.</strong> This project is a labour of love rather than a startup trying to sell millions of watches. There may be some rough edges (literally). Things will get delayed. Some features will not be ready at launch. Things could break. Things could not last as long as you’d like. The only thing we can guarantee is that it will be awesome and a lot of fun! Every time you look down at your watch, you will smile 🙂</p><p><strong>You’re looking for a fitness or sports watch</strong>. That’s not what we’re making. From what we hear, Garmin watches are great for runners/cyclists/triathletes!   </p><p><strong>You’re comparing this to an Apple Watch</strong>. There is NO way for a 3rd party smartwatch to compete with Apple Watch. Apple restricts 3rd parties in major ways - read <a href=\"https://ericmigi.com/blog/apple-restricts-pebble-from-being-awesome-with-iphones\">my blog post</a> for more information. For example, 3rd party watches on iOS cannot send replies to notifications.   </p><p>These watches are not made for everyone. We want to be upfront with you about what to expect. </p><blockquote><p>Watch images above feature impeccably designed watchfaces from <a href=\"https://ttmm.is/pebble/\">TTMM</a>, including one of my all-time favourites - <a href=\"https://apps.rebble.io/en_US/application/57812aa56c21044501000ed5?query=ttmm&amp;section=watchfaces\">TTMMBRN</a>. Thank you Albert!</p></blockquote>","contentLength":9212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43400989"},{"title":"Block YouTube ads on AppleTV by decrypting and stripping ads from Profobuf (2022)","url":"https://ericdraken.com/pfsense-decrypt-ad-traffic/","date":1742284790,"author":"udev4096","guid":181,"unread":true,"content":"<div><div><p>I discovered that putting a man-in-the-middle proxy between my Apple TV and the world lets me decrypt HTTPS traffic. From there, I can read the Protocol Buffer data Google uses to populate YouTube with ads. It is too CPU-intensive to decode Protobuf on the fly, so instead, I found a flaw in the Protobuf format which allows me to reliably change one byte to obliterate ads.</p><p>What follows is a reference guide for setting up a bare-metal network router to block malicious ads, obnoxious ads, tracking, clickbait, crypto-jackers, scam popups, Windows spying on you, etc. using blocklists to protect all networked devices.</p></div></div><div><div> Let’s build a cryptographically-strong router with FreeBSD and pfSense to completely block YouTube ads using a flaw in the Google Protocol Buffer format to completely block pre-roll, mid-roll, and end-roll YouTube ads on Apple TV and iPhones, network-wide.</div></div><div><div> I want to support content creators, so to be fair, after a few months of blocking YouTube ads, I am now paying for YouTube Premium; Just because I can break something, doesn’t mean I need to.</div></div><h3>Part 1 – Setup pfSense on Bare-Metal</h3><h3>Part 2 – Isolate Network LANs</h3><h3>Part 3 – Setup DNS Adblocking</h3><h3>Part 4 – Trick the YouTube Ad Algorithm</h3><h3>Part 5 – Decrypt HTTPS Traffic</h3><h3>Part 6 – Intercept Apple TV and iOS YouTube Ads</h3><h3>Part 7 – Reverse-Engineer Protobuf Messages</h3><h2>Why block Malicious Ads and Behaviour Tracking?</h2><p>You are a valuable commodity that is bought and sold without your knowledge or consent. You will be tricked with clickbait, distracted with large ads, and enticed to leave the site you are on at every opportunity. Plus, everything you do online is being monitored so your habits and searches can be remarketed and sold over and over again for years.</p><p> – Knowing what you like to watch and read, what phone you have, what you watch on Netflix, what you shop for, what you ask Alexa about, yout taste in music, etc. is  valuable to advertisers. Spying on people is such a big problem that Europe passed the <a href=\"https://en.wikipedia.org/wiki/General_Data_Protection_Regulation\" target=\"_blank\" rel=\"nofollow\">GDPR law</a> so every site you visit asks if you are okay with cookies (and we blindly click “ok” to hide the banner). We must wrestle back privacy ourselves.</p><p> – If privacy doesn’t concern you, how about this: it is well-known that between 25% and 40% of network traffic is ads, tracking, JavaScript to load trackers (<a href=\"https://github.com/fingerprintjs/fingerprintjs\" target=\"_blank\" rel=\"nofollow\">fingerprint.js</a>, <a href=\"https://www.npmjs.com/package/@analytics/google-tag-manager\" target=\"_blank\" rel=\"nofollow\">googletagmanager.js</a>), websocket traffic to collect how you scroll and what you type (<a href=\"https://www.hotjar.com/\" target=\"_blank\" rel=\"nofollow\">Hotjar</a>), and the like. Do you have a 100 Mbps internet connection? Consider it 60 Mbps!</p><p> – Then there is clickbait. “You won’t believe what Tom Cruise did. He…” and you may want to click. Then you are in the spider’s web. How about fake news? Or articles that don’t say “sponsored” in size-8 font, but now say “underscored” to be clever. What is even real anymore? As soon as you click on clickbait, you may end up on a page with a dozen more ads that aren’t approved by Google but lead to a dark world of maliciousness. <strong>Clickbait is so incredibly profitable to scammers.</strong></p><p> – Some websites will load crypto-mining JavaScript (e.g. <a href=\"https://www.fortinet.com/blog/threat-research/the-growing-trend-of-coin-miner-javascript-infection\" target=\"_blank\" rel=\"nofollow\">CoinHive.js</a>) so while you read, they overheat and abuse your computer to try to make a few pennies. Some sites will load JavaScript that tries to steal from your crypto wallet or trick you into transferring cryptocurrency.</p><div><div> It is highly lucrative yet detrimental to you to track and trick you, and only  can do something about it.</div></div><p>A virtual machine, Docker image, or Raspberry Pi are not performant enough to protect a whole SMB network; We need dedicated hardware with a cryptographic instruction set so that its only function is to route, decrypt, and monitor packets in and out. Here is what I used.</p><ul><li>A mini PC with the AES-NI instruction set (e.g. J4125)</li><li>Several gigabytes of DDR4 RAM (e.g. 32 GiB)</li><li>A decent mSATA SSD drive (e.g. 128 GiB)</li><li>A USB drive to transfer pfSense</li></ul><p>I’ve ordered a mini J4125 PC from AliExpress, ordered 32 GB of DDR4 RAM and a 128 GB mSATA from Amazon, and will assemble them for the first time now.</p><div><div> Out of caution, I searched diligently for a barebones mini PC that did not include RAM or an SSD; there is nothing stopping an overseas seller from including some generic RAM and SSD but charging Samsung prices.</div></div><div><div> 128 GB of disk space on a router? Yup. That should be plenty of space to hold logs and not wear down the SSD too quickly,  to allow beautiful packet capture (and maybe an edge cache for NPM and Docker?).</div></div><p>A beautiful box, isn’t it? It only has 3 LAN ports, but it can be extended with network switches.</p><h2>Install pfSense on Bare Metal</h2><p>I’ve never used <a href=\"https://www.pfsense.org/getting-started/\" target=\"_blank\" rel=\"nofollow\">pfSense</a> before, so we will explore this together. The compressed image is about 360 MB and can be flashed to a USB drive with an AppImage binary of Etcher (very cool). Decisions, decisions: VGA install or serial? Let’s serial into the new router. Why not?</p><p>Well, that looks painful. It would also be a whole production to serial into the box in case of an emergency because the serial port is inside, and there isn’t even an RS232 or JTAG connector – just some narrow header pins. Yikes. Let’s go with VGA and plug a keyboard into the USB port – get ready to navigate with arrows and tabs.</p><p>I’ll follow this guide on <a href=\"https://www.youtube.com/watch?v=9kSZ1oM-4ZM\" target=\"_blank\" rel=\"nofollow\">YouTube</a>. I’ll pass on encrypting the disk since I would like to avoid entering a passphrase each time the mini PC reboots. A stripe disk is fine since there is only one disk. I have no idea what to expect yet, so I will pass on dropping to a shell for a more advanced configuration.</p><p>I ejected the USB containing the boot image (important) and rebooted the little box. It played a melody on the internal speaker (there is an internal buzzer and thankfully it isn’t very loud).</p><p>Do I need to have a LAN connection already, or can I just start the thing? I’ll just start pfSense and let it complain to me if it wants… and according to the YouTube <a href=\"https://youtu.be/9kSZ1oM-4ZM?t=610\" target=\"_blank\" rel=\"nofollow\">tutorial</a>, I should guess which port is LAN 1. I’ll do that now.</p><p>I figured out that I should set the LAN 1 to a static IP address that is not in my existing router’s DHCP range, so I went with . Now I can access an admin web portal (/). Hooray.</p><p>Yikes, the mini PC beeped at me and informed me that ‘admin’ has logged in. That startled me a bit, but hey that is pretty neat.</p><h2>Enable the AES-NI Cryptographic Instruction</h2><p>I played around with the wizard, used defaults, and got to the web configurator. The first thing that caught my eye was <code>AES-NI CPU Crypto: Yes (inactive)</code>. I went out of my way to get a mini PC with AES-NI. What gives?</p><p>Ah, this needs to be enabled in System &gt; Advanced &gt; Miscellaneous. Why not auto-detect this and use the best option? I’m glad I spotted that, or else this mini PC might as well be a Celeron J1900 of yesteryear.</p><p>Having 32 GiB of RAM, let’s take advantage of that and use a generous amount of RAM for  and , and since hopefully this 128 GiB SSD has wear levelling, let’s take a RAM Disk backup every hour.</p><p>Reboot!  is now active.</p><p>This dashboard is pretty slick. I’m just discovering that there are widgets that can be added to the Dashboard, including S.M.A.R.T to alert us if the SSD is going bad. Nice.</p><p>Hang on, when I added the Services Status widget, something called  shows up. What is that? Research shows it’s a daemon for hardware smart keys that we can probably do without(?). It can be disabled in the  file like so:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Wait. After some time went by, I noticed the router slowed down, fatally.</p><div><div> Do NOT try to disable the Smart Card Service as it is needed by IPsec; if you start experimenting with an IPsec VPN tunnel and the  daemon is disabled, then your hard disk will fill up with logs and your CPU will run hot.</div></div><h2>Adblocking with pfBlockerNG</h2><p>This unboxing and setup has been fun, but I’d like to block all the bad traffic on my network. I’ve been using a workhorse of a DNS-level adblocker called <a href=\"https://ericdraken.com/block-malicious-ads-with-pi-hole/\" target=\"_blank\">Pi-Hole</a> on a… yes, Pi, but it would be nice if I can reclaim that wee bit of hardware for something else and use a comparable add-on module in pfSense. Let’s explore that now.</p><blockquote><p>pfBlockerNG is a very powerful package for pfSense® which provides advertisement and malicious content blocking along with geo-blocking capabilities.</p></blockquote><p>Question: Do I install the first  or the  which feels like a developer version? I’m a software developer, so this is for me, but am I a pfSense developer? No. Maybe it will show me advanced logs or I can mess about with LUA? Let’s Google this.</p><p>From <a href=\"https://forum.netgate.com/topic/156604/pfblockerng-vs-pfblockerng-devel\" target=\"_blank\" rel=\"nofollow\">here</a>, random people are saying to install the development version. Another <a href=\"https://linuxincluded.com/block-ads-malvertising-on-pfsense-using-pfblockerng-dnsbl/\" target=\"_blank\" rel=\"nofollow\">blogger</a> advocates using the dev version as well. Meh, I guess we can install , , and Python 3.8. This doesn’t  like a development version since it has exciting dependencies.</p><p>That was painless and only added an extra 20 MiB. It seems a lot of the dependencies are part of pfSense already. The knight at the end of Raiders would say that I have chosen wisely (hey, why did Indy age like a normal person up to Indy 4 if he drank the immortality water that the thousand-year-old knight also drank?).</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>There are a lot of options in step three. This is not like Pi-hole at all. I’m going to <a href=\"https://www.youtube.com/watch?v=xizAeAqYde4\" target=\"_blank\" rel=\"nofollow\">come back to this</a> and set up my network instead so I accomplish retiring my Nighthawk R700 or giving it new life as a Wi-Fi AP.</p><div><div> If the  service won’t start or the status tab states , try deleting the empty file  (<a href=\"https://www.reddit.com/r/pfBlockerNG/comments/9x5sid/pfb_dnsbl_service_will_not_start/\" target=\"_blank\" rel=\"nofollow\">ref</a>).</div></div><h2>Isolate LANs for Security</h2><p>An opportunity has presented itself: I can create real networks on each of the three router Gigabit ports (not VLANs), and should I do so? Yes, yes I should. I would like a dedicated hardware network for all my home-phoning spy devices (Alexas and Apple TV) so they don’t flood my network with metrics info and “sure I’m muted and not listening to you” audio payloads back to their HQs.</p><p>I can see it now: A Wi-Fi AP on a hardware LAN that is isolated from everything else and dedicated to these gadgets,  runs through the adblocker  traps hard-coded DNS queries to  and  and others (I’ll have to explore this) so YouTube on my TV doesn’t sneakily bypass  any DNS-level blocker. It’s so Utopian an outcome I may not be able to sleep.</p><p>I’ve decided that my bottom-shelf TP-Link wireless router that is so old that  might as well be “A.D. 1200” is going to be my Wi-Fi AP for those IoT spy devices.</p><p>In sum, there will be a dedicated hardware LAN</p><ul><li>with a wireless AP (AC1200) for Amazon/Apple gadgets and the TV.</li><li>with a wired switch for all the beefy computers and clusters in my lab.</li><li>with another wireless AP (R7000) just for iPhones and watches.</li></ul><p>As an aside, since doing an Offensive Security hacking course in my spare time, and I rare-earth-magnet-strongly suggest isolating Wi-Fi devices from any critical LAN segments connected to devices that touch daily banking or stock trading (or crypto wallets).</p><h2>Class B IPv4 172.31.1.0/24 Network for Untrusted Devices</h2><p>The class B IPv4 range 172.16/16 is a valid range of private IP addresses. I’m not uncomfortable with Alexa and Apple TV being even on the same class network as my main LAN segment, so I will banish them to the class B private network at the hardware level, and my more trusted LANs will be on the traditional class C network (192.168/16). This helps mitigate any misconfigured  rules by naturally having no routes between the two networks.</p><p>Be sure to enable the DHCP resolver on the physical NIC that will connect smart devices (which mainly just tell me the weather and creepily listen to me sleep).</p><p>From this point, DHCP works on this new network, but by default, it assigns IP addresses but does  routing. All traffic is blocked by default.</p><p>We need to manually add rules so traffic on the physical NICs goe somewhere.</p><p>There is a logging message. Let me reproduce it below.</p><blockquote><p>Hint: the firewall has limited local log space. Don’t turn on logging for everything.</p></blockquote><p>I read that to mean, “Congratulations on not cheaping out on your SSD. Now go forth and log everything, my son.”</p><p>I’m not a new-age, fancy-jazz, coloured-light- or smart-plug-controlling guy who forgot how to turn on a light without his phone, so I do not need to have smart devices on the same network as my phone (why create dozens of wireless attack vectors into your home?). I’m classicly trained to actuate an electromechanical current interrupter on the wall and light let there be.</p><h2>Setup the Untrusted Wi-Fi AP</h2><p>How do I reach the admin UI of AC1200 Wi-Fi AP now? I factory reset it, plugged the WAN NIC into the ETH3 NIC of the pfSense router, but both devices are just blinking at me.</p><p>I suppose I can just Wi-Fi into the factory-reset AC1200. Yikes, 2016 was a bad year for responsive web UIs I take it. This is horrible; I’ll pull out a netbook for this. One sec.</p><p>It seems the Archer C5 has no AP Mode. This is my problem, not yours, but I’m still going to vent.</p><p>Oh, and the “refresh” icon on the top of the DCHP Leases page in pfSense is not “refresh”, but “reload service”. Whoops.</p><p>Well, I bricked the AC1200 router. I will have to run an Ethernet cable manually… but, wait, my thin notebook has no Ethernet ports and needs a USB-NIC adapter. Happy Friday.</p><div><div> Connect LAN to LAN, not the AP’s WAN to pfSense’s LAN unless you want to do double NATing.</div></div><p>There were shenanigans, but I set the LAN IP of the AC1200 to , the ETH3 NIC IP of the pfSense router to , and set the pfSense DHCP service on ETH3 to assign addresses . What  was setting the AC1200 to  as it was unreachable (reason unknown). Oh yes, I had to turn off firewally things and , and basically drop the horsepower of this TP-Link router down to that of a potato battery. The above settings allow me to access the AC1200 remotely now.</p><p>The other video ran its course, so I started following this <a href=\"https://www.youtube.com/watch?v=FPgPHJvLmh0\" target=\"_blank\" rel=\"nofollow\">YouTube video</a> (set the speed to 1.5x).</p><p>One more thing: I installed the  package for pfSense and scanned the AC1200 router, and found some sneaky ports open.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Port  is a print server port that I’ve now closed. However, the Archer C5 AC1200 is vulnerable to all kinds of Kali <a href=\"https://www.hackingtutorials.org/wifi-hacking-tutorials/tp-link-archer-c5-router-hacking/\" target=\"_blank\" rel=\"nofollow\">mischief</a> so it was wise to put it on its own network. I’m not sure how to close port 22 and the  service on it the AC1200 because the stock firmware is ancient and crippled, so I’ll just have to block port 22 on the whole LAN segment.</p><p>I’ve also taken care of disallowing private networks to ingress on the WAN (see the next section to set up DMZ).</p><h2>Unable to Reach 172.31.1.x from 192.168.10.x</h2><p>Ping and Traceroute are aiding me in my efforts to connect to the AC1200 Wi-Fi AP from my  LAN. I went ahead and added the subnet to the Symantec Firewall rules just in case (Symantec has its place now and then, but yes, definitely have available PC CPU horsepower to spare).</p><p>Now, it seems ICMP packets are no longer blocked between networks, but I still cannot ping the AP web management UI even though I can see the pings in the traffic logs.</p><p>I’ve even added an “any to any” firewall rule on the Untrusted network. No change.</p><p>Let’s try a stealth scan instead: <code>sudo nmap -sS -v 172.31.1.*</code>.</p><p>Nope, pfSense doesn’t like that at all. And, the whole network stopped working. Nice security! Also, dang.</p><p>The good news is that I’ve isolated the packet malaise to the TP-Link AC1200 box itself. I suspect that I need to add  to forward packets with no addresses in them, but I’d need root access to the AC1200. Let’s burn it to the ground and rebuild from its sprinkler-soaked ashes.</p><h2>Replace Stock Firmware on the AC1200 Wi-Fi Access Point</h2><p>Of course, I cannot actually stop Untrusted LAN devices from reaching the AC1200 as they all exist downstream from the pfSense box.</p><p>DD-WRT open-source router firmware, meet my ancient Archer C5 and do your thing.</p><p>The Archer C5 did not accept the DD-WRT firmware. Hmm… how about OpenWRT?</p><p>The Archer C5 did not accept the OpenWRT firmware either. What the actual facepalm (WTAF)?</p><p> My hardware is revision 2 using the Broadcom chipsets which are notoriously difficult networking chips.</p><div><div> Devices with Broadcom Wi-Fi chipsets have limited OpenWrt supportability (due to limited FLOSS driver availability for Broadcom chips). (REF: OpenWRT.org)</div></div><p>Alright, so OpenWRT, DD-WRT, and Tomato projects have no firmware for this AC1200 with unpopular Broadcom chipsets. Into the refuse bin it goes.</p><h2>Archer C5 v2 into the Refuse Bin, R7000 as the New Wi-Fi AP</h2><p>I’ve dismantled the AC1200 so I do not forget why I threw it out. It’s too bad because it’s so pretty on the inside, and they always say, “It is what is inside that counts… except if you are a router with Broadcom chips.”</p><p>The R7000 is factory reset, and here is the first problem:</p><div><div> On factory reset, the Nighthawk R7000 is pretty uptight about the format of the password. One rule is that no more than two identical consecutive characters are allowed. Well, thanks Netgear for pretty much providing a Regex to password crackers. Let’s disable all those rules with a few keystrokes to delete the JavaScript “blocking” the form submission. Now my admin password does not conform to the Regex and is super long. Muhahaha to Netgear password crackers.</div></div><p>The R7000 is in AP mode, but I can still access the pfSense web management page from the Untrusted network. Let’s lock down the web UI in pfSense under Firewall Rules.</p><h2>Set up the Trusted Wireless Network</h2><p>The Untrusted network is now looking good. It’s time to make the  R7000 Nighthawk I have into a Wi-Fi AP as well so my phone and watch have a safe place to connect to, as well as a laptop when I want to RDP into my wired machines from the kitchen. I was saving that for a honeypot AP, but I can come back to that later.</p><p>Let’s see if I can Wi-Fi into the Wireless LAN’s R7000…</p><div><div> Remember to physically unplug the pfSense upstream router from the R7000 because the R7000 is too helpful and will enter into AP mode by sensing any upstream routers, then you cannot get into the web UI anymore.</div></div><p>Since only my trusted devices should be on the Wireless LAN, I’ll turn off 2.4 GHz wi-fi because anything recent and wireless should support 5 GHz. That means those pesky AliExpress Pineapple wi-fi password stealers on the cheap side only use 2.4 GHz, so a neighbour is going to have to put in some effort to snoop on my network. Plus, 5 GHz is blocked more easily by walls and concrete, so I prefer it for averting medium-range snooping. But, I so am going to set up a honeypot and to brake check my faith in humanity.</p><p>It is normally straightforward to put a Wi-Fi router into AP mode by disabling WAN and DHCP.</p><h2>Network Devices Interconnectivity Check</h2><p>Do all my dozens of computers, laptops, Pis, clusters, NAS drives, and the like still connect as before? Most important is my web-scraping bot in a hardened, RAIDed, dedicated machine with its own UPS. But alas, I cannot SSH into it even though the SSH handshake packets make it to the hefty box.</p><p>Could this be our old frienemy IPv4 forwarding being disabled? Possibly. I’m able to SSH into the machine from my iPhone (seriously) when on the same network.</p><p>Nope. Adding  in the right place with a restart did not yield joy.</p><p>According to  (to tail  logs), UFW (Uncomplicated Firewall) is not blocking ICMP requests or TCP requests on port 22. When I do something nutty like try to SSH on, say, port 23, then I can see the UFW block logs in . Confirmed: Packets can reach that machine.</p><p>Running <code>tcpdump src 192.168.10.100</code> where the IP is from the Trusted network on the target machine shows it  responding to pings. I’m even getting replies to SSH handshake requests. So now we know that  packets are being dropped. Interesting! Aside:  is awesome.</p><p>Let’s follow the trail. Digging a little deeper I see replies to ICMP and SSH handshakes are being sent to some IP over HTTPS that I do not recognize. Bizzare. When I run the usual  tools I see that <a href=\"https://ericdraken.com/ssh-using-real-ip-bypass-vpn/\" target=\"_blank\">replies are going over a VPN</a> that I completely forgot about. Ha. Replies to a different subnet are egressing over the VPN, but cannot return properly. Neat.</p><p>Now that I remember what I did in 2019, I re-added NAT alias rules, and it’s showtime again.</p><h2>Windows File Sharing Gotchas</h2><p>Your path may be smoother, but I’ve always seem to make the Trench Run instead of remote-piloting a handful of lead-filled X-Wings at light speed right  the Death Star’s reactor to make it go boom: the easy way.</p><p>I’ve added some rules to allow Static DHCP devices to talk to each other – Windows devices – but by default, the Private Network in the Windows Defender uses the local subnet as the rule scope. That means different subnets are isolated. We can’t just relax the pfSense DHCP subnet mask to say  because it conflicts with another subnet. Instead, just to get file sharing working, I relax the  in Advanced Settings like below. Be sure to modify In and Out for SMB and ICMP.</p><p>Again, please add whatever subnets you desire instead of .</p><h2>Public Service Announcement: Edge Browser</h2><p>Why does the Microsoft Edge browser start automatically and run in the background, and why can’t I kill it when I ? If you’ve asked yourself this, you’re not alone. It turns out Edge starts up when you log in and it keeps running in the background. Here is the fix:</p><p>I suggest downloading Winaero Tweaker and applying registry tweaks to cut down on the Redmond Spy Machine.</p><h2>Block Clickbait, Endless Ads, and Dangerous Sites</h2><p>Thanks to web-browser and DNS-level adblockers (i.e. Pi-hole), it’s commonplace to block bad sites, crypto-miners, fingerprinters, trackers, remarketers, banners, pop-ups, fake tech-support scam alerts, and all manner of unscrupulousness designed to take advantage of you. Let’s take pfBlockerNG on pfSense for spin.</p><p> If you have multiple network interfaces (the mini PC has four), then you need to enable the Permit Firewall Rules for multiple interfaces and select them.</p><p>Would you like to have discretion over blocklists? Let’s add a DNS blocklist related to gambling and reload pfBlockerNG to see if a poker site is blocked on the Trusted LAN.</p><p>If you would prefer the connection to just close instead of rendering a PHP page, create a new PHP script with the following code and select it in the pfBlockerNG settings page:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><h2>Intercept All DNS Requests, Even to Hardcoded DNS Servers</h2><p>Let’s make sure all clients behind the pfSense router use the local Unbound DNS server so pfBlockerNG can act on them. We do not want apps and home assistants to bypass our DNS server, so we have to add some NAT rules.</p><p>First, we have to block DNS over TLS (for now) and only allow local DNS requests (note the rule order):</p><p>Here is a NAT rule for one interface. I started by making a rule for each interface except WAN (obviously) like this below.</p><div><div> NAT reflection should be disabled so the wild Internet cannot access our DNS server.</div></div><p>To make life simpler, I made a firewall alias of all non-WAN interfaces called . Covering IPv4 and IPV6 to redirect local DNS queries on port 53 to localhost are the following redirect rules:</p><p>Let’s also log trapped DNS requests. Head to the  page, click “Display Custom Options”, and add the lines:</p><p>Well, hello there, Microsoft Windows. What are you up to trying to reach Google Tag Manager? Naughty OS. That request is now black-holed to a non-existent IP at .</p><p>Let’s turn our attention to the TV and see how it fares under DNS interception.</p><h2>How to Restrict Apple TV and iPhone YouTube Ads?</h2><div><div> Regarding YouTube, YouTube has been showing 7-second and 15-second ads, twice, back-to-back, nearly every few minutes. Why are the ads incessant and so long? I do not mind the occasional ad, similar to live TV, but these frequent ads would warrant FTC complaints it they were on live TV.</div></div><p>YouTube is tricky because ads are also videos that come from the same domain, so domain-name blockers like pfBlockerNG cannot act on them. The best pfBlockerNG and Pi-hole can do is block  only after you watch an ad video and click on the ad.</p><p>Many people opt to use a web browser like Firefox or Chrome with <a href=\"https://chrome.google.com/webstore/detail/ublock-origin/cjpalhdlnbpafiamejdnhcphjbkeiagm?hl=en\" target=\"_blank\" rel=\"nofollow\">uBlock Origin</a> that acts on JavaScript as a workaround. It might be enough to watch YouTube on a web browser and stream that to a smart TV. However, we cannot restrict ads on the iPhone (without jailbreaking and compromising it).</p><p>What are our options? How can we safely restrict YouTube ads on all network devices?</p><h2>Trick the YouTube Ad Algorithm Instead</h2><div><div> Among friends, let’s say that English-speaking countries get ads for the most ridiculous things because their residents are assumed to have disposable income. Can we instead make YouTube think we are an undesirable advertising target?</div></div><p>What do ads in other parts of the world look like? Are those living in Antarctica or <a href=\"https://xkcd.com/713/\" target=\"_blank\" rel=\"nofollow\">Low Earth Orbit</a> getting a lot of ads too?</p><p>What would happen if we leverage the capabilities of this pfSense router to route YouTube Location Tracking information through a VPN that terminates in some remote part of the world with fewer YouTube viewers per capita? In other words, let’s make ourselves undesirable to advertisers and see if we get fewer ads.</p><h2>Research into YouTube Advertizing Spend</h2><p>Let’s also check some <a href=\"https://medium.com/@ChannelMeter/youtubes-top-countries-47b0d26dded\" target=\"_blank\" rel=\"nofollow\">YouTube statistics</a> about viewers by country for insights. Thinking about following some Reddit advice and VPN’ing into India? Think again.</p><p>That was 2019. This is <a href=\"https://backlinko.com/youtube-users\" target=\"_blank\" rel=\"nofollow\">2020</a>:</p><p>I’m not a digital advertiser, but I can see that people in the UK and Canada watch a large number of videos per sitting. If I  an advertiser though, I’d pump those two countries with video ad after video ad because, statistically, those residents will take the eyeball kicking. All things being equal, I definitely need a VPN to terminate outside of Canada, the UK, and the United States (English-speaking countries) to enjoy YouTube more.</p><p>Does age play a factor? Who  advertisers want? I want to be that guy on paper.</p><div><div> Let’s trick YouTube into believing I am a 70-year-old male living in Italy. Yes, that should definitely cut down on the Nespresso and Starbucks ads, at least.</div></div><p>How then to convince YouTube that I am a retired Sicilian living on a small chain island? I embellished that last part. Seventy and in Italy is sufficient.</p><p>Let’s do this. In the YouTube account…</p><p>It is doubtful that this is all it takes for our goal. Let’s find a VPN exit point in Italy.</p><p>Nice. <a href=\"https://ericdraken.com/out/nordvpn\" target=\"_blank\">NordVPN</a> has about 60 servers in Italy (that’s an affiliate link by the way).</p><h2>Selectively Route Apple TV Over the VPN</h2><p>Let’s go through some tutorials to set up  in pfSense. Just kidding! We’re going to use WireGuard – we have the Intel AES-NI crypto instruction set because we didn’t go cheap and get a yesteryear J1900 mini PC that sellers are trying to offload.</p><p>I’ll now install the FreeBSD WireGuard package.</p><p>Next, add a tunnel and enable it. According to this <a href=\"https://www.reddit.com/r/PFSENSE/comments/m0989o/nordvpn_wireguard_setup_works/\" target=\"_blank\" rel=\"nofollow\">thread</a> and this <a href=\"https://www.reddit.com/r/PFSENSE/comments/m0989o/nordvpn_wireguard_setup_works/\" target=\"_blank\" rel=\"nofollow\">thread</a> on Reddit, we need to get some information for WireGuard and NordLynx from a sacrificial Linux VM to transpose the settings (i.e. private key) to the pfSense router. No problem.</p><p>Run <code>sudo wg showconf nordlynx</code> to see your private key needed by the pfSense tunnel config.</p><p>Here are various screenshots that show the steps in more detail.</p><div><div> Enter  and then  as the subnet mask. Do not go for  as there is a glitch or bug in the UI or whathaveyou. The result will still be .</div></div><p>That should be enough to allow Diagnostics to  Italy.</p><p>Now that the easy part is out of the way, let’s set some Policy rules to send the Apple TV traffic over the VPN to Italy as a baseline test.</p><blockquote><p>Traffic from LAN to WAN is processed as described in the following more detailed example.</p><ul><li>Port forwards or 1:1 NAT on the LAN interface (e.g. proxy or DNS redirects)</li><li>Firewall rules for the LAN interface:<ul><li>Floating rules inbound on LAN</li><li>Rules for interface groups including the LAN interface</li></ul></li><li>1:1 NAT or Outbound NAT rules on WAN</li><li>Floating rules that match outbound on WAN</li></ul></blockquote><p>I’ll make an alias, for now, to hold some clients that have static DHCP entries and hostnames I gave them in pfSense.</p><p>Floating rules  have high precedence, so I’ll add some rules below the automatic pfBlockerNG rules that were created, and I’ll add a nice little blue separator while I’m here.</p><p>And here is that rule as a very long screenshot:</p><p>Apply. Wait. Let’s try it out using one of my notebooks connected to the Untrusted network.</p><p>Google is in Italian. Very cool. Now for the Apple TV.</p><p>Winner winner, chicken diner. All my YouTube is in Italian. I get some ads, not as many, but because Italians speak slowly and with a kind of sexy accent I do not mind the ads for Nutella at all.</p><p>With this technique, I no longer feel manipulated by non-English ads. I have personalized ads , but given my new status as a retired gentleman I should turn that back on to scare away advertising dollars, er, euros. I wonder if Netflix and Amazon Prime behave any differently…</p><p>Dang. Netflix is having problems. Amazon Prime is even worse. It looks like some CSS or font files are blocked as well, and the thumbnails aren’t loading. It’s time to move to Phase Two: Tunnel only YouTube traffic over the VPN.</p><div><div> Do not try to send all the Apple TV traffic over a VPN because Netflix, Prime, and others are wise to VPN providers and have gotten great at geofencing.</div></div><h2>Selectively Route Apple TV YouTube Traffic Over the VPN</h2><p>Let’s start by adding Firewall Policy rules to send the most common YouTube domains over the VPN.</p><p>As I’m about to add the rules, my hands hover over the keyboard not knowing what domains to tunnel. They need to be FQDN (fully-qualified domain names, no wildcards). Let’s open up a Chromium-based browser and see what traffic it generates in DevTools.</p><p>Here are some candidate FQDNs to add:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>But wait, I hear you ask, why  and ? This is a preventative measure just in case one of those domains is geo-jacked (Geo-IP LowJacking). I wouldn’t put it past Google engineers to geo-jack the fonts domains like , but I’ll take a chance they don’t in the interest of scaling to billions of page views efficiently.</p><p>Here are my new rules where I chain two of them using a tag so I can limit YouTube tunnelling to only the same untrusted machines (including Apple TV).</p><p>And with that, YouTube thinks I’m in Milan, Netflix and Prime Video think I am still in Canada, and the ads… oh the ads… they are few and far between, and when they do come on, they are just a treat to listen to in that slow, lack-of-harsh-aspirants-or-yelling of a beautiful language Italian is.</p><h2>Gotcha: DNS Race Condition</h2><p>A day has gone by and I’ve noticed that I only get Nutella and Ferrero Roche ads in the middle of videos, not at the start. Odd. I did some research and this is what I found:</p><p>This means that the hostnames are resolved to IP addresses  and those IPs are used in my VPN tunnelling policy rules.</p><blockquote><p>A hostname entry in a host or network type alias is <strong>periodically resolved and updated by the firewall every few minutes</strong>. The default interval is 300 seconds (5 minutes), and can be changed by adjusting the value of Aliases Hostnames Resolve Interval on System &gt; Advanced, Firewall &amp; NAT tab. – pfSense</p></blockquote><p>Ah-ha, so I suspect there is a DNS race condition. Let me explain:</p><p>This happens if, say, the Alias Daemon updates the IPs of the FQDNs. Then, I turn on the Apple TV for the first time all day. Since the usual TTL (time-to-live) of DNS queries is 1440 seconds (30 minutes), all the YouTube DNS entries will be cache misses and will need to be updated. At this point, the IPs from the second DNS queries may be from a pool and are not guaranteed to be the same that the Alias Daemon has. When the Alias Daemon checks again in five minutes, it may resolve the FQDNs to yet different IPs!</p><p>Let’s solve this by overwriting whatever TTL (time-to-live) YouTube has in its DNS entries:</p><p>And with that, no more DNS lookup race condition.</p><h2>Gotcha: Authentication Trouble, Forbidden 403 Error</h2><p>Sometimes videos will not play. For security, YouTube embeds your IP in the  request. I’ve known about this since my post about <a href=\"https://ericdraken.com/download-youtube-videos-with-php/\" target=\"_blank\">Download YouTube 4K Videos with PHP</a> back in 2016. The new problem is that various JavaScript and “are you human?” assets are tunnelled over VPN, but those darn domains like <code>r5---sn-hpa7kn76.googlevideo.com</code> are not tunnelled and thus come from the wrong IP. Queue the  error.</p><p>Let’s fail fast with a quick experiment: I’ve gotten the IP of the above second-level domain name (SLD), added it manually to the list of domains/IPs to VPN tunnel, applied the change, and refreshed YouTube:</p><p>Excellent. Now, we just need a way to tunnel that wildcard  domain. Unfortunately, the NAT and Firewall rules work with IPs, not <a href=\"https://www.reddit.com/r/PFSENSE/comments/7wwnun/wildcard_domains_in_aliases/\" target=\"_blank\" rel=\"nofollow\">wildcard domain names</a>. Can we predict or enumerate these domains?</p><p>Here is a Wireshark capture of DNS requests to  to show that the SLDs (second-level domains) are not eyeballably predictable:</p><p>Let’s drop into a web browser with adblocking disabled and walk the HAR waterfall of my interaction with YouTube that led to ads showing up.</p><p>What are GET requests like</p><p><code>GET https://r7---sn-uxa0n-t8ge.googlevideo.com/generate_204</code></p><p>doing, exactly? I’ll give this problem some thought offline.</p><h2>Gotcha: YouTube is Now Showing UK Ads, Not Italian Ads</h2><p>Before I could even solve the previous gotcha, British ads started showing up with the same frequency as if we did nothing. Ads from the UK are even more incessant than those from Canada, trailing behind the USA and India according to my earlier stats. It would be a complete failure if we get UK ads. Why does this happen suddenly? I’ve opened a fresh browser in a VM and tunnelled all traffic through Italy. The only leak I can find is when I query  on my Italian tunnel and see a UK address in the ASN. Could this small leak be our undoing?</p><p>Even with my browser’s language set to  and location data off, this is the only leak I can spot. Then, in addition to a VPN exiting in Italy, it has to be one that doesn’t leak  (Autonomous System Numbers – used for automated routing) that gives up a different country. Dang, Google, you’re good. I’m going to have to bring my A+ game to this one.</p><h2>Find a VPN Exit Node with no ASN Leak</h2><p>By visiting <code>https://nordvpn.com/servers/tools/</code>, I can see the VPN endpoint nodes in Italy. There are many Wireguard endpoints with <a href=\"https://ericdraken.com/out/nordvpn\" target=\"_blank\">NordVPN</a>. Just to move things forward for this exercise, I’ll add an OpenVPN tunnel in pfSense and connect to several VPN nodes and examine the ASNs. It’s better than nothing, and more importantly, I’d like to eliminate the  as the leak of GeoIP information. Here is the <a href=\"https://support.nordvpn.com/Connectivity/Router/1626958942/pfSense-2-5-Setup-with-NordVPN.htm\" target=\"_blank\" rel=\"nofollow\">guide</a> I used.</p><p>Through trial and error, I found a VPN node that is registered to an ISP in Italy as found in the  and  info.</p><h2>Hijack Google Video DNS Queries</h2><p>To make any of this work, I need a technique to route the wildcard  domain through the VPN.</p><div><div> Suppose I write a plugin for pfSense that periodically s the DNS query log, keeps track of the  queries, and adds them to a unique list of aliases for Google Video domains; if backed by an LRU eviction policy, this could keep working indefinitely. However, if each video uses a unique, mangled domain, then this does not work unless I hit refresh on every single video.</div></div><p>On the other hand, if I “hold up” the DNS query for the  domains, add the IPs to some alias list,  allow the DNS response to finish the round trip, we may be in business!</p><p>Where to even start? Here are some <a href=\"https://github.com/NLnetLabs/unbound/tree/master/pythonmod/examples\" target=\"_blank\" rel=\"nofollow\">Python example scripts</a> just to get some inspiration. A quick, mental reverse-engineering of a handful of scripts reveals that there are some event hooks available. Nice.</p><p>Among friends, let’s say that I can build up the pool of Google video IPs in real-time. How then to add these IPs programmatically to the firewall alias list for YouTube  restarting the firewall? One person actually hacked the PHP scripts in pfSense. Tempting, but I’ll do more research. Another person created a <a href=\"https://github.com/jaredhendrickson13/pfsense-api\" target=\"_blank\" rel=\"nofollow\">REST API for pfSense</a>. Jackpot!</p><div><div> We need to add IPs to the firewall policy rule to route YouTube videos over a VPN to avoid incessant and obnoxious North-American ads, but the IPs keep changing due to changing, mangled second-level domain names (SLDs). Using Python 3 and a REST API, we will monitor the appropriate DNS queries, note the IP(s) of the response, hold the response, add the IP(s) to the VPN tunnelling policy rule, then release the DNS query response.</div></div><h2>Research Python Methods to Hijack DNS Requests</h2><p>Why this approach? It’s future-proof, modular, elegant, maintainable, automated, and it lends itself to a future decision tree that could truly restrict YouTube ads outright.</p><p>First, I will enable SSHd in pfSense and take a peek around.</p><p>Let’s take this opportunity to make a disk backup.  or “duh” shows that only 800 MiB is in use on the SSD. Let’s  the whole box from our local machine in about four minutes.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><div><div> To verify the owners and permissions are set in the extended attributes locally, run<code>getfattr -d -m ^ -R -- ~/.pfsense-backup</code></div></div><p>Now that we have a pfSense backup (I’m told just backing up  works too), let’s install the <a href=\"https://github.com/jaredhendrickson13/pfsense-api\" target=\"_blank\" rel=\"nofollow\">REST API</a>.</p><p>This part had me confused. You see, I was looking at the bottom of the screen wondering how the heck I could copy a truncated hash as a token. After a few tries, I noticed the green message at the top that I had been trained to ignore. It has the token.</p><p>Next, with the API credentials set up, let’s try out the API:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><h3>Explore the Unbound Python Module</h3><p>Running  shows that the current version of Python is 3.8.</p><p>As for the Unbound DNS Resolver, I had some luck tinkering in  and writing simple Python 3.8 code to log DNS query messages. We now have both parts needed to dynamically update the firewall aliases and tunnel all YouTube traffic once and for all.</p><p>If you are looking for Python module docs for Unbound, here they are:</p><p>Run these commands to quickly get the documentation.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><div><div> The example code is from Python 2.4, so be prepared to run Black and PyCharm code formatting, or run . Also, the most important part of this whole exercise (getting the IPs from the DNS reply) is missing, so here is the hint: . Don’t forget to manually hack the byte strings to pull out the proper IP addresses in binary form, first.</div></div><p>Now we have Python docs and access to all the capabilities. Excellent.</p><p>Next, take a backup of your OS or VM and install  and  wherever, <code>./configure --with-pythonmodule</code>, , fix some errors in the Unbound code,  again, then you’ll have the generated python module () in order to remove all the missing-method red error lines in PyCharm.</p><h2>Smoke Test: A Python DNS-Hijacking Script</h2><p>Here is a smoke test of the ability to hijack  DNS requests with reply IPs that the script has caught in just a few minutes (the timestamps are just to maintain a crude LRU cache):</p><p>Duplicate IP addresses are possible, and that is fine. I let the smoke test run overnight. Here is the PoC (proof of concept) script I ran as the Unbound Python module script.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p>When I woke up, the Unbound DNS resolver service . Here are the logs:</p><div><div> Capturing all the IPs from the DNS queries to  and  puts pfSense into a crawl as all the rules need to be reloaded on each addition.</div></div><div><div> Research and install a Squid-like proxy, create a fake-but-trusted CA certificate, host it, install it in a browser as a PoC, decode TLS traffic, and victory dance.</div></div><p>Actually, it is  illegal to <a href=\"https://github.com/NSSpiral/Blackb0x\" target=\"_blank\" rel=\"nofollow\">jailbreak most Apple TV boxes</a>, so we could break in, add a root certificate valid for the pfSense box, MITM traffic from the Apple TV, and then Microsoft Bob is your uncle. That works because the pfSense box as the gateway can decrypt Apple TV traffic, inspect the request headers for the offending ad , block the request, and re-encrypt other valid requests to Mountainview, California.</p><p>But, then my iPhone would still show ads because it is harder to jailbreak, plus banking apps may detect this and not work anymore. Jailbreaking is too extreme, anyway.</p><div><div> I used a jailbroken iPhone all the time in Japan because of a quirky cellphone law. You see, because of icky perverts who like to take photos inappropriately on elevators and escalators, Japan passed a law that made the camera shutter sound mandatory on all photos.Super unfortunate was that taking a  of a web page also made the same loud, unmuteable shutter sound. Imagine you are on a train and you screenshot a Google map, it makes that loud shutter noise, and then you get dirty looks from the train riders. Yeah, I had to jailbreak and zero out the camera sound file.</div></div><p>Let’s see what it takes to spy on the HTTPS traffic from the Apple TV and iPhone to see if we can block ad URLs that way.</p><h2>Install a Fake-but-Trusted CA Cert on Apple TV and iPhone?</h2><p>Not wanting to jailbreak and add self-signed certs to Apple TV and iPhone, how hard would it be instead to add fake-but-trusted Certificate Authority (CA) certificates to each device?</p><p>The ‘A’ in CA means there is no one higher to vet such a certificate. The ‘A’ is so powerful, that back in 2001 only a Windows patch was able to revoke some <a href=\"https://en.wikipedia.org/wiki/Verisign#2001:_Code_signing_certificate_mistake\" target=\"_blank\" rel=\"nofollow\">dangerous Verisign certificates</a>. As a thought experiment, new CAs must come into existence from time to time. Let’s Encrypt is relatively new, for example. There should then be an in-warranty way to get a fake, trusted CA cert into an Apple TV and iPhone. If that is possible, then an entire world of MITM spycraft is available to decrypt TLS packets in the clear and use good ‘ol URL blocking on requests like</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Let’s see how easy this would be.</p><p>In fact, there are many, many CAs. Here is a quick  in pfSense:</p><h2>Experiment with Squid and SquidGuard</h2><p>I’m aware of <a href=\"https://mitmproxy.org/\" target=\"_blank\" rel=\"nofollow\">mitmproxy</a>, but it needs to be side-channel installed onto the pfSense router. Let’s see if the  proxy that is available as a pfSense package can do what we need. First, I will take a bare-metal backup again so I can roll back in case  is better.</p><p>I’ve installed those packages, and naturally, there are more buttons and options than in a space shuttle. I’ll find a <a href=\"https://turbofuture.com/internet/Intercepting-HTTPS-Traffic-Using-the-Squid-Proxy-in-pfSense\" target=\"_blank\" rel=\"nofollow\">guide</a>.</p><p>I’ve followed the steps in the guide, however, since I have a large SSD and generous RAM, I’ve made a dedicated folder  (and ) with 8 GiB of cache and a juicy allowance on the per-item cache size which should also help with Docker and NPM speed-up. Two birds, one stone. With Transparent HTTPS support, this should be pretty rad.</p><div><div> If web traffic slows down while using Squid, here are some System Tunables that can make Squid faster (<a href=\"https://forum.netgate.com/topic/85937/pfsense-2-2-3-internet-is-very-slow-via-squid3/12\" target=\"_blank\" rel=\"nofollow\">ref</a>):<p><code>kern.ipc.nmbclusters 32768</code></p><p>Also, for local disk cache,  is asynchronous  (great for Docker too) and uses POSIX-threads to avoid blocking the main Squid process on disk-I/O.</p></div></div><p>We can actually generate a CA cert in pfSense itself.</p><p>Now, how to get it into the Apple TV and iPhone? It should be hosted somewhere, right? How about on the router?</p><h2>Self-Host the MITM CA Certificate</h2><p>Self-hosting with a single command is ridiculously easy. From the SSH shell into pfSense, I can create a web folder and server like so:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>When I visit  I should get a blank page with “Hello”. From here, clients behind the pfSense router can temporarily access static documents.</p><p>To make like easier, here is a PHP script to cause the MITM cert to download.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>As another smoke test, I’ll add the MITM CA to Chrome (manually) and enable the SSL Filtering. The defaults are fine in Squid. Here is the log file when I visit :</p><p>However, on every other browser and machine there are HTTPS errors like so:</p><div><div> If you get locked out of pfSense with a TLS error, you may have to disable Remote Cert Checks as the pfSense web configurator uses a self-signed certificate. Or else, you can bypass the proxy for the pfSense UI under <em>Bypass Proxy for These Destination IPs</em> with <code>pfsense; pfsense.localdomain</code>.</div></div><h2>Abandoning Squid: Too Slow, Too Heavy</h2><p>After a day of painfully setting up Squid and SquidGuard and adding blacklists and even manual regex for things like , I’m having nothing but issues with Squid. Here are the top pain points:</p><ul><li>It’s slow. It’s really slow.</li><li>The ACL (Access Control List) settings are cumbersome.</li><li>There is an issue with  (<a href=\"https://forum.netgate.com/topic/141472/https-filter-with-https-http\" target=\"_blank\" rel=\"nofollow\">ref</a>).</li><li>The SquidGuard URL filter takes eons to update a list.</li><li>The Squid UI is unbelievably lacking.</li></ul><p>Squid makes me sad. I don’t get sad, but Squid makes me sad with its promise and ultimate letdown. I’ve now obliterated Squid and restored the router from the  backup I made earlier. Here is a handy little script to show a diff of what has been added by Squid and related packages.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p>The output is something like this under the  option:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><h2>Install MITMProxy in a FreeBSD Jail</h2><p>Even though written in Python, I’ll give <a href=\"https://mitmproxy.org/\" target=\"_blank\" rel=\"nofollow\">mitmproxy</a> a try next; at the very least it can be purpose-built to block YouTube ads with its rich API and Python-hook extensibility. It was a coin toss between  and  – a Metasploit hack tool – to achieve on-the-fly TLS interception, but the former can be scripted with Python and has a satisfying UI. Let’s go.</p><div><div> Please read the whole section before trying any commands because I backtracked a bit but want to explain why.</div></div><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>You’ll notice that there are only three binaries about 24 MiB each. As I understand it, they have a self-contained Python 3 environment and frozen dependencies. I’d like to jail these binaries because, well, because. First, let’s see if there is a vulnerability report for  at <a href=\"https://vuxml.freebsd.org/freebsd/index.html\" target=\"_blank\" rel=\"nofollow\">vuxml.freebsd.org</a>. Nothing. How about at <a href=\"https://www.exploit-db.com/\" target=\"_blank\" rel=\"nofollow\">Exploit-DB</a>? Nothing again. Good.</p><p>First, what version of FreeBSD is this pfSense install?</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Now, according to this <a href=\"https://blog.viktorpetersson.com/2018/01/27/jails-on-pfsense.html\" target=\"_blank\" rel=\"nofollow\">guide</a>, I’ll need to set up jails myself as they are disabled in a default pfSense installation. Not knowing FreeBSD at all before today, I had to hack around to find a URL to download the  package manually. After another bare-metal backup, here are the steps I took:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p>We need to do some hacking to get  working on pfSense’s take on FreeBSD because  is missing completely. What I’ve done is copy the  binaries  a jail (via ) back to the root system.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Let’s set up a jail for .</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p> We must enable raw sockets in this jail to allow transparent proxy mode to work. If not, MITMProxy will report errors like “Transparent mode failure: FileNotFoundError(2, ‘No such file or directory’)” or “Cannot open connection, no hostname given.” This is because raw sockets are inaccessible and server information is unavailable. We can easily edit the  config file per jail like so:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p><strong></strong> MITMProxy calls <code>sudo -n /sbin/pfctl -s state</code> but there is no  in . Run  inside the jail.</p><div><div> If you are unsuccessful when you run  inside the jail, you may get an error like this: “ssend socket: Operation not permitted”. If you are successful, then  works as it needs access to raw sockets.</div></div><p>Now we can copy over the  binaries and take them for a spin.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Things are getting tricky with this next part. Running any of the binaries above results in:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>So, there is no  folder nor any similar dynamic linker that I could find. I tried this, however:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Apparently, there is a  that can solve this for us (unavailable on pfSense), however, this is getting ridiculous! Let’s try a new tactic. Since we are in a jail, we are not bound to the crippled (read: secured) pfSense environment. Maybe we can install the  package normally in a jail?</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>And, Bingo was his name-o. After this, simply running  in the jailed console opens the MITMProxy UI. Nice. Note, this version  be one or two minor versions behind the master branch. Let’s clean up with  and do another bare-metal backup.</p><p>This is getting exciting. First, in pfSense, add a virtual IP for  attached to . Then, add a NAT rule to temporarily forward port  to  to access the proxy from the LANs.</p><p>If not in the jail console, I’ll run</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>and add the proxy setting  to my sacrificial notebook (that is auto-wiped daily). When the browser opens, we can already see colourful log entries in the MITMProxy UI.</p><p>The next step is to get the auto-generated CA PEM file used by MITMProxy (<code>~/.mitmproxy/mitmproxy-ca-cert.pem</code>). Since any CA cert here is snake oil, I’ll use the provided one. TLS traffic from my devices is safe as long as I use my own proxies.</p><p>Let’s put our experience from our previous attempt at self-hosting a CA into action. However, there is no PHP in the jail, so we can use a Python 3 web server instead.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><div><div> MITMProxy conveniently has onboarding settings to serve the same CA cert, as we did manually, just by visiting .</div></div><p>After installing the CA in the Trusted Root Store on my clean notebook (and rebooting), I am treated to this display:</p><p>Let’s see if we can get this cert on my iPhone.</p><p>This is incredibly exciting. Can we LoJack the Apple TV box next?</p><p>But wait, the router is slowing down.  is burning up the CPU… on idle.</p><p>Of course: Python is a single-threaded paradigm with the GIL (Global Interpreter Lock) ensuring threads do not actually run concurrently – unless they are blocking on I/O, which is the case here(?). Except, most of the CPU work is to generate TLS certs on the fly for each request. Yikes. Running  forgoes the UI and extreme logging. The extreme logging of all the headers and full responses heavily slows down , but  by default only logs entries like classic Apache logs – much kinder on the CPU.</p><div><div> Some advanced, high-security web servers have trouble with the MITMProxy certificates due to <a href=\"https://security.stackexchange.com/a/29990/114882\" target=\"_blank\" rel=\"nofollow\">Certificate Pinning</a> – this is a technique where the server or the client know the fingerprint of the expected certificate in advance so it cannot be forged. A workaround is to use the  option to let them bypass the proxy.</div></div><p>For my fun, I’ll go with this CLI command:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>While on YouTube, we can see the page ads clear as day with their unencrypted headers; can a simple regex now block them? They are exposed, and afraid, and their days have run out.</p><p>We can even see details about each request. For example, all the SAN info is laid out for this wide-reaching certificate. There are curiously a lot of  domains covered by this cert.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Shortly, I’ll write a Python script to block YouTube  URLs.</p><h2>Patch MITMProxy Source Code for Server SNI Interrogation</h2><p>This step may be optional for most, but as a reminder to myself, to make  work better in Transparent Proxy Mode, the SNI of the server request needs to be checked against the list of regular expressions or else only the server’s IP is used for matching in many cases. Here is a quick patch I made that can be applied directly in the jail shell (or just type a few lines manually) for  version 7.0.4:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td><div><div>Index:venv/lib/python3.8/site-packages/mitmproxy/addons/next_layer.py</div><div>Subsystem:com.intellij.openapi.diff.impl.patch.CharsetEP</div><div>===================================================================</div><div>diff--gita/usr/local/lib/python3.8/site-packages/mitmproxy/addons/next_layer.pyb/usr/local/lib/python3.8/site-packages/mitmproxy/addons/next_layer.py</div></div></td></tr></tbody></table></div></div><p>With the above patch, I can now reliably intercept a few hosts and let all others pass through.</p><h2>Smoke Test: Intercept YouTube Ads with MITMProxy</h2><p>After reading the docs and navigating the  source code in the PyCharm IDE, I’ve written a little script to block ads and tracking URLs coming from YouTube from my clean notebook. I won’t reproduce the code just yet because it didn’t succeed in blocking ads as hoped, so instead, I’ll spend the time investigating why.</p><p>Here are the smoke test filters I used where for a given top-level domain, URLs with the following partial strings are blocked:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>My initial results on blocking are positive. Everything I wanted to be blocked is faithfully blocked. Note, the  entries are due to my script, and the  failures are due to pfBlockerNG black-holing the request.</p><p>Even in the DevTools network panel, the requests are truly blocked.</p><p>Then how come I am still seeing ads? I’ve disabled HTTP/2 so that subsequent requests on the same channel don’t slide by. Mind you, sometimes the ads skip on their own, or fail to play, but they still show up. Interesting. Could YouTube be using WebSockets? I need some inspiration, so I’ll look at uBlock Origin’s regex filters for some ideas.</p><div><div> If you see the error , then the DNS blocker (i.e. pfBlockerNG) is breaking the upstream TLS handshake for a given domain. Either whitelist it in pfBlockerNG (so the request goes through), or intercept it and block the connection in . This error happens to black-holed domains when the upstream TLS cert cannot be sniffed. The cleanest strategy is to use .</div></div><h2>Examine uBlock Origin Regex Patterns for Inspiration</h2><p>Here are some of the regex/filters that uBlock Origin uses on YouTube.</p><p>At first blush, it seems that a community of like-minded individuals is playing whack-a-mole with YouTube’s HTML and JavaScript. This has got me thinking: How does a video know to play an ad with JavaScript?</p><p>How does YouTube know if the ad converts? They must target ads for individuals, so a given video must receive some unique information about an ad, such as the click link and alt text. WebSockets would be a pain to maintain, especially with all the mobile clients. They must be using stateless JSON to relay that pertinent information in an innocuous URL request that has no telltale signs of ad-ness. Let’s hunt for this info in the JSON replies captured by .</p><p>Snap, Crackle, and Pop. We have a new plan: surgically alter the JSON response body to eliminate or Byzantine-up the ad information.</p><h2>Surgically Alter the JSON Response to Remove Ads</h2><p>After a bit more playful exploration, a trove of blocklorne URLs is right there in the JSON payload. In fact, most of what I am trying to block shows up right here:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p>However, YouTube has bobby-trapped their UI and there is more than one way their obfuscated JavaScript code can pull down the ad details.</p><p><strong>Let’s blow it all away right now.</strong></p><p>After a lot of fun taking apart the YouTube UI and HTTP workflow, taking into account cookies and naughty service workers, I am successfully able to strip away all the pre-roll, post-roll, mid-video, and, well, all the video ads. Here is a screenshot from  showing how select REST queries are intercepted, decrypted, modified, put back into the response, and the headers updated (content length, etc.).</p><p>With this new ability, we could even inject JavaScript into the main YouTube web page and subvert their JavaScript in a sort of ECMAScript arms race, possibly even leveraging some of the filters from uBlock Origin. However, we can hang our hats on this accomplishment for today.</p><div><div> We can strip out ads from the JSON payload for YouTube web ads using a router.</div></div><h2>The iOS YouTube App Uses Protobuf, not JSON</h2><p>I can see very similar data in the Protocol Buffer (Protobuf) version of the same API calls as the web version to that of the YouTube iOS app. That complicates things, somewhat: We cannot lean on JSONPath to hunt down advertisement sections of JSON because with Protobuf the keys are just numbers that can even change.</p><div><div> YouTube compiles a large list of all the ads you are going to see and sends that to you in a sneaky payload. In fact, it is easier to visualize this when reading Protobuf. If you manage to exhaust that list, then another large list will be coming your way.</div></div><p>I can see strings like “Telus” and “Samsung TV” and “Boxing Week” and “Buy now”. Remember when YouTube was a fun place? A fable about a Golden Goose comes to mind, Alphabet.</p><p>As a consequence of being able to see unencrypted traffic from my iPhone, I’m taken aback by the sheer amount of tracking information laid bare; It’s like I have electrodes on my head and chest while I’m running on a treadmill and a bunch of scientists in white lab coats with clipboards are standing shoulder-to-shoulder recording everything about my internals.</p><div><div> Your apps are tracking you like crazy: what you do, how long you dwell, when you leave a given app, and so much more. The URL <code>https://play.googleapis.com/log/batch</code> shows up a lot in my logs.</div></div><p>The next question is: Does the iOS app protocol behave like the web app?</p><h2>Timing Analysis to Detect Ad Videos?</h2><p>The iOS network traffic is not like the web traffic; Google has teams and teams of engineers dedicated to making sure blocking their ads isn’t computationally feasible. Daunted but undeterred, I was staring at network requests to let my mind zone out and wander when I noticed a pattern I had not noticed before.</p><p>For the  version of YouTube, I can eyeball which URLs are ads and which are the videos I want to watch. Take a look:</p><p>How am I able to eyeball which video URLs are ads in this chaos?</p><p>Take a look at the query parameter . For the web version, a chunk of the video I want is fetched from the 0th byte, then immediately another video is fetched with a  starting again at the 0th byte. Both happen near-simultaneously – faster than a human can click on a new video. It turns out this, as well as examining the  parameter for the length of the full video (short videos are likely ads), can reasonably allow us to detect and doctor ad videos.</p><p>However, the iOS YouTube protocol does  use the  query parameter or even the  header; video chunks use a counter like  and  etc. We must reverse engineer the Protobuf responses.</p><h2>Decode the YouTube Protobuf Responses</h2><p>Here are some decoded Protobuf log files I created then opened in the PyCharm IDE.</p><p>After logging decoded Protobuf messages to disk for offline analysis, I did notice something that piqued my interest.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td><div><div>entitlement</div><div>entitlement</div></div></td></tr></tbody></table></div></div><p>I wonder what would happen if I were to, say, toggle those? This is tantalizing, but it is cheating, and hence no fun. Back to heuristics.</p><div><div> As with JSON, can I blow away the Protobuf sections that serve up ads? Could I instead detect the ad videos in the payload, then dynamically modify their responses to be, say, a cached 0.01s video file? The 30s ~ 300s of unskippable ads could be over in the blink of an eye without blocking all those URLs.</div></div><p>Let’s start by blocking the ads as intended.</p><p>The Protobuf responses are a hot mess of bytes, but there are human-readable URLs that can be grepped.</p><p>You’d think a simple LRU cache that blocks soon-encountered ad URLs could be the way to go, but, alas, the ad URLs do not quite match the URLs sent over the wire. Also, who is to say that YouTube won’t randomize the position of query-string parameters one day? We need an  lookup of flagged ad URLs that are polymorphic (and group homomorphic) to live ad URLs.</p><p>It might be tempting to split a query string into a sorted dictionary and reassemble it, but we have no way of knowing what the query string boundary is. Plus, a live ad URL could add a key and disrupt the sorting.</p><p>Addionally, I’ve encountered URLs like this that purposely try to obfuscate the query params:</p><blockquote><p>https://r4—sn-vgqsrns6.googlevideo.com/videoplayback/expire/1640607416<p>/ei/WFrJYdWnFfyTsfIP4s2BsAk</p>/id/o-AE7swWOPOwXu3GyRght/requiressl/yesmm/31,26/…</p></blockquote><p>Notice how  is just ?</p><p>I propose heuristically scanning for query and path parameters of ad URLs with high entropy and using those as keys (fingerprints). For example, in</p><blockquote><p>https://.googlevideo.com/initplayback?source=youtube&amp;orc=1&amp;oeis=1&amp;c=IOS&amp;oss=1&amp;oda=1&amp;oad=5500&amp;ovd=5500&amp;oaad=11000&amp;oavd=11000<p>&amp;ocs=700&amp;oputc=1&amp;oses=1&amp;ofpcc=1&amp;osbr=1&amp;osnz=1&amp;msp=1&amp;odeak=1&amp;odepv=1</p>&amp;osfc=1&amp;id=&amp;ip=&amp;initcwndbps=&amp;mt=</p></blockquote><p>One could note the following candidates in descending order of length:</p><ul></ul><p>Any or all of them could be lookup keys each pointing to the same dictionary of deconstructed query parameters. A lookup of a live URL would involve the same process of finding the highest entropy parameters and checking the URL dictionary for a match. The cache data structure can even be multi-level with the root keys being just the length of the high-entropy strings.</p><div><div> Even with the ability to block polymorphic URLs, the video ads are still indistinguishable from content video without context from the Protobuf structure.</div></div><h2>Smoke Test: Intercept and Decode Protobuf in Python</h2><div><div> Decoding ~500 kiB of raw Protobuf in pure Python is painfully slow.</div></div><p>Decoding ~500 kiB of Protobuf in pure Python, especially the decoding step of converting it to over 1 MiB of human-readable text to parse the ad URLs, takes more time than the connection timeout most of the time. I’ll run some benchmarks using pure Python vs. the native C++ library.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>If you caught that, it takes about  in Python, and  in C++! In this Never Ending Story, we have to find a way to parse the raw Protobuf payloads in Python using the C++ library . In the interest of time, I’ll use  and communicate with the C++  binary directly (since raw decoding is not supported in Python anyway).</p><h2>Fuzzing the YouTube Video Ad Responses</h2><p>How about fuzzing the ad video responses? Now being able to isolate ad videos, as a smoke test, I sent back  responses with empty bodies and the iOS app went bananas; it was as if there is an infinite loop with no delay just hammering YouTube’s own servers trying to get the next part of the video in panic mode. I felt bad for their servers, so I stopped. Then, what would a happy-path response payload look like?</p><p>Try as I might, when I send back empty s, s, s, truncate response bodies, or just null-out part of the ad video, the iOS app crawls then crashes spectacularly with a dying breath of a messed up iOS UI. I now block some error reporting endpoint at  that indicates a “dev assertion failed” so I don’t make some overworked QA pull out their hair.</p><div><div> We’ve learned that blocking ad URLs causes the app to deploy countermeasures and even when defeated, the app hangs forever on the ad screen. We’ve also learned that fuzzing ad videos often causes the app to crash – there is even session meta data in the video response chunks.</div></div><p>Let’s go back to what worked with JSON and obliterate the section of the Protobuf responses that contain the array of ad details.</p><h2>Enter Burp Suite Tools for Penetration Testing</h2><p>There is a library for <a href=\"https://portswigger.net/burp\" target=\"_blank\" rel=\"nofollow\">Burp Suite</a> called  (get the <a href=\"https://github.com/nccgroup/blackboxprotobuf/tree/master/lib\" target=\"_blank\" rel=\"nofollow\">original Burp Suite version</a>,  the PyPi fork, unless you like infinite recursion bugs) that is designed to decode raw Protobuf wire messages, inject something naughty, then re-encode them again to see how a Protobuf endpoint behaves. We are going to have so much fun together in this next section.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>You may encounter a small world of pain because some forks of blackboxprotobuf will cause a stack overflow due to deep recursion. You can see this by adding <code>sys.setrecursionlimit(200)</code>.</p><p>Compiling the original library source code for Burp Suite and using the C++ bindings will allow us to transcode ~500 kiB of raw Protobuf bytes in just a few seconds.</p><div><div> At the top of your import chain before you import , add<div data-nosnippet=\"data-nosnippet\" data-settings=\" no-popup minimize scroll-mouseover\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>to use the C++  implementation whenever possible.</p></div></div><p>It is now possible to generate a best-guess  schema with a single function:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>The schema isn’t perfect, and it is huge and deeply nested, and takes forever to pretty-print, and is probably wrong, but is just good enough to pull out the ad details like so (Protobuf to JSON in this sample):</p><p>The Python schema is huge and looks like this for about 250,000 more charcters:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Reverse engineering the Protobuf schema sounds good on paper, but our target is spectacularly complex and a moving target.</p><h2>Exfil the Proto Schemas from the App, Cleanly?</h2><p>As fun as it to reverse the Protobuf and generate a best-guess schema, wouldn’t it be more ninja-like to exfil the actual, working  or schema files from the smartphone app? Let’s pull out the Protobuf schemas from the Android version of the YouTube app and see if the schemas are the same or compatible.</p><p>This is what I tried , but it went nowhere with the <a href=\"https://github.com/marin-m/pbtk\" target=\"_blank\" rel=\"nofollow\">Protobuf Toolkit</a> (PBTK). I reproduce it here so I remember what I tried:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>After installing Qt dependencies (pronounced “cute”), I was treated to a GUI.</p><p>Next, I got the most recent release of a 100 MiB Android APK file from <a href=\"https://apkpure.com\" target=\"_blank\" rel=\"nofollow\">apkpure.com</a>.</p><p>Excited in vain, the most PBTK could get was a 59-byte proto file. Another tool called <a href=\"https://ibotpeaches.github.io/Apktool/install/\" target=\"_blank\" rel=\"nofollow\">Apktool</a> also looked promising, but the best it can do is  bytecode, not decompile it – this may be good enough for Pen Testers, however.</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>You can see that Google went out of its way to complicate reverse engineering.</p><p>Google thoughtfully did leave some hints.</p><p>Upon deeper inspection, the Protobuf classes are right here, in Java, decorated with getters and setters. Since we are using Python, and we cannot get the true schema files, I will leave this approach for now.</p><h2>Hardcore Deep-Dive into Protobuf and Wire Format</h2><p>After gazing into a sea of decrypted network traffic again, then triggering errors and assertion fails on my iPhone with Protobuf fuzzing, and taking a peek at the error logs being phoned home, I’ve noticed that ads register for “slots” in a given video. They can register for pre-roll, mid-roll, end-roll, full-page, and ad pods (back-to-back ads). Blocking an ad URL causes an error along the lines of “some ad that doesn’t exist booked a slot” and UI panic sets in.</p><p>I’m back. The Wire Format is surprisingly elegant, except for <a href=\"https://developers.google.com/protocol-buffers/docs/encoding#signed_integers\" target=\"_blank\" rel=\"nofollow\">ZigZag encoding</a>. Through trial and error, editing out chunks of Protobuf with a hex editor is just a no-go.</p><p>While computationally expensive, decoding, editing, and re-encoding without the original schema leads to a modified encoding. This is likely because we cannot detect if ZigZag encoding is being used, or if a number is an , , , , etc., plus the order of object fields is normally non-deterministic. Here is some <a href=\"https://developers.google.com/protocol-buffers/docs/encoding#implications\" target=\"_blank\" rel=\"nofollow\">Protobuf trivia</a> on the matter:</p><h2>Exploit a Protobuf Flaw to Easily Remove All Ads by Changing One Byte</h2><p>Casually poring over the C++ source code, an interesting comment in the Protobuf code caught my eye:</p><blockquote><p> is used to keep track of fields that were seen when parsing a protocol message but whose field numbers or types are unrecognized. This most frequently occurs when new fields are added to a message type and then messages containing those fields are read by old software that was compiled before the new types were added. (<a href=\"https://developers.google.com/protocol-buffers/docs/proto3#unknowns\" target=\"_blank\" rel=\"nofollow\">ref</a>)</p></blockquote><p>Yes, what to do with unknown fields? What to do indeed. And, how easy would it be to say, change a  field key to, say,  thus making an entire substructure of advertisement and tracking information suddenly unavailable? Tantalizing.</p><p>And, if we can calculate the field tags in bytes with bit-twiddling, then can we use a simple regex to AMF the section of ads in  time?</p><p>As a motivating example, I’d like to find the field key  which is not as simple as searching for . Here is an implementation of a tag-scanning algorithm so you can see the bit-twiddling:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>We know the wire type is  (length-delimited nested string/message), and one target field  is . When bit-twiddled, we get the target </p><p>where the final  happens to mean  (the wire type) in hex. In binary, this is:</p><p><code>10101010 11111111 10111000 10111100 00000001</code></p><p>Let’s lose the MSB from each byte as per the var-length wire format:</p><p><code>.0101010 .1111111 .0111000 .0111100 .0000001</code></p><p>Then we shift and add only the first four bytes since the LSB is first:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Finally, we shift out the number of wire type bits (3) to get back the field key:</p><p><code>395198378 &gt;&gt; 3 = 49399797</code></p><p>And that, folks, is a taste of how Wire Format works.</p><p>Fantastic. Now, all we have to do is scan the Protobuf bytes for classic ad URL signatures like  to bound our field search, then move backward from there until we find the target(s) field tags and thus field keys we would like to denature (e.g.  –&gt; ).</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td></td></tr></tbody></table></div></div><p>Notice how the Protobuf response payload is ? As I said, Google makes it computationally expensive to decode, alter, and re-encode without the C++ source proto files, but a quick linear scan takes no effort at all.</p><p>Just a quick note, there is more than one field tag, but not all of them represent ads. That is why we need to backtrack from the  markers.</p><h2>Smoke Test: Remove Ads from Protobuf in O(n)-Time</h2><p> In one pass with no additional memory, I’m able to scan a huge 1.8 MiB chunk of jibberish-looking Protobuf data, and in the screenshot below only at the 30,593 byte (of 1.8 MiB) is our target found, and then backtracking ~600 characters yields our target field key to denature. Not only is this amazing, but I don’t even need to block  or URLs with  in them; Those requests are never made in the first place, anymore.</p><h2>Analysis of this Successful Adblocking Technique</h2><p>By taking advantage of a feature (flaw?) in Protobuf that allows it to be backward compatible with schema changes, along with the fact that Protobuf is very sensitive to byte changes due to its compact nature, we can change a single byte in a critical location and tell Protobuf that an entire section of deeply-nested data is from a future schema version and it should be ignored.</p><p>Google returns huge responses in Protobuf (e.g. 1.8 MiB) – including even the layout of the iOS app – so only C++/Swift is fast enough to understand it all before the connection times out. I’ve shown that Python is several orders of magnitude too slow in decoding these Protobuf payloads, so connections do time out waiting on Python. With web-based JSON, the whole payload needs to be parsed, edited, and re-serialized; With my Protobuf technique, it takes microseconds thanks to a single linear scan and then ultra-quick backtracking. This technique is suitable for real-time adblocking without blocklists.</p><p>All those  and  URLs on Apple devices originate from the Protobuf payload. This means they all go away for free – we don’t need to block them. In fact, the YouTube app is zippier because fewer connections are made to ad URLs in the first place. This means we can avoid keeping a blocklist of YouTube ad URLs and stay on the sidelines of the whack-a-mole fun. Ads do not register for video location “slots” on the Apple devices and the content just plays.</p><p>This is a heuristic technique that looks for two strings:  and some calculated field tag nearby, so this technique is designed to be future-proof.</p><p>Even if Google changes the field tag (and breaks millions of apps and Apple TVs before they upgrade), it’s an academic exercise to enhance the following script to discover the new field tag(s) automatically.</p><h3>Should Google be Worried?</h3><p>This is a <strong>highly-specialized technique</strong> to block Apple-device YouTube ads (or Instagram, Whatsapp, Facebook, etc. tracker blocking). The CPU requirements to decrypt and re-encrypt HTTPS traffic greatly exceed those available to Raspberry Pis. Even if some company takes my script and considers making and selling a NIC dongle, it would likely not be powerful enough. An <a href=\"https://www.nvidia.com/en-us/shield/shield-tv/\" target=\"_blank\" rel=\"nofollow\">Nvidia Shield</a> could handle it, but if you already have Android devices, then just hack the binaries; My technique is for Apple device owners where we don’t want to compromise the OS so that further reduces the audience of this technique.</p><h2>The MITMProxy YouTube Adblocking Script</h2><p>Here is the MITMProxy addon script that serves as a proof-of-concept to block YouTube ads on networked Apple devices. The script can be run as follows (note the prerequisites in the script and be sure to install them first). Name it  and run the following command:</p><p><code>mitmdump --listen-port 8080 --listen-host 127.0.0.1 -s \"youtube.py\"</code></p><p>Here is the script, including a fairness function to allow ads 5% of the time:</p><div data-nosnippet=\"data-nosnippet\" data-settings=\" minimize scroll-always\"><div><table><tbody><tr><td data-settings=\"show\"></td><td></td></tr></tbody></table></div></div><p>This script happens to work in Python for a TLS-decrypting man-in-the-middle proxy written in Python. As a working proof-of-concept, it’s pretty rad. Of course, it can be rewritten in Rust or Go or anything but single-threaded Python, but as an intellectual exercise to defeat ads that are served from the same domain as content, it’s elegant.</p><p>It’s unknown if CAD  $11.99/mo ($13.43/mo with tax) is even reasonable: Do I personally incur CAD $11.99 of cost to advertisers each month?</p><p>Since ads are auctioned, the CPV (cost-per-view) varies. Also, many ad campaigns have a capped daily budget, so theoretically there should be fewer ads in the evenings as budgets run out during the day.</p><p>I watched YouTube on and off for a day on a clean notebook computer with private browsing. My history showed that I only “watched” 10 videos:</p><ul><li>I fast-forwarded through a few of them to get past the “like and subscribe” runtime padding.</li><li>I jumped to the end of one just to get to the “top three” from a “top twenty” list.</li><li>Two were low quality so I left early.</li><li>The rest were music videos.</li></ul><p>In all, for watching parts of 10 videos, I was exposed to 8 ads, and only two were skippable (which I skipped).</p><p>Let’s use USD $0.15 as a CPV. In one day, let’s say, I incurred 8 x $0.15, or $1.20 to advertisers. Extrapolated to one month, that is roughly USD $36/mo. Do I really cost advertisers USD $36/mo for very casual YouTube viewing? That sounds terrible for advertisers.</p><h3>CPV from US Advertising Spend Divided by Total Views</h3><p>From <a href=\"https://www.statista.com/statistics/289658/youtube-global-net-advertising-revenues/\" target=\"_blank\" rel=\"nofollow\">Statistica</a>, in 2019, US YouTube advertisers spent $15.1 billion dollars. Also in 2019, US residents had 916 billion views (<a href=\"https://ericdraken.com/pfsense-decrypt-ad-traffic/#ad-spend\">ref</a>). That works out to an average of $15.1B / 916B, or USD $0.0165 per view. Then for me, that is only USD 13 cents. Extrapolated to one month, I theoreticaly cost advertisers only USD $3.96/mo.</p><h3>Is YouTube Premium Worth It?</h3><p>When I allowed ads for my experiment, I hit the hardware mute button. I also looked away because I have several computers with a lot going on. Ad spend is wasted on me, but I still want to support content creators. For me, CAD $13.48/mo is more than I incur on actual ads and more than I pay for a Netflix subscription. The only way to justify the cost is to have YouTube playing constantly in the background on a TV.</p><p>However, I truly enjoy a handful of creators, so I may start watching them in the background on non-stop play. Let’s give the three-month YouTube Premium trial a chance, and I will still be monitoring what they track about me.</p><p>Recently I learned that due to abuses of the <a href=\"https://en.wikipedia.org/wiki/Digital_Millennium_Copyright_Act\" target=\"_blank\" rel=\"nofollow\">DMCA Act of 1998</a>, YouTube content creators who make reaction videos and “easter egg” videos may have their videos claimed by big companies like Sony and Viacom. That means that from when a claim is made, all ad revenue goes to those big companies, and not even to the creators. That means in all likelihood I unknowingly may not even be supporting my favourite YouTube creators.</p><div><div> Many fair-use and video-game-commentary videos may have automated copyright claims against them, meaning that ad revenue goes to big companies with deep legal pockets and your favourite creators may get nothing, so more and more creators leave YouTube for Twitch.</div></div><h2>Summary of Accomplishments</h2><p>I rarely give up, so this is an example of going into an extreme problem-solving mode to solve a fun problem loosely using cryptography and reverse engineering. In the end, a single byte turned it all around, so it was all worth it to come to an elegant and satisfying solution.</p><div><div> We were able to set up a hardware router from scratch, segment LANs into trusted and untrusted zones, set up traditional DNS adblocking, add a transparent MITM proxy, and ultimately block YouTube ads on networked Apple devices.<p>Note: This was a hard problem – now solved – so I am paying for YouTube Premium to give the CPU a rest.</p></div></div>","contentLength":73881,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43396735"},{"title":"GIMP 3.0","url":"https://testing.gimp.org/news/2025/03/16/gimp-3-0-released/","date":1742253953,"author":"wicket","guid":180,"unread":true,"content":"<p>At long last, the <strong>first release of  3.0 is here</strong>! This is the end result of\nseven years of hard work by volunteer developers, designers, artists, and community members (for reference,  2.10 was first <a href=\"https://www.gimp.org/news/2018/04/27/gimp-2-10-0-released/\">published in 2018</a>\nand the initial development version of  3.0 was <a href=\"https://www.gimp.org/news/2020/11/06/gimp-2-99-2-released/\">released in 2020</a>). With  3.0 you can do more than ever before, more easily, more&nbsp;quickly!</p><p>While we can’t cover every single change in  from 2.10, we want to highlight some of the biggest ones as you start exploring this new&nbsp;release.</p><ul><li><p>Need to tweak a filter you applied hours ago?\nNew in  3.0 is non-destructive editing for\nmost commonly-used filters. See the changes in\nreal time with on-canvas&nbsp;preview.</p></li><li><p>Exchange files with more applications, including\n files as well as better  export and\nmany new&nbsp;formats.</p></li><li><p>Don’t know how big to make your drawing?\nSimply set your paint tool to expand layers automatically as&nbsp;needed.</p></li><li><p>Making pro-quality text got easier, too. Style your text,\napply outlines, shadows, bevels, and more, and you can\nstill edit your text, change font and size,\nand even tweak the style&nbsp;settings.</p></li><li><p>Organizing your layers has become much easier with the ability to\n  select multiple items at once, move them or transform them all&nbsp;together!</p></li><li><p>Color Management was again improved, as our long-term project to make\n   an advanced image editor for all&nbsp;usages.</p></li><li><p>Updated graphical toolkit () for modern desktop&nbsp;usage.</p></li></ul><p>We’ve prepared <a href=\"https://testing.gimp.org/release-notes/gimp-3.0.html\">release notes</a> to go over all the changes, improvements, new features, and more. And if you’d like even more details, you can peruse the <a href=\"https://gitlab.gnome.org/GNOME/gimp/-/blob/master/NEWS\"></a> changelog for all 2.99 and 3.0 &nbsp;releases.</p><p>But to see it for yourself, you can get  3.0 directly from our <a href=\"https://www.gimp.org/downloads/\">Downloads page</a> and try it&nbsp;out!</p><h2>Other Releases in GIMPVerse</h2><p>To accompany our release of  3.0.0, packagers should also be aware\nthat we&nbsp;released:</p><p> 3.0 is a new milestone. The application is in active\ndevelopment and if you think this is awesome, wait until\nyou see our plans for the&nbsp;future!</p>","contentLength":1937,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43393822"},{"title":"The Alexa feature \"do not send voice recordings\" you enabled no longer available","url":"https://discuss.systems/@dev/114161826926246661","date":1742186509,"author":"luu","guid":179,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43385268"},{"title":"Launching RDAP; sunsetting WHOIS","url":"https://www.icann.org/en/announcements/details/icann-update-launching-rdap-sunsetting-whois-27-01-2025-en","date":1742172528,"author":"radeeyate","guid":178,"unread":true,"content":"<p>As of 28 January 2025, the Registration Data Access Protocol (RDAP) will be the definitive source for delivering generic top-level domain name (gTLD) registration information in place of sunsetted WHOIS services. RDAP offers several advantages over WHOIS including support for internationalization, secure access to data, authoritative service discovery, and the ability to provide differentiated access to registration data. RDAP was developed by the Internet Engineering Task Force.</p><p>RDAP has been offered by ICANN-accredited registrars and gTLDs since 2019.</p><p>To request access to nonpublic gTLD registration data, use the Registration Data Request Service (<a href=\"https://rdrs.icann.org/\">RDRS</a>) for participating registrars or contact the sponsoring registrar directly to determine their disclosure process. Please make sure you have first checked that the data is unavailable through the ICANN Lookup tool. The RDRS is intended for use by those with a legitimate interest in nonpublic data like law enforcement, intellectual property professionals, consumer protection advocates, cybersecurity professionals, and government officials.</p>","contentLength":1102,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43384069"},{"title":"Docs – Open source alternative to Notion or Outline","url":"https://github.com/suitenumerique/docs","date":1742125132,"author":"maelito","guid":177,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43378239"},{"title":"OpenAI asks White House for relief from state AI rules","url":"https://finance.yahoo.com/news/openai-asks-white-house-relief-100000706.html","date":1741868429,"author":"jonbaer","guid":176,"unread":true,"content":"<div><p>(Bloomberg) -- OpenAI has asked the Trump administration to help shield artificial intelligence companies from a growing number of proposed state regulations if they voluntarily share their models with the federal government.</p><p>In a 15-page set of policy suggestions released on Thursday, the ChatGPT maker argued that the hundreds of AI-related bills currently pending across the US risk undercutting America’s technological progress at a time when it faces renewed competition from China. OpenAI said the administration should consider providing some relief for AI companies big and small from state rules – if and when enacted – in exchange for voluntary access to models.</p><p>The recommendation was one of several included in OpenAI’s response to a request for public input issued by the White House Office of Science and Technology Policy in February as the administration drafts a new policy to ensure US dominance in AI. President Donald Trump previously rescinded the Biden administration’s sprawling executive order on AI and tasked the science office with developing an AI Action Plan by July.</p><p>To date, there has been a notable absence of federal legislation governing the AI sector. The Trump administration has generally signaled its intention to take a hands-off approach to regulating the technology. But many states are actively weighing new measures on everything from deepfakes to bias in AI systems.</p><p>Chris Lehane, OpenAI’s vice president of global affairs, said in an interview that the US AI Safety Institute – a key government group focused on AI – could act as the main point of contact between the federal government and the private sector. If companies work with the group voluntarily to review models, the government could provide them “with liability protections including preemption from state based regulations that focus on frontier model security,” according to the proposal.</p><p>“Part of the incentive for doing that ought to be that you don’t have to go through the state stuff, which is not going to be anywhere near as good as what the federal level would be,” Lehane said.</p><p>In its policy recommendations, OpenAI also reiterated its call for the government to take steps to support AI infrastructure investments and called for copyright reform, arguing that America’s fair use doctrine is critical to maintaining AI leadership. OpenAI and other AI developers have faced numerous copyright lawsuits over the data used to build their models.</p></div>","contentLength":2483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43352531"},{"title":"Mark Klein, AT&T whistleblower who revealed NSA mass spying, has died","url":"https://www.eff.org/deeplinks/2025/03/memoriam-mark-klein-att-whistleblower-about-nsa-mass-spying","date":1741813508,"author":"leotravis10","guid":175,"unread":true,"content":"<p>EFF is deeply saddened to learn of the passing of Mark Klein, a bona fide hero who risked civil liability and criminal prosecution to help expose a massive spying program that violated the rights of millions of Americans.</p><p>Mark didn’t set out to change the world. For 22 years, he was a telecommunications technician for AT&amp;T, most of that in San Francisco. But he always had a strong sense of right and wrong and a commitment to privacy.</p><p>When the New York Times reported in late 2005 that the NSA was engaging in spying inside the U.S., Mark realized that he had witnessed how it was happening. He also realized that the President was not telling Americans the truth about the program. And, though newly retired, he knew that he had to do something. He showed up at EFF’s front door in early 2006 with a simple question: “Do you folks care about privacy?”&nbsp;</p><p>We did. And what Mark told us changed everything. Through his work, Mark had learned that the National Security Agency (NSA) had installed a secret, secure room at AT&amp;T’s central office in San Francisco, called <a href=\"https://en.wikipedia.org/wiki/Room_641A\">Room 641A</a>. Mark was assigned to connect circuits carrying Internet data to optical “splitters” that sat just outside of the secret NSA room but were hardwired into it. Those splitters—as well as similar ones in cities around the U.S.—<a href=\"https://www.eff.org/files/2014/07/24/backbone-3c-color.jpg\">made a copy</a> of all data going through those circuits and delivered it into the secret room.</p><div><div><div><img src=\"https://www.eff.org/files/2025/03/12/secretroom1_f.jpg\" width=\"500\" height=\"373\" alt=\"Photo of NSA-controlled &quot;secret room&quot; \" title=\"Photo of NSA-controlled &quot;secret room&quot; \"><p>A photo of the NSA-controlled 'secret room' in the AT&amp;T facility in San Francisco (Credit: Mark Klein)</p></div></div></div><p>Mark not only saw how it works, he had the documents to prove it. He brought us over a hundred pages of authenticated AT&amp;T schematic diagrams and tables. Mark also shared this information with major media outlets, numerous Congressional staffers, and at least two senators personally. One, Senator <a href=\"https://www.youtube.com/watch?v=Pylu8ycLfC4\">Chris Dodd</a>, took the floor of the Senate to acknowledge Mark as the great American hero he was.</p><div><div><div><img src=\"https://www.eff.org/files/2025/03/12/mark-klein-washington.jpg\" width=\"1284\" height=\"633\" alt=\"cartoon featuring Mark Klein climbing Capitol Hill\" title=\"cartoon featuring Mark Klein climbing Capitol Hill\"><p>Archival EFF graphic promoting Mark Klein's DC tour</p></div></div></div><p>Mark stood up and told the truth at great personal risk to himself and his family. AT&amp;T threatened to sue him, although it wisely decided not to do so. While we were able to use his evidence to make some change, both EFF and Mark were ultimately let down by Congress and the Courts, which have refused to take the steps necessary to end the mass spying even after Edward Snowden provided even more evidence of it in 2013.&nbsp;</p><p>But Mark certainly inspired all of us at EFF, and he helped inspire and inform hundreds of thousands of ordinary Americans to demand an end to illegal mass surveillance. While we have not yet seen the success in ending the spying that we all have hoped for, his bravery helped to usher numerous reforms so far.</p><p>And the fight is not over. The law, <a href=\"https://www.eff.org/702-spying\">called Section 702</a>, that now authorizes the continued surveillance that Mark first revealed, expires in early 2026. EFF and others will continue to push for continued reforms and, ultimately, for the illegal spying to end entirely.</p><p>Mark’s legacy lives on in our continuing fights to reform surveillance and honor the Fourth Amendment’s promise of protecting personal privacy. We are forever grateful to him for having the courage to stand up and will do our best to honor that legacy by continuing the fight.&nbsp;</p>","contentLength":3237,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43347662"},{"title":"Gemini Robotics","url":"https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/","date":1741792149,"author":"meetpateltech","guid":174,"unread":true,"content":"<div><p data-block-key=\"q2c97\">Introducing Gemini Robotics, our Gemini 2.0-based model designed for robotics</p><p data-block-key=\"3djrg\">At Google DeepMind, we've been making progress in how our Gemini models solve complex problems through multimodal reasoning across text, images, audio and video. So far however, those abilities have been largely confined to the digital realm. In order for AI to be useful and helpful to people in the physical realm, they have to demonstrate “embodied” reasoning — the humanlike ability to comprehend and react to the world around us— as well as safely take action to get things done.</p><p data-block-key=\"73qu7\">Today, we are introducing two new AI models, based on Gemini 2.0, which lay the foundation for a new generation of helpful robots.</p><p data-block-key=\"4he0s\">The first is Gemini Robotics, an advanced vision-language-action (VLA) model that was built on Gemini 2.0 with the addition of physical actions as a new output modality for the purpose of directly controlling robots. The second is Gemini Robotics-ER, a Gemini model with advanced spatial understanding, enabling roboticists to run their own programs using Gemini’s embodied reasoning (ER) abilities.</p><p data-block-key=\"dtsn7\">Both of these models enable a variety of robots to perform a wider range of real-world tasks than ever before. As part of our efforts, we’re partnering with Apptronik to build the next generation of humanoid robots with Gemini 2.0. We’re also working with a selected number of trusted testers to guide the future of Gemini Robotics-ER.</p><p data-block-key=\"im3d\">We look forward to exploring our models’ capabilities and continuing to develop them on the path to real-world applications.</p></div><div><h2 data-block-key=\"2q7pg\">Gemini Robotics: Our most advanced vision-language-action model</h2><p data-block-key=\"3d8qf\">To be useful and helpful to people, AI models for robotics need three principal qualities: they have to be general, meaning they’re able to adapt to different situations; they have to be interactive, meaning they can understand and respond quickly to instructions or changes in their environment; and they have to be dexterous, meaning they can do the kinds of things people generally can do with their hands and fingers, like carefully manipulate objects.</p><p data-block-key=\"5bthq\">While our previous work demonstrated progress in these areas, Gemini Robotics represents a substantial step in performance on all three axes, getting us closer to truly general purpose robots.</p><p data-block-key=\"at04h\">Gemini Robotics leverages Gemini's world understanding to generalize to novel situations and solve a wide variety of tasks out of the box, including tasks it has never seen before in training. Gemini Robotics is also adept at dealing with new objects, diverse instructions, and new environments. In <a href=\"https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf\" rel=\"noopener\" target=\"_blank\">our tech report</a>, we show that on average, Gemini Robotics more than doubles performance on a comprehensive generalization benchmark compared to other state-of-the-art vision-language-action models.</p></div><div><p data-block-key=\"5merb\">To operate in our dynamic, physical world, robots must be able to seamlessly interact with people and their surrounding environment, and adapt to changes on the fly.</p><p data-block-key=\"lvkr\">Because it’s built on a foundation of Gemini 2.0, Gemini Robotics is intuitively interactive. It taps into Gemini’s advanced language understanding capabilities and can understand and respond to commands phrased in everyday, conversational language and in different languages.</p><p data-block-key=\"eo201\">It can understand and respond to a much broader set of natural language instructions than our previous models, adapting its behavior to your input. It also continuously monitors its surroundings, detects changes to its environment or instructions, and adjusts its actions accordingly. This kind of control, or “steerability,” can better help people collaborate with robot assistants in a range of settings, from home to the workplace.</p></div><div><p data-block-key=\"ev4ji\">The third key pillar for building a helpful robot is acting with <a href=\"https://deepmind.google/discover/blog/advances-in-robot-dexterity/\" rel=\"noopener\" target=\"_blank\">dexterity</a>. Many everyday tasks that humans perform effortlessly require surprisingly fine motor skills and are still too difficult for robots. By contrast, Gemini Robotics can tackle extremely complex, multi-step tasks that require precise manipulation such as origami folding or packing a snack into a Ziploc bag.</p></div><div><p data-block-key=\"2co1s\">Finally, because robots come in all shapes and sizes, Gemini Robotics was also designed to easily adapt to different robot types. We trained the model primarily on data from the bi-arm robotic platform, <a href=\"https://aloha-2.github.io/\" rel=\"noopener\" target=\"_blank\">ALOHA 2</a>, but we also demonstrated that it could control a bi-arm platform, based on the Franka arms used in many academic labs. Gemini Robotics can even be specialized for more complex embodiments, such as the humanoid Apollo robot developed by Apptronik, with the goal of completing real world tasks.</p></div><div><h2 data-block-key=\"2q7pg\">Enhancing Gemini’s world understanding</h2><p data-block-key=\"f1glf\">Alongside Gemini Robotics, we’re introducing an advanced vision-language model called Gemini Robotics-ER (short for ‘“embodied reasoning”). This model enhances Gemini’s understanding of the world in ways necessary for robotics, focusing especially on spatial reasoning, and allows roboticists to connect it with their existing low level controllers.</p><p data-block-key=\"6k4gb\">Gemini Robotics-ER improves Gemini 2.0’s existing abilities like pointing and 3D detection by a large margin. Combining spatial reasoning and Gemini’s coding abilities, Gemini Robotics-ER can instantiate entirely new capabilities on the fly. For example, when shown a coffee mug, the model can intuit an appropriate two-finger grasp for picking it up by the handle and a safe trajectory for approaching it.</p><p data-block-key=\"1lam2\">Gemini Robotics-ER can perform all the steps necessary to control a robot right out of the box, including perception, state estimation, spatial understanding, planning and code generation. In such an end-to-end setting the model achieves a 2x-3x success rate compared to Gemini 2.0. And where code generation is not sufficient, Gemini Robotics-ER can even tap into the power of in-context learning, following the patterns of a handful of human demonstrations to provide a solution.</p></div><div><h2 data-block-key=\"2q7pg\">Responsibly advancing AI and robotics</h2><p data-block-key=\"16hbn\">As we explore the continuing potential of AI and robotics, we’re taking a layered, <a href=\"https://sites.google.com/corp/view/safe-robots\" rel=\"noopener\" target=\"_blank\">holistic</a> approach to addressing safety in our research, from low-level motor control to high-level semantic understanding.</p><p data-block-key=\"7s8sj\">The physical safety of robots and the people around them is a longstanding, foundational concern in the science of robotics. That's why roboticists have classic safety measures such as avoiding collisions, limiting the magnitude of contact forces, and ensuring the dynamic stability of mobile robots. Gemini Robotics-ER can be interfaced with these ‘low-level’ safety-critical controllers, specific to each particular embodiment. Building on Gemini’s core safety features, we enable Gemini Robotics-ER models to understand whether or not a potential action is safe to perform in a given context, and to generate appropriate responses.</p><p data-block-key=\"3dcnu\">To advance robotics safety research across academia and industry, we are also releasing a new dataset to evaluate and improve semantic safety in embodied AI and robotics. In previous work, we showed how a <a href=\"https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/\" rel=\"noopener\" target=\"_blank\">Robot Constitution</a> inspired by Isaac Asimov’s Three Laws of Robotics could help prompt an LLM to select safer tasks for robots. We have since developed a framework to automatically generate data-driven constitutions - rules expressed directly in natural language – to steer a robot’s behavior. This framework would allow people to create, modify and apply constitutions to develop robots that are safer and more aligned with human values. Finally, the <a href=\"https://asimov-benchmark.github.io/\" rel=\"noopener\" target=\"_blank\">new ASIMOV dataset</a> will help researchers to rigorously measure the safety implications of robotic actions in real-world scenarios.</p><p data-block-key=\"ch213\">To further assess the societal implications of our work, we collaborate with experts in our Responsible Development and Innovation team and as well as our Responsibility and Safety Council, an internal review group committed to ensure we develop AI applications responsibly. We also consult with external specialists on particular challenges and opportunities presented by embodied AI in robotics applications.</p><p data-block-key=\"cis52\">In addition to our partnership with Apptronik, our Gemini Robotics-ER model is also available to trusted testers including Agile Robots, Agility Robots, Boston Dynamics, and Enchanted Tools. We look forward to exploring our models’ capabilities and continuing to develop AI for the next generation of more helpful robots.</p></div><div><p data-block-key=\"duspu\">This work was developed by the Gemini Robotics team. For a full list of authors and acknowledgements please view <a href=\"https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf\" rel=\"noopener\" target=\"_blank\">our technical report</a>.</p></div>","contentLength":8374,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43344082"},{"title":"The DuckDB Local UI","url":"https://duckdb.org/2025/03/12/duckdb-ui.html","date":1741784161,"author":"xnx","guid":173,"unread":true,"content":"<div><p><em>TL;DR: The DuckDB team and MotherDuck are excited to announce the release of a local UI for DuckDB shipped as part of the  extension.</em></p></div><p>The DuckDB CLI provides advanced features like interactive multi-line editing, auto-complete, and progress indicators.\nHowever, it can be cumbersome for working with lengthy SQL queries, and its data exploration tools are limited.\nMany of the available third party UIs are great, but selecting, installing, and configuring one is not straightforward.\nUsing DuckDB through a UI should be as simple as using the CLI.\nAnd now it is!</p><p>Starting with <a href=\"https://github.com/duckdb/duckdb/releases/tag/v1.2.1\">DuckDB v1.2.1</a>, a full-featured local web user interface is available out-of-the-box!\nYou can start it from the terminal by launching the DuckDB CLI client with the  argument:</p><p>You can also run the following SQL command from a <a href=\"https://duckdb.org/docs/stable/clients/overview.html\">DuckDB client</a> (e.g., CLI, Python, Java, etc.):</p><p>Both of these approaches install the  extension (if it isn't installed yet),\nthen open the DuckDB UI in your browser:</p><p>The DuckDB UI uses interactive notebooks to define SQL scripts and show the results of queries.\nHowever, its capabilities go far beyond this.\nLet’s go over its main features.</p><blockquote><p>The DuckDB UI runs all your queries locally: your queries and data never leave your computer.\nIf you would like to use <a href=\"https://motherduck.com/\">MotherDuck</a> through the UI, you have to <a href=\"https://duckdb.org/2025/03/12/duckdb-ui.html#motherduck-integration\">opt-in explicitly</a>.</p></blockquote><p>Your attached databases are shown on the left.\nThis list includes in-memory databases plus any files and URLs you’ve loaded.\nYou can explore tables and views by expanding databases and schemas.</p><p>Click on a table or view to show a summary below.\nThe UI shows the number of rows, the name and type of each column, and a profile of the data in each column.</p><p>Select a column to see a more detailed summary of its data.\nYou can use the  button near the top right to inspect the first 100 rows.\nYou can also find the SQL definition of the table or view here.</p><p>You can organize your work into named notebooks.\nEach cell of the notebook can execute one or more SQL statements.\nThe UI supports syntax highlighting and autocomplete to assist with writing your queries.</p><p>You can run the whole cell, or just a selection,\nthen sort, filter, or further transform the results using the provided controls.</p><p>The right panel contains the <a href=\"https://motherduck.com/blog/introducing-column-explorer/\">Column Explorer</a>, which shows a summary of your results.\nYou can dive into each column to gain insights.</p><p>If you would like to connect to <a href=\"https://motherduck.com/\">MotherDuck</a>, you can sign into MotherDuck to persist files and tables to a <a href=\"https://motherduck.com/docs/getting-started/\">cloud data warehouse</a> crafted for using DuckDB at scale and sharing data with your team.</p><p>The DuckDB UI is under active development. Expect additions and improvements!</p><p>Like the DuckDB CLI, the DuckDB UI creates some files in the  directory in your home directory.\nThe UI puts its files in a sub-directory, :</p><ul><li>Your notebooks and some other state are stored in a DuckDB database, .</li><li>When you export data to the clipboard or a file (using the controls below the results), some tiny intermediate files (e.g. ) are generated.\nYour data is cleared from these files after the export is completed, but some near-empty files remain, one per file type.</li></ul><p>Support for the UI is implemented in a <a href=\"https://duckdb.org/docs/stable/extensions/overview.html\">DuckDB extension</a>.\nThe extension embeds a localhost HTTP server, which serves the UI browser application, and also exposes an API for communication with DuckDB.\nIn this way, the UI leverages the native DuckDB instance from which it was started, enabling full access to your local memory, compute, and file system.</p><p>Results are returned in an efficient binary form closely matching DuckDB’s in-memory representation (<a href=\"https://github.com/duckdb/duckdb/blob/v1.2.1/src/include/duckdb/common/types/data_chunk.hpp\">DataChunk</a>).\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events\">Server-sent events</a> enable prompt notification of updates such as attaching databases.\nThese techniques and others make for a low-latency experience that keeps you in your flow.</p><p>In this blog post, we presented the new DuckDB UI, a powerful web interface for DuckDB.</p><p>The DuckDB UI shares many of its design principles with the DuckDB database.\nIt’s simple, fast, feature-rich, and portable, and runs locally on your computer.\nThe DuckDB UI extension is also open source: visit the <a href=\"https://github.com/duckdb/duckdb-ui\"> repository</a> if you want to dive in deeper into the extension's code.</p><blockquote><p>The repository does not contain the source code for the frontend, which is currently not available as open-source.\nReleasing it as open-source is under consideration.</p></blockquote><p>We are very happy to see the overwhelmingly positive response to the announcement of the DuckDB Local UI. In this short update, we’d like to address some of the questions raised by the community:</p><ul><li>Regarding the <strong>licensing of the frontend code</strong> (HTML, JavaScript, etc.), MotherDuck is currently reviewing licensing options for the UI code and assets and will clarify shortly.</li><li>Regarding <strong>offline (“air-gapped”) operations,</strong> we are working towards supporting offline usage in a future version, and are currently exploring how to provide version notifications as upgrades become available.</li><li>Regarding the <strong>client-server protocol for the UI,</strong> we’re going to work on opening up the client-server protocol so that other user interfaces can use the “back-end” part of the UI extension to run queries in DuckDB.</li></ul>","contentLength":5052,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43342712"},{"title":"A 10x Faster TypeScript","url":"https://devblogs.microsoft.com/typescript/typescript-native-port/","date":1741703543,"author":"DanRosenwasser","guid":172,"unread":true,"content":"<p>Today I’m excited to announce the next steps we’re taking to radically improve TypeScript performance.</p><p>The core value proposition of TypeScript is an excellent developer experience.\nAs your codebase grows, so does the value of TypeScript itself, but in many cases TypeScript has not been able to scale up to the very largest codebases.\nDevelopers working in large projects can experience long load and check times, and have to choose between reasonable editor startup time or getting a complete view of their source code.\nWe know developers love when they can rename variables with confidence, find all references to a particular function, easily navigate their codebase, and do all of those things without delay.\nNew experiences powered by AI benefit from large windows of semantic information that need to be available with tighter latency constraints.\nWe also want fast command-line builds to validate that your entire codebase is in good shape.</p><p>To meet those goals, we’ve begun work on a native port of the TypeScript compiler and tools.\nThe native implementation will <strong>drastically improve editor startup, reduce most build times by 10x, and substantially reduce memory usage</strong>.\nBy porting the current codebase, we expect to be able to preview a native implementation of  capable of command-line typechecking by mid-2025, with a feature-complete solution for project builds and a language service by the end of the year.</p><p>You can , which is offered under the same license as the existing TypeScript codebase.\nCheck the README for instructions on how to build and run  and the language server, and to see a summary of what’s implemented so far.\nWe’ll be posting regular updates as new functionality becomes available for testing.</p><p>Our native implementation is already capable of loading many popular TypeScript projects, including <a href=\"https://github.com/microsoft/TypeScript/tree/main/src/compiler\">the TypeScript compiler itself</a>.\nHere are times to run  on some popular codebases on GitHub of varying sizes:</p><table><thead><tr></tr></thead></table><p>While we’re not yet feature-complete, these numbers are representative of the order of magnitude performance improvement you’ll see checking most codebases.</p><p>We’re incredibly excited about the opportunities that this massive speed boost creates. Features that once seemed out of reach are now within grasp.\nThis native port will be able to provide instant, comprehensive error listings across an entire project, support more advanced refactorings, and enable deeper insights that were previously too expensive to compute.\nThis new foundation goes beyond today’s developer experience and will enable the next generation of AI tools to enhance development, powering new tools that will learn, adapt, and improve the coding experience.</p><p>Most developer time is spent in editors, and it’s where performance is most important.\nWe want editors to load large projects quickly, and respond quickly in all situations.\nModern editors like Visual Studio and Visual Studio Code have excellent performance as long as the underlying language services are also fast.\nWith our native implementation, we’ll be able to provide incredibly fast editor experiences.</p><p>Again using the Visual Studio Code codebase as a benchmark, the current time to load the entire project in the editor on a fast computer is about 9.6 seconds.\nThis drops down to about 1.2 seconds with the native language service, an <strong>8x improvement in project load time</strong> in editor scenarios.\nWhat this translates to is a faster working experience from the time you open your editor to your first keystroke in any TypeScript codebase.\nWe expect all projects to see this level of improvement in load time.</p><p>Overall memory usage also appears to be roughly half of the current implementation, though we haven’t actively investigated optimizing this yet and expect to realize further improvements.\nEditor responsiveness for all language service operations (including completion lists, quick info, go to definition, and find all references) will also see significant speed gains.\nWe’ll also be moving to the Language Server Protocol (LSP), a longstanding infrastructural work item to better align our implementation with other languages.</p><p>Our most recent TypeScript release was TypeScript 5.8, with TypeScript 5.9 coming soon.\nThe JS-based codebase will continue development into the 6.x series, and TypeScript 6.0 will introduce some deprecations and breaking changes to align with the upcoming native codebase.</p><p>When the native codebase has reached sufficient parity with the current TypeScript, we’ll be releasing it as .\nThis is still in development and we’ll be announcing stability and feature milestones as they occur.</p><p>For the sake of clarity, we’ll refer to them simply as TypeScript 6 (JS) and TypeScript 7 (native), since this will be the nomenclature for the foreseeable future.\nYou may also see us refer to “Strada” (the original TypeScript codename) and “Corsa” (the codename for this effort) in internal discussions or code comments.</p><p>While some projects may be able to switch to TypeScript 7 upon release, others may depend on certain API features, legacy configurations, or other constraints that necessitate using TypeScript 6.\nRecognizing TypeScript’s critical role in the JS development ecosystem, we’ll still be maintaining the JS codebase in the 6.x line until TypeScript 7+ reaches sufficient maturity and adoption.</p><p>Our long-term goal is to keep these versions as closely aligned as possible so that you can upgrade to TypeScript 7 as soon as it meets your requirements, or fall back to TypeScript 6 if necessary.</p><p>In the coming months we’ll be sharing more about this exciting effort, including deeper looks into performance, a new compiler API, LSP, and more.\nWe’ve written up some <a href=\"https://github.com/microsoft/typescript-go/discussions/categories/faqs\">FAQs</a> on the GitHub repo to address some questions we expect you might have.\nWe also invite you to join us for an AMA at the <a href=\"https://discord.gg/typescript\">TypeScript Community Discord</a> at  on March 13th.</p><p>A 10x performance improvement represents a massive leap in the TypeScript and JavaScript development experience, so we hope you are as enthusiastic as we are for this effort!</p>","contentLength":6051,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43332830"},{"title":"Happy 20th birthday, Y Combinator","url":"https://twitter.com/garrytan/status/1899092996702048709","date":1741702475,"author":"btilly","guid":171,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43332658"},{"title":"Show HN: Factorio Learning Environment – Agents Build Factories","url":"https://jackhopkins.github.io/factorio-learning-environment/","date":1741694522,"author":"noddybear","guid":170,"unread":true,"content":"<p>I'm Jack, and I'm excited to share a project that has channeled my Factorio addiction recently: the Factorio Learning Environment (FLE).</p><p>FLE is an open-source framework for developing and evaluating LLM agents in Factorio. It provides a controlled environment where AI models can attempt complex automation, resource management, and optimisation tasks in a grounded world with meaningful constraints.</p><p>A critical advantage of Factorio as a benchmark is its unbounded nature. Unlike many evals that are quickly saturated by newer models, Factorio's geometric complexity scaling means it won't be \"solved\" in the next 6 months (or possibly even years). This allows us to meaningfully compare models by the order-of-magnitude of resources they can produce - creating a benchmark with longevity.</p><p>The project began 18 months ago after years of playing Factorio, recognising its potential as an AI research testbed. A few months ago, our team (myself, Akbir, and Mart) came together to create a benchmark that tests agent capabilities in spatial reasoning and long-term planning.</p><p>Two technical innovations drove this project forward: First, we discovered that piping Lua into the Factorio console over TCP enables running (almost) arbitrary code without directly modding the game. Second, we developed a first-class Python API that wraps these Lua programs to provide a clean, type-hinted interface for AI agents to interact with Factorio through familiar programming paradigms.</p><p>Agents interact with FLE through a REPL pattern:\n1. They observe the world (seeing the output of their last action)\n2. Generate Python code to perform their next action\n3. Receive detailed feedback (including exceptions and stdout)</p><p>We provide two main evaluation settings:\n- Lab-play: 24 structured tasks with fixed resources\n- Open-play: An unbounded task of building the largest possible factory on a procedurally generated map</p><p>We found that while LLMs show promising short-horizon skills, they struggle with spatial reasoning in constrained environments. They can discover basic automation strategies (like electric-powered drilling) but fail to achieve more complex automation (like electronic circuit manufacturing). Claude Sonnet 3.5 is currently the best model (by a significant margin).</p><p>You'll need:\n- Factorio (version 1.1.110)\n- Docker\n- Python 3.10+</p><p>The README contains detailed installation instructions and examples of how to run evaluations with different LLM agents.</p><p>We would love to hear your thoughts and see what others can do with this framework!</p>","contentLength":2527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43331582"},{"title":"Show HN: Seven39, a social media app that is only open for 3 hours every evening","url":"https://www.seven39.com/","date":1741655110,"author":"mklyons","guid":169,"unread":true,"content":"<p>Because social media is better when we're all online together.</p><p>No endless scrolling. No FOMO. Just 3 hours of fun every evening.</p><p>The domain was available.</p>","contentLength":152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43328095"},{"title":"uBlock Origin is no longer available on the Chrome Store","url":"https://chromewebstore.google.com/detail/ublock-origin/cjpalhdlnbpafiamejdnhcphjbkeiagm?hl=en","date":1741627834,"author":"non-","guid":168,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43322922"},{"title":"Go European: Discover European products and services","url":"https://www.goeuropean.org/","date":1741601747,"author":"doener","guid":167,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43318798"},{"title":"It is as if you were on your phone","url":"https://pippinbarr.com/it-is-as-if-you-were-on-your-phone/info/","date":1741527635,"author":"bookofjoe","guid":166,"unread":true,"content":"<p><em>Look at you! On your phone! But you’ve got a secret! And you won’t tell! You’re not on your phone! It is only as if you were on your phone! You’re just pretending to be on your phone! On your phone!</em></p><p><em>It is as if you were on your phone</em> is an  game about an  in which we’re all simultaneously under significant pressure to be on our phones all the time, but also to not be on our phones all the time. Our fingers want to touch the screen, our eyes want to watch the surface, our brains want to be occupied efficiently and always. But it’s also exhausting liking photos, swiping profiles, watching short-form video, and everything else we’re always doing. <em>It is as if you were on your phone</em> presents an alternative:  to be on your phone so that you pass as human, but actually do essentially nothing instead. Follow the prompts and be free.</p><p><em>It is as if you were on your phone</em> was created using <a href=\"https://p5js.org\">p5</a> along with <a href=\"https://hammerjs.github.io/\">Hammer.js</a> for touch gestures.</p>","contentLength":945,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43308994"},{"title":"US Ends Support For Ukrainian F-16s","url":"https://ukrainetoday.org/us-ends-support-for-ukrainian-f-16s-but-french-mirages-will-be-salvation-forbes/","date":1741518507,"author":"ctack","guid":165,"unread":true,"content":"<p>The Donald Trump administration has cut off vital support for F-16 jamming capabilities.</p><p>The United States, having decided to end its support for Ukraine, cannot simply turn off the Ukrainian Air Force’s U.S.-designed&nbsp;<a href=\"https://www.unian.net/world/f-16-dlya-ukrainy-aviaekspert-nazval-veroyatnuyu-prichinu-zaderzhki-postavki-belgiyskih-istrebiteley-12937956.html\" target=\"_blank\" rel=\"noreferrer noopener\">F-16</a>&nbsp;fighters . But the Trump administration has cut off vital support for their jamming capabilities, which could deprive the Ukrainian Air Force of a critical air countermeasure.</p><p><a href=\"https://www.forbes.com/sites/davidaxe/2025/03/07/france-to-the-rescue-french-made-mirage-2000-jets-could-become-ukraines-most-important-aerial-radar-jammers/\" rel=\"noreferrer noopener\" target=\"_blank\">However, as Forbes</a>&nbsp;analyst David Ax writes&nbsp;, the Ukrainians are not powerless and can shift the burden of air jamming onto the French Dassault Mirage 2000 fighters.</p><p>As Aks points out, the Ukrainian Air Force is taking full advantage of the ability of F-16s equipped with AN/ALQ-131 pods to flood Russian radar screens with electronic noise. They act as “flying air defenses” with advanced missile warning technology, notes the Conflict Intelligence Team.</p><p>But the Russian Air Force can circumvent the jamming by reprogramming its radars to different frequencies. As Axe points out, while the Biden Air Force was able to keep up with the Russian adaptation by constantly tweaking the AN/ALQ-131 frequencies, under Trump, Ukrainian pilots are not receiving updates, and the programs could soon become obsolete.</p><p>However, France’s Mirage 2000s are equipped with their own powerful jammers, and the Americans are not involved in their programming, the analyst notes.</p><p>The Mirage 2000-5F currently in service with the French Air Force flies with a combination of the Serval radar warning receiver, Sabre jammer and Eclair flare dispenser. This system was cutting edge in the 1980s, but within a generation it was falling behind.</p><p>“Recognizing this weakness and understanding the seriousness of the Russian missile threat over Ukraine, the French Ministry of Defense promised to install new electronic countermeasures on the Mirage 2000s before handing them over to Ukraine. The ministry probably had in mind the predominantly analog Integrated Countermeasures Suite Mark 2 or the all-digital Integrated Countermeasures Suite Mark 3,” Ax notes.</p><p>Either system is an improvement over the older system and a potential replacement for the AN/ALQ-131, as the American pods lag behind the Russian adaptation. The French are firm allies of Ukraine and are willing to reprogram the jammers if necessary.</p><p>In the longer term, the Ukrainians could refit their F-16s with non-American electronic countermeasures, Axe notes. But that could take time and money that Ukraine cannot afford.</p>","contentLength":2470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43307996"},{"title":"My 16-month theanine self-experiment","url":"https://dynomight.net/theanine/","date":1741489692,"author":"dynm","guid":164,"unread":true,"content":"<p>The internet <a href=\"https://en.wikipedia.org/wiki/Theanine\">theanine</a>. This is an amino acid analog that’s naturally found in tea, but now sold as a nutritional supplement for anxiety or mood or memory.</p><p>Biologically speaking, it’s plausible. Theanine is structurally related to the neurotransmitter <a href=\"https://en.wikipedia.org/wiki/Glutamate_(neurotransmitter)\">glutamate</a> (theanine = C₇H₁₄N₂O₃, glutamate = C₅H₈NO₄-). For some reason, everyone is obsessed with stupid flashy dopamine and serotonin, and no one cares about glutamate. But it’s the most common neurotransmitter and theanine is both metabolized into glutamate and seems to itself have various <a href=\"https://en.wikipedia.org/wiki/Theanine#In_vitro\">complicated</a> effects on glutamate receptors.</p><p>Of course, there are lots of supplements that  act on the brain, but are useless when taken orally. That’s because your brain is isolated from your circulatory system by a <a href=\"https://en.wikipedia.org/wiki/Blood%E2%80%93brain_barrier\">thin layer of cells</a> that are extremely picky about what they let through. But it appears that theanine <a href=\"https://doi.org/10.1271/bbb.63.615\"></a> get through these cells and into the brain.</p><p>So that sounds good. But do these low-level effects actually lead to changes in mood in real humans? When I looked into the academic research, I was surprised by how weak it was. Personally, on <a href=\"https://dynomight.net/aspartame/#the-european-food-safety-authority-efsa\">these kinds of issues</a>, I find the European Food Safety Authority to be the single most trustworthy scientific body. They did an <a href=\"https://www.efsa.europa.eu/en/efsajournal/pub/2238\">assessment</a> in 2011 and found:</p><table><tbody><tr><td>Improvement of cognitive function</td><td>cause and effect relationship has not been established</td></tr><tr><td>Alleviation of psychological stress</td><td>cause and effect relationship has not been established</td></tr><tr><td>Maintenance of normal sleep</td><td>cause and effect relationship has not been established</td></tr><tr><td>Reduction of menstrual discomfort</td><td>cause and effect relationship has not been established</td></tr></tbody></table><p><a href=\"https://examine.com/supplements/theanine/\">Examine</a> is an independent website that’s respected for summarizing the scientific literature on health and supplements. They looked into if theanine helped with various things, like alertness, anxiety, and attention. In all cases found  for .</p><p>A <a href=\"https://doi.org/10.1007/s11130-019-00771-5\">2020 review</a> of eight randomized double-blind placebo controlled trials found that theanine  help with stress and anxiety. While this review seems generally good, I found it to be insufficiently paranoid. One study they review found that theanine worked better than <a href=\"https://en.wikipedia.org/wiki/Alprazolam\">alprazolam</a> (xanax) for acute anxiety. The correct response would be, “That’s , and the fact that normal scientific practices could lead to such a conclusion casts doubt on everything.” But the review sort of takes it at value and moves on.</p><p>After 2020, the only major trial I could find was <a href=\"https://doi.org/10.1089/jmf.2020.4803\">this 2021 study</a> that took 52 healthy older Japanese people and gave them theanine (or placebo) for 12 weeks. They tested for improvements in a million different measures of cognitive functioning and mostly found nothing.</p><p>I’ve long found that tea makes me much less nervous than coffee, even with equal caffeine. Many people have suggested theanine as the explanation, but I’m skeptical. Most tea only has ~5 mg of theanine per cup, while when people supplement, they take 100-400 mg. Apparently grassy shade-grown Japanese teas are particularly high in theanine. And I  find those teas particularly calming. But they still only manage ~25 mg per cup. (Maybe it’s because tea is better than coffee?)</p><p>Still, I’ve supplemented theanine on and off for more than 10 years, and it seems helpful. So after seeing the weak scientific evidence, I thought: Why not do a self-experiment?</p><p>Theanine seems ideal because it’s a supplement with short term effects. So you can test it against placebo. (Try that with meditation.) And you can build up a large sample using a single human body without waiting weeks for it to build up in the body before each measurement.</p><p>Everyone agrees theanine is <a href=\"https://www.fda.gov/media/182086/download\">safe</a>. It’s biologically plausible. While academic studies haven’t proven a benefit, they haven’t  one either. Given the vast  evidence, I saw a chance to stick it to the stodgy scientific establishment, to show the power of internet people and give the first rigorous evidence that theanine really works. Stockholm, prepare thyself.</p><p>First, I needed placebos. This was super annoying. The obvious way to create them would be to buy some empty capsules and fill some with theanine and others with some inert substance. But that doesn’t sound fun. Isn’t the whole idea of modernity that we’re supposed to replace labor with capital?</p><p>So I went searching for a pair of capsules I could buy off the shelf, subject to the following constraints:</p><ol><li>Capsule A contains 200 mg of theanine.</li><li>Capsule B contains something with minimal acute effects on anxiety, stress, memory, concentration, etc.</li><li>Capsule B contains something I don’t mind putting into my body.</li><li>Both capsules are exactly the same size and weight.</li><li>Both capsules are almost but not  the same color.</li><li>Both capsules are made by some company with a history of making at least a modest effort to sell supplements that contain what they say they contain, and that don’t have terrifying levels of heavy metals.</li></ol><p>After a ludicrous amount of searching, I found that NOW® sells these veggie capsules:</p><p> 200 mg L-Theanine</p><p> 25 mcg (1,000 IU) Vitamin D</p><p>These are exactly the same size, exactly the same weight, exactly the same texture, and  close in color. They’re so close in color that under warm lighting, they’re indistinguishable. But under cold/blue lighting, the vitamin D capsules are  more yellow. Vitamin D might have some effects on mood, but no one seems to claim that they’re , that you’d feel them within an hour.</p><p>For dosing, I decided to take a capsule whenever I was feeling stressed or anxious. Some people worry this invalidates the results. Not so! I’m still choosing randomly, and this better reflects how people use theanine in practice.</p><p>Theanine is often recommended for reducing anxiety from caffeine. While I didn’t explicitly take caffeine as part of this experiment, I had almost always taken some anyway.</p><p>Statistically, it would have been best to randomize so I had a 50% chance of taking theanine and a 50% chance of taking vitamin D. But I decided that would be annoying, since I was taking these capsules when stressed. So I decided to randomize so I got theanine ⅔ of the time and vitamin D ⅓ of the time.</p><p>Randomization was very easy: I took two theanine capsules and one vitamin D capsule and put them into a little cup. I then closed my eyes, shook the cup around a bit and took one. I then covered the cup with a card.</p><p>This picture shows one vitamin D capsule (top) and two theanine capsules.</p><p>For each trial, I recorded my subjective starting stress level on a scale of 1-5, then set an alarm for an hour, which is <a href=\"https://doi.org/10.3945/jn.112.166371\">enough</a> to reach near-peak concentrations in the blood. After the alarm sounded (or occasionally later, if I missed it) I recorded the end time, my end stress level, and my percentage prediction that what I’d taken was actually theanine. Then, and only then, I looked into the cup. If the two remaining pills were different colors, I’d taken theanine. If not, it was vitamin D.</p><p>After ~14 months, I got frustrated by how slowly data was coming in. This was the first time in my life I’ve had  much chill. At that point, I decided to start taking the capsules once or twice a day, even if I wasn’t stressed. I’ll show the transition point in the graphs below.</p><p>Ultimately, I collected 94 data points, which look like this:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Here are the raw stress levels. Each line line shows one trial, with the start marked with a tiny horizontal bar. Note the clear change when I started dosing daily:</p><p>Alternatively, here’s the  in stress (end - start) as a function of time. If “Δ Stress” is negative, that means stress went down.</p><p>Here are the start and end stress levels for each trial, ignoring time. The dotted line shows equal stress levels, so anything below that line means stress went down:</p><p>Finally, here are the probabilities I gave that each capsule was in fact theanine.</p><p>My stress level  usually go down, at least provided I was stressed at the start. But it went down regardless of if I took theanine or not. And I was  at guessing what I’d taken.</p><p>Why did my stress decrease when I took vitamin D? Maybe it’s the placebo effect. But I suspect it’s mostly reversion to the mean: If you mark down the times in your life when you’re most stressed, on average you’ll be less stressed an hour later. You can see some evidence for this in that stress tended to decrease more when it started at a higher level.</p><p>So, eyeballing the above figures, theanine doesn’t appear to do anything. (We can argue about statistics below.) Why? I think these are the possibilities:</p><ol><li>Theanine works, but I got fake theanine.</li><li>Theanine works, but vitamin D works equally well.</li><li>Theanine works, but I was unlucky.</li><li>Theanine works, but I’m disembodied and unable to report my internal states.</li><li>Theanine works on some people, but not me.</li></ol><p>It’s hard to disprove the idea that theanine works. But I tell you this: I expected it to work. And I  tried. For almost 100 trials over 16 months, I paid attention to what I was feeling and tried to detect  sign that I’d taken theanine, even if it wasn’t a change in stress. I could detect nothing. Even after months of failure, I’d often feel confident that  time I could tell, only to be proven wrong.</p><table><tbody></tbody></table><p>Should I have been surprised by these results? Well, the scientific literature on theanine hasn’t found much of an effect. And the only other  self-experiment on theanine I’ve found is by <a href=\"https://niplav.site/nootropics#LTheanine\">Niplav</a>, who found it did slightly worse than chance and declared it a “hard pass”.</p><p>What about other blinded self-experiments with other substances? They’re surprisingly scarce, but here’s what I could find:</p><p>Stimulants work! But for everything else…</p><p>I particularly encourage you to read the sleep support post. He was confident it worked, he’d recommended it to lots of friends, but it totally failed when put to the test.</p><p>I’ve seen  other self-experiments (including for theanine), but they’re non-blinded and I’d be doing you a disservice if I linked to them. People often mention that  this means the results aren’t scientific, but treat it like a small niggling technicality. It’s not.</p><p>So I propose a new rule: Blind trial or GTFO.</p><p>I know many people reading this probably use and like theanine. Maybe it works for you! But given the weak academic results, and given the fact that I actually did a blinded experiment, I think you now have the burden of proof. Doing this kind of test isn’t hard. If you’re sure theanine (or anything else) works, prove it.</p><h2>Appendix: OK fine let’s argue about statistics</h2><p>Do you demand p-values? Are you outraged I just plotted the data and then started talking about it qualitatively?</p><p>I think faith in statistics follows a U-shaped curve. By default, people don’t trust them. If you learn a  statistics, they seem great. (Particularly if you’re part of a community that’s formed a little cult around one set of statistical practices and convinced each other that they’re more reliable than they are.) But if you learn a  of statistics, then you realize all the assumptions that are needed and all the ways things can go wrong and you become very paranoid.</p><p>If you want p-values, I’ll give you p-values. But first let me point out a problem.</p><p>While I was blinded during each trial, I saw the theanine/D result when I wrote it down. Over time I couldn’t help but notice that my stress dropped even when I took vitamin D, and that I was terrible at predicting what I’d taken. So while this experiment is randomized and blinded, the data isn’t  or . If I did this again, I’d make sure I couldn’t see any outcomes until the end, perhaps by making 100 numbered envelopes, putting three capsules in each, and only looking at what was left at the end.</p><p>But if you want to compute p-values anyway, OK! Here are the basic numbers for the trials when I took theanine:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Stress went down, p &lt; .0000001. But here are the the numbers for vitamin D:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Technically, I did find two significant results. But the second row says that end stress was slightly  with theanine than with vitamin D, and the last row says that I gave slightly  probabilities that I’d taken theanine when I’d actually taken vitamin D.</p><p>Of course, I don’t think this means I’ve proven theanine is harmful. I just think this confirms my general paranoia. To a first approximation, if it ain’t visible in the raw data, I ain’t going.</p><p>Speaking of raw data, you can download mine <a href=\"https://dynomight.net/img/theanine/data.csv\">here</a>.</p>","contentLength":12343,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43305803"},{"title":"Kill your Feeds – Stop letting algorithms dictate what you think","url":"https://usher.dev/posts/2025-03-08-kill-your-feeds/","date":1741457506,"author":"tom_usher","guid":163,"unread":true,"content":"<div data-intro=\"\">\nWe are being boiled like frogs. It happened gradually, one algorithmic tweak at a time. What started as a way to connect with friends has become a system that gives the corporations that run social media control over what we consume and the ability to subtly shape how we think.\n</div><p>We used to control apps like Facebook and Instagram with our own choices. They became daily comforts, making the world seem a little bit smaller and closer by bringing the people that we cared about together in to one place.</p><p>But from the perspective of these companies, that’s a problem. Our personal worlds, our friends, family, and connections, are finite. Once we’ve caught up, we put the app down. That’s bad for business.</p><p>Social media companies need us flicking through their apps as long as they can keep us there. More eyes on ads is more money. So they play the system a bit. You’ve lingered on enough photos of cute puppies, they know what you like.</p><p>Before long those feeds of finite content are replaced by infinite algorithmic content pulled from millions of users trying to optimise their posts to be picked up by the omnipotent algorithms. Algorithms which are completely opaque to us.</p><p>Sci-fi imagines megacorporations controlling our minds with brain implants. Some worry that companies are already listening in. But they don’t have to - they already control our eyes.</p><p>The creators of TikTok, Instagram etc. have gained control over exactly what we see. What we see strongly influences how we think. <a href=\"https://en.m.wikipedia.org/wiki/2021_Facebook_leak\">They know</a> that their feeds make us angry, they know the negative effects on our mental health (particularly that of teens), and they know that they have an influence on our opinion.</p><p>With the power to shape what we see comes the power to shape what we believe. Whether through deliberate manipulation or the slow creep of algorithmic recommendations, engagement is fueled by outrage, and outrage breeds extremism. The result is a feedback loop that isolates users, reinforces beliefs, and deprioritises opposing viewpoints.</p><p>We live in times where being able to form our own opinion is more important than ever. Where knowing how to source and identify truthful information is a critical skill.</p><p>Our reliance on being spoon fed ideas is destroying those abilities, Alec of Technology Connections calls this <a href=\"https://youtu.be/QEJpZjg8GuA\">algorithmic complacency</a>, referencing our increasing inability to look outside our algorithmically created bubble. The social media companies don’t care, the only person who has any interest in fixing this is you.</p><p>It’s time to take back control of how we think. We’ve identified the problem, now it’s time to take action.</p><p>We don’t all have the freedom, interest or willpower to delete social media from our lives entirely. It’s still where our friends are, an occasional distraction from reality and a source of entertainment. You don’t have to become a digital outcast to hold back this influence.</p><ol><li>Go directly to the source - if you like a particular TikTok creator, Facebook page or YouTube channel, skip the feed and go directly to their pages. Consider bookmarking their profiles individually.</li><li>Learn to find information and entertainment without a feed - try to find a creator making videos or writing about a topic of interest without having to stumble across them in a feed.</li><li>Use platforms and platform features that let you control your experience - Instagram’s ‘Following’ feed, YouTube’s Subscriptions page, <a href=\"https://bsky.app\">Bluesky</a>, <a href=\"https://mastodon.social\">Mastodon</a> and <a href=\"https://zapier.com/blog/how-to-use-rss-feeds/\">RSS feeds</a></li><li>Be mindful of engagement traps - recognise how algorithmic feeds are designed to keep you engaged and scrolling. Take a breath and stop the cycle.</li><li>Talk about it - if you’re reading this you a already know this is a problem. Your friends and family may not be aware of how their feeds are manipulating their attention and beliefs. Without intervention, the radicalisation of opinions, and the consequences we’re already seeing, will only escalate.</li></ol><p>The internet should serve you, not the other way around. Take back control. Kill your feeds before they kill your ability to think independently.</p>","contentLength":4057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43302132"},{"title":"Mistral OCR","url":"https://mistral.ai/fr/news/mistral-ocr","date":1741282779,"author":"littlemerman","guid":162,"unread":true,"content":"<p dir=\"ltr\">Throughout history, advancements in information abstraction and retrieval have driven human progress. From hieroglyphs to papyri, the printing press to digitization, each leap has made human knowledge more accessible and actionable, fueling further innovation.&nbsp;</p><p dir=\"ltr\">Today, we’re at the precipice of the next big leap—to unlock the collective intelligence of all digitized information. Approximately&nbsp;<a href=\"https://resources.data.gov/glossary/unstructured-data/\">90%</a> of the world’s organizational data is stored as documents, and to harness this potential, we are introducing <a href=\"https://docs.mistral.ai/capabilities/document/\">Mistral OCR</a>. <p>Mistral OCR is an Optical Character Recognition API that sets a new standard in document understanding. Unlike other models, Mistral OCR comprehends each element of documents—media, text, tables, equations—with unprecedented accuracy and cognition. It takes images and PDFs as input and extracts content in an ordered interleaved text and images.</p></p><p dir=\"ltr\">As a result, Mistral OCR is an ideal model to use in combination with a RAG system taking multimodal documents (such as slides or complex PDFs) as input.</p><p dir=\"ltr\">We have made Mistral OCR as the default model for document understanding across millions of users on Le Chat, and are releasing the API  at 1000 pages / $ (and approximately double the pages per dollar with batch inference). The API is available today on our developer suite&nbsp;<a href=\"http://console.mistral.ai\">la Plateforme</a>, and coming soon to our cloud and inference partners, as well as on-premises.</p><ol><li dir=\"ltr\" aria-level=\"1\"><p dir=\"ltr\" role=\"presentation\">State of the art understanding of complex documents</p></li><li dir=\"ltr\" aria-level=\"1\"><p dir=\"ltr\" role=\"presentation\">Natively multilingual and multimodal</p></li><li dir=\"ltr\" aria-level=\"1\"><p dir=\"ltr\" role=\"presentation\">Doc-as-prompt, structured output</p></li><li dir=\"ltr\" aria-level=\"1\"><p dir=\"ltr\" role=\"presentation\">Selectively available to self-host for organizations dealing with highly sensitive or classified information</p></li></ol><h3 dir=\"ltr\">State of the art understanding of complex documents</h3><p dir=\"ltr\">Mistral OCR excels in understanding complex document elements, including interleaved imagery, mathematical expressions, tables, and advanced layouts such as LaTeX formatting. The model enables deeper understanding of rich documents such as scientific papers with charts, graphs, equations and figures.&nbsp;</p><p dir=\"ltr\">Below is an example of the model extracting text as well as imagery from a given PDF into a markdown file. You can access the notebook <a href=\"https://colab.research.google.com/github/mistralai/cookbook/blob/main/mistral/ocr/structured_ocr.ipynb\" target=\"_blank\" rel=\"noopener\">here</a>.&nbsp;</p><p dir=\"ltr\">Below we have side-by-side comparisons of PDFs and their respective OCR's outputs. Hover the slider&nbsp; to switch between input and output.&nbsp;</p><p dir=\"ltr\">Mistral OCR has consistently outperformed other leading OCR models in rigorous benchmark tests. Its superior accuracy across multiple aspects of document analysis is illustrated below. We extract embedded images from documents along with text. The other LLMs compared below, do not have that capability. For a fair comparison, we evaluate them on our internal “text-only” test-set containing various publication papers, and PDFs from the web; below:</p><div dir=\"ltr\" align=\"left\"><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><p dir=\"ltr\">Since Mistral’s founding, we have aspired to serve the world with our models, and consequently strived for multilingual capabilities across our offerings. Mistral OCR takes this to a new level, being able to parse, understand, and transcribe thousands of scripts, fonts, and languages across all continents. This versatility is crucial for both global organizations that handle documents from diverse linguistic backgrounds, as well as hyperlocal businesses serving niche markets.</p><div dir=\"ltr\" align=\"left\"><table><thead><tr><th>Fuzzy Match in Generation</th></tr></thead><tbody><tr></tr></tbody></table></div><div dir=\"ltr\" align=\"left\"><table><thead><tr></tr></thead><tbody></tbody></table></div><p dir=\"ltr\">Being lighter weight than most models in the category, Mistral OCR performs significantly faster than its peers, processing up to 2000 pages per minute on a single node. The ability to rapidly process documents ensures continuous learning and improvement even for high-throughput environments.</p><h3 dir=\"ltr\">Doc-as-prompt, structured output</h3><p dir=\"ltr\">Mistral OCR also introduces the use of documents as prompts, enabling more powerful and precise instructions. This capability allows users to extract specific information from documents and format it in structured outputs, such as JSON. Users can chain extracted outputs into downstream function calls and build agents. See this example&nbsp;<a href=\"https://colab.research.google.com/github/mistralai/cookbook/blob/main/mistral/ocr/structured_ocr.ipynb\" target=\"_blank\" rel=\"noopener\">notebook</a>.&nbsp;</p><h3 dir=\"ltr\">Available to self-host on a selective basis</h3><p dir=\"ltr\">For organizations with stringent data privacy requirements, Mistral OCR offers a self-hosting option. This ensures that sensitive or classified information remains secure within your own infrastructure, providing compliance with regulatory and security standards. If you would like to explore self-deployment with us, please <a href=\"https://mistral.ai/contact\">let us know</a>.</p><p dir=\"ltr\">We are empowering our beta customers to elevate their organizational knowledge by transforming their extensive document repositories into actions and solutions. Some of the key use cases where our technology is making a significant impact include:</p><p dir=\"ltr\"><strong>Digitizing scientific research</strong>: Leading research institutions have been experimenting with Mistral OCR to convert scientific papers and journals into AI-ready formats, making them accessible to downstream intelligence engines. This has facilitated measurably faster collaboration and accelerated scientific workflows.</p><p dir=\"ltr\"><strong>Preserving historical and cultural heritage</strong>: Organizations and nonprofits that are custodians of heritage have been using Mistral OCR to digitize historical documents and artifacts, ensuring their preservation and making them accessible to a broader audience.</p><p dir=\"ltr\"><strong>Streamlining customer service</strong>: Customer service departments are exploring Mistral OCR to transform documentation and manuals into indexed knowledge, reducing response times and improving customer satisfaction.</p><p dir=\"ltr\"><strong>Making literature across design, education, legal, etc. AI ready</strong>: Mistral OCR has also been helping companies convert technical literature, engineering drawings, lecture notes, presentations, regulatory filings and much more into indexed, answer-ready formats, unlocking intelligence and productivity across millions of documents.</p><p dir=\"ltr\">Mistral OCR capabilities are free to try on <a href=\"http://chat.mistral.ai\">le Chat</a>. To try the API, head over to <a href=\"http://console.mistral.ai\">la Plateforme</a>. We’d love to get your feedback; expect the model to continue to get even better in the weeks to come. As part of our strategic engagement programs, we will also offer on-premises deployment on a <a href=\"https://mistral.ai/contact\">selective basis</a>.</p>","contentLength":5946,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43282905"},{"title":"Age and cognitive skills: Use it or lose it","url":"https://www.science.org/doi/full/10.1126/sciadv.ads1560?af=R","date":1741264407,"author":"nabla9","guid":161,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43279494"}],"tags":["dev"]}